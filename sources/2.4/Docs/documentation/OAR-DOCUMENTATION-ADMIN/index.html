<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>[OAR Archive] OAR-DOCUMENTATION-ADMIN</title>

<link rel="icon" href="../../../../../favicon.ico" type="image/x-icon" />

<link rel="stylesheet" href="../../../../../style.css" type="text/css" />

<link rel="stylesheet" href="../../../../../local.css" type="text/css" />





</head>
<body>

<div class="page">

<div class="pageheader">
<div class="header">
<span>
<span class="parentlinks">

<a href="../../../../../">OAR</a>/ 

<a href="../../../../../">sources</a>/ 

<a href="../../../../../">2.4</a>/ 

<a href="../../../../../">Docs</a>/ 

<a href="../../../../../">documentation</a>/ 

</span>
<span class="title">
OAR-DOCUMENTATION-ADMIN

</span>
</span>

<form method="get" action="http://oar.imag.fr//ikiwiki.cgi" id="searchform">
<div>
<input type="text" id="searchbox" name="P" value="" size="16"
 />
</div>
</form>


</div>





</div>


<div class="sidebar">
<ul>
<li><strong><a href="../../../../../">Home</a></strong></li>
<li><strong><a href="../../../../../about/">About</a></strong></li>
<li><strong><a href="../../../../../news/">News</a></strong></li>
<li><strong><a href="http://oar.imag.fr/archive/wiki-oar">Wiki</a></strong></li>
<li><strong>Getting OAR</strong>
<ul>
<li><a href="../../../../../repositories/">Repositories</a></li>
<li><a href="../../../../../installation/">Installation</a></li>
<li><a href="../../../../../changelog/">Changelog</a></li>
</ul></li>
<li><strong>Using OAR</strong>
<ul>
<li><a href="../../../../../user-quickstart/">First user steps</a></li>
<li><a href="../../../../../user-usecases/">Use cases</a></li>
<li><a href="../../../../../faq/">FAQ</a></li>
</ul></li>
<li><strong>Getting help</strong>
<ul>
<li><a href="../../../../../support/">Contact/Mailing lists</a></li>
<li><a href="../../../../../documentation/">Documentation</a></li>
</ul></li>
<li><strong>Contributing to OAR</strong>
<ul>
<li><a href="../../../../../contributing/repositories/">Source repositories</a></li>
<li><a href="../../../../../contributing/workflow/">Workflow</a></li>
<li><a href="../../../../../contributing/GSOCs/">GSOCs</a></li>
</ul></li>
<li><strong><a href="../../../../../research/">Researchers' Corner</a></strong></li>
<li><strong><a href="../../../../../partners-projects/">Partners &amp; Projects</a></strong></li>
</ul>

</div>


<div id="pagebody">

<div id="content">






OAR Documentation - Admin Guide



<div class="document" id="oar-documentation-admin-guide">
<h1 class="title">OAR Documentation - Admin Guide</h1>

<img alt="OAR logo" class="align-center" src="../schemas/oar_logo.png" />
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Authors:</th><td class="field-body">Capit Nicolas, Emeras Joseph</td>
</tr>
<tr class="field"><th class="field-name">Address:</th><td class="field-body">Laboratoire d'Informatique de Grenoble
Bat. ENSIMAG - antenne de Montbonnot
ZIRST 51, avenue Jean Kuntzmann
38330 MONTBONNOT SAINT MARTIN</td>
</tr>
<tr class="field"><th class="field-name">Contact:</th><td class="field-body"><a class="reference external" href="mailto:nicolas.capit@imag.fr">nicolas.capit&#64;imag.fr</a>, <a class="reference external" href="mailto:joseph.emeras@imag.fr">joseph.emeras&#64;imag.fr</a></td>
</tr>
<tr class="field"><th class="field-name">Authors:</th><td class="field-body">LIG laboratory</td>
</tr>
<tr class="field"><th class="field-name">Organization:</th><td class="field-body">LIG laboratory</td>
</tr>
<tr class="field"><th class="field-name">Status:</th><td class="field-body">Stable</td>
</tr>
<tr class="field"><th class="field-name">Copyright:</th><td class="field-body">licenced under the GNU GENERAL PUBLIC LICENSE</td>
</tr>
<tr class="field"><th class="field-name">Dedication:</th><td class="field-body">For administrators.</td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Abstract:</th><td class="field-body">OAR is a resource manager (or batch scheduler) for large clusters. By it's
functionnalities, it's near of PBS, LSF, CCS and Condor. It's suitable for
productive plateforms and research experiments.</td>
</tr>
</tbody>
</table>
<p><strong>BE CAREFULL : THIS DOCUMENTATION IS FOR OAR &gt;= 2.3.0</strong></p>
<p>PDF version : <a class="reference external" href="OAR-DOCUMENTATION-ADMIN.pdf">OAR-DOCUMENTATION-ADMIN.pdf</a></p>
<div class="contents topic" id="table-of-contents">
<p class="topic-title first">Table of Contents</p>
<ul class="auto-toc simple">
<li><a class="reference internal" href="#oar-capabilities" id="id6">1&nbsp;&nbsp;&nbsp;OAR capabilities</a></li>
<li><a class="reference internal" href="#installing-the-oar-batch-system" id="id7">2&nbsp;&nbsp;&nbsp;Installing the OAR batch system</a><ul class="auto-toc">
<li><a class="reference internal" href="#requirements" id="id8">2.1&nbsp;&nbsp;&nbsp;Requirements</a></li>
<li><a class="reference internal" href="#configuration-of-the-cluster" id="id9">2.2&nbsp;&nbsp;&nbsp;Configuration of the cluster</a></li>
<li><a class="reference internal" href="#cpuset-installation" id="id10">2.3&nbsp;&nbsp;&nbsp;CPUSET installation</a><ul class="auto-toc">
<li><a class="reference internal" href="#what-are-oarsh-and-oarsh-shell-scripts" id="id11">2.3.1&nbsp;&nbsp;&nbsp;What are &quot;oarsh&quot; and &quot;oarsh_shell&quot; scripts ?</a></li>
<li><a class="reference internal" href="#cpuset-definition" id="id12">2.3.2&nbsp;&nbsp;&nbsp;CPUSET definition</a></li>
<li><a class="reference internal" href="#oarsh" id="id13">2.3.3&nbsp;&nbsp;&nbsp;OARSH</a></li>
<li><a class="reference internal" href="#oarsh-shell" id="id14">2.3.4&nbsp;&nbsp;&nbsp;OARSH_SHELL</a></li>
<li><a class="reference internal" href="#important-notes" id="id15">2.3.5&nbsp;&nbsp;&nbsp;Important notes</a></li>
</ul>
</li>
<li><a class="reference internal" href="#using-taktuk" id="id16">2.4&nbsp;&nbsp;&nbsp;Using Taktuk</a></li>
<li><a class="reference internal" href="#visualization-tools-installation" id="id17">2.5&nbsp;&nbsp;&nbsp;Visualization tools installation</a></li>
<li><a class="reference internal" href="#debian-packages" id="id18">2.6&nbsp;&nbsp;&nbsp;Debian packages</a></li>
<li><a class="reference internal" href="#starting" id="id19">2.7&nbsp;&nbsp;&nbsp;Starting</a></li>
<li><a class="reference internal" href="#energy-saving" id="id20">2.8&nbsp;&nbsp;&nbsp;Energy saving</a></li>
<li><a class="reference internal" href="#further-informations" id="id21">2.9&nbsp;&nbsp;&nbsp;Further informations</a></li>
</ul>
</li>
<li><a class="reference internal" href="#security-aspects-in-oar" id="id22">3&nbsp;&nbsp;&nbsp;Security aspects in OAR</a></li>
<li><a class="reference internal" href="#administrator-commands" id="id23">4&nbsp;&nbsp;&nbsp;Administrator commands</a><ul class="auto-toc">
<li><a class="reference internal" href="#oarproperty" id="id24">4.1&nbsp;&nbsp;&nbsp;<em>oarproperty</em></a></li>
<li><a class="reference internal" href="#oarnodesetting" id="id25">4.2&nbsp;&nbsp;&nbsp;<em>oarnodesetting</em></a></li>
<li><a class="reference internal" href="#oaradmin" id="id26">4.3&nbsp;&nbsp;&nbsp;<em>oaradmin</em></a></li>
<li><a class="reference internal" href="#oarremoveresource" id="id27">4.4&nbsp;&nbsp;&nbsp;<em>oarremoveresource</em></a></li>
<li><a class="reference internal" href="#oaraccounting" id="id28">4.5&nbsp;&nbsp;&nbsp;<em>oaraccounting</em></a></li>
<li><a class="reference internal" href="#oarnotify" id="id29">4.6&nbsp;&nbsp;&nbsp;<em>oarnotify</em></a></li>
<li><a class="reference internal" href="#oarmonitor" id="id30">4.7&nbsp;&nbsp;&nbsp;<em>oarmonitor</em></a></li>
</ul>
</li>
<li><a class="reference internal" href="#database-scheme" id="id31">5&nbsp;&nbsp;&nbsp;Database scheme</a><ul class="auto-toc">
<li><a class="reference internal" href="#accounting" id="id32">5.1&nbsp;&nbsp;&nbsp;<em>accounting</em></a></li>
<li><a class="reference internal" href="#admission-rules" id="id33">5.2&nbsp;&nbsp;&nbsp;<em>admission_rules</em></a></li>
<li><a class="reference internal" href="#event-logs" id="id34">5.3&nbsp;&nbsp;&nbsp;<em>event_logs</em></a></li>
<li><a class="reference internal" href="#event-log-hostnames" id="id35">5.4&nbsp;&nbsp;&nbsp;<em>event_log_hostnames</em></a></li>
<li><a class="reference internal" href="#files" id="id36">5.5&nbsp;&nbsp;&nbsp;<em>files</em></a></li>
<li><a class="reference internal" href="#frag-jobs" id="id37">5.6&nbsp;&nbsp;&nbsp;<em>frag_jobs</em></a></li>
<li><a class="reference internal" href="#gantt-jobs-resources" id="id38">5.7&nbsp;&nbsp;&nbsp;<em>gantt_jobs_resources</em></a></li>
<li><a class="reference internal" href="#gantt-jobs-resources-visu" id="id39">5.8&nbsp;&nbsp;&nbsp;<em>gantt_jobs_resources_visu</em></a></li>
<li><a class="reference internal" href="#gantt-jobs-predictions" id="id40">5.9&nbsp;&nbsp;&nbsp;<em>gantt_jobs_predictions</em></a></li>
<li><a class="reference internal" href="#gantt-jobs-predictions-visu" id="id41">5.10&nbsp;&nbsp;&nbsp;<em>gantt_jobs_predictions_visu</em></a></li>
<li><a class="reference internal" href="#jobs" id="id42">5.11&nbsp;&nbsp;&nbsp;<em>jobs</em></a></li>
<li><a class="reference internal" href="#job-dependencies" id="id43">5.12&nbsp;&nbsp;&nbsp;<em>job_dependencies</em></a></li>
<li><a class="reference internal" href="#moldable-job-descriptions" id="id44">5.13&nbsp;&nbsp;&nbsp;<em>moldable_job_descriptions</em></a></li>
<li><a class="reference internal" href="#job-resource-groups" id="id45">5.14&nbsp;&nbsp;&nbsp;<em>job_resource_groups</em></a></li>
<li><a class="reference internal" href="#job-resource-descriptions" id="id46">5.15&nbsp;&nbsp;&nbsp;<em>job_resource_descriptions</em></a></li>
<li><a class="reference internal" href="#job-state-logs" id="id47">5.16&nbsp;&nbsp;&nbsp;<em>job_state_logs</em></a></li>
<li><a class="reference internal" href="#job-types" id="id48">5.17&nbsp;&nbsp;&nbsp;<em>job_types</em></a></li>
<li><a class="reference internal" href="#resources" id="id49">5.18&nbsp;&nbsp;&nbsp;<em>resources</em></a></li>
<li><a class="reference internal" href="#resource-logs" id="id50">5.19&nbsp;&nbsp;&nbsp;<em>resource_logs</em></a></li>
<li><a class="reference internal" href="#assigned-resources" id="id51">5.20&nbsp;&nbsp;&nbsp;<em>assigned_resources</em></a></li>
<li><a class="reference internal" href="#queues" id="id52">5.21&nbsp;&nbsp;&nbsp;<em>queues</em></a></li>
<li><a class="reference internal" href="#challenges" id="id53">5.22&nbsp;&nbsp;&nbsp;<em>challenges</em></a></li>
</ul>
</li>
<li><a class="reference internal" href="#configuration-file" id="id54">6&nbsp;&nbsp;&nbsp;Configuration file</a></li>
<li><a class="reference internal" href="#modules-descriptions" id="id55">7&nbsp;&nbsp;&nbsp;Modules descriptions</a><ul class="auto-toc">
<li><a class="reference internal" href="#almighty" id="id56">7.1&nbsp;&nbsp;&nbsp;Almighty</a></li>
<li><a class="reference internal" href="#sarko" id="id57">7.2&nbsp;&nbsp;&nbsp;Sarko</a></li>
<li><a class="reference internal" href="#judas" id="id58">7.3&nbsp;&nbsp;&nbsp;Judas</a></li>
<li><a class="reference internal" href="#leon" id="id59">7.4&nbsp;&nbsp;&nbsp;Leon</a></li>
<li><a class="reference internal" href="#runner" id="id60">7.5&nbsp;&nbsp;&nbsp;Runner</a></li>
<li><a class="reference internal" href="#nodechangestate" id="id61">7.6&nbsp;&nbsp;&nbsp;NodeChangeState</a></li>
<li><a class="reference internal" href="#scheduler" id="id62">7.7&nbsp;&nbsp;&nbsp;Scheduler</a><ul class="auto-toc">
<li><a class="reference internal" href="#oar-sched-gantt-with-timesharing" id="id63">7.7.1&nbsp;&nbsp;&nbsp;oar_sched_gantt_with_timesharing</a></li>
<li><a class="reference internal" href="#oar-sched-gantt-with-timesharing-and-fairsharing" id="id64">7.7.2&nbsp;&nbsp;&nbsp;oar_sched_gantt_with_timesharing_and_fairsharing</a></li>
</ul>
</li>
<li><a class="reference internal" href="#hulot" id="id65">7.8&nbsp;&nbsp;&nbsp;Hulot</a></li>
</ul>
</li>
<li><a class="reference internal" href="#internal-mechanisms" id="id66">8&nbsp;&nbsp;&nbsp;Internal mechanisms</a><ul class="auto-toc">
<li><a class="reference internal" href="#job-execution" id="id67">8.1&nbsp;&nbsp;&nbsp;Job execution</a></li>
<li><a class="reference internal" href="#scheduling" id="id68">8.2&nbsp;&nbsp;&nbsp;Scheduling</a></li>
</ul>
</li>
<li><a class="reference internal" href="#faq-admin" id="id69">9&nbsp;&nbsp;&nbsp;FAQ - ADMIN</a><ul class="auto-toc">
<li><a class="reference internal" href="#release-policy" id="id70">9.1&nbsp;&nbsp;&nbsp;Release policy</a></li>
<li><a class="reference internal" href="#what-means-the-error-bad-configuration-option-permitlocalcommand-when-i-am-using-oarsh" id="id71">9.2&nbsp;&nbsp;&nbsp;What means the error &quot;Bad configuration option: PermitLocalCommand&quot; when I am using oarsh?</a></li>
<li><a class="reference internal" href="#how-to-manage-start-stop-of-the-nodes" id="id72">9.3&nbsp;&nbsp;&nbsp;How to manage start/stop of the nodes?</a></li>
<li><a class="reference internal" href="#how-can-i-manage-scheduling-queues" id="id73">9.4&nbsp;&nbsp;&nbsp;How can I manage scheduling queues?</a></li>
<li><a class="reference internal" href="#how-can-i-handle-licence-tokens" id="id74">9.5&nbsp;&nbsp;&nbsp;How can I handle licence tokens?</a></li>
<li><a class="reference internal" href="#how-can-i-handle-multiple-clusters-with-one-oar" id="id75">9.6&nbsp;&nbsp;&nbsp;How can I handle multiple clusters with one OAR?</a></li>
<li><a class="reference internal" href="#how-to-configure-a-more-ecological-cluster-or-how-to-make-some-power-consumption-economies" id="id76">9.7&nbsp;&nbsp;&nbsp;How to configure a more ecological cluster (or how to make some power consumption economies)?</a></li>
<li><a class="reference internal" href="#how-to-configure-temporary-uid-for-each-job" id="id77">9.8&nbsp;&nbsp;&nbsp;How to configure temporary UID for each job?</a></li>
<li><a class="reference internal" href="#how-to-enable-jobs-to-connect-to-the-frontales-from-the-nodes-using-oarsh" id="id78">9.9&nbsp;&nbsp;&nbsp;How to enable jobs to connect to the frontales from the nodes using oarsh?</a></li>
<li><a class="reference internal" href="#a-job-remains-in-the-finishing-state-what-can-i-do" id="id79">9.10&nbsp;&nbsp;&nbsp;A job remains in the &quot;Finishing&quot; state, what can I do?</a></li>
<li><a class="reference internal" href="#how-can-i-write-my-own-scheduler" id="id80">9.11&nbsp;&nbsp;&nbsp;How can I write my own scheduler?</a></li>
<li><a class="reference internal" href="#what-is-the-syntax-of-this-documentation" id="id81">9.12&nbsp;&nbsp;&nbsp;What is the syntax of this documentation?</a></li>
</ul>
</li>
<li><a class="reference internal" href="#oar-changelog" id="id82">10&nbsp;&nbsp;&nbsp;OAR CHANGELOG</a><ul class="auto-toc">
<li><a class="reference internal" href="#next-version" id="id83">10.1&nbsp;&nbsp;&nbsp;next version</a></li>
<li><a class="reference internal" href="#version-2-4-7" id="id84">10.2&nbsp;&nbsp;&nbsp;version 2.4.7:</a></li>
<li><a class="reference internal" href="#version-2-4-6" id="id85">10.3&nbsp;&nbsp;&nbsp;version 2.4.6:</a></li>
<li><a class="reference internal" href="#version-2-4-5" id="id86">10.4&nbsp;&nbsp;&nbsp;version 2.4.5:</a></li>
<li><a class="reference internal" href="#version-2-4-4" id="id87">10.5&nbsp;&nbsp;&nbsp;version 2.4.4:</a></li>
<li><a class="reference internal" href="#version-2-4-3" id="id88">10.6&nbsp;&nbsp;&nbsp;version 2.4.3:</a></li>
<li><a class="reference internal" href="#version-2-4-2" id="id89">10.7&nbsp;&nbsp;&nbsp;version 2.4.2:</a></li>
<li><a class="reference internal" href="#version-2-4-1" id="id90">10.8&nbsp;&nbsp;&nbsp;version 2.4.1:</a></li>
<li><a class="reference internal" href="#version-2-4-0" id="id91">10.9&nbsp;&nbsp;&nbsp;version 2.4.0:</a></li>
<li><a class="reference internal" href="#version-2-3-5" id="id92">10.10&nbsp;&nbsp;&nbsp;version 2.3.5:</a></li>
<li><a class="reference internal" href="#version-2-3-4" id="id93">10.11&nbsp;&nbsp;&nbsp;version 2.3.4:</a></li>
<li><a class="reference internal" href="#version-2-3-3" id="id94">10.12&nbsp;&nbsp;&nbsp;version 2.3.3:</a></li>
<li><a class="reference internal" href="#version-2-3-2" id="id95">10.13&nbsp;&nbsp;&nbsp;version 2.3.2:</a></li>
<li><a class="reference internal" href="#version-2-3-1" id="id96">10.14&nbsp;&nbsp;&nbsp;version 2.3.1:</a></li>
<li><a class="reference internal" href="#version-2-2-12" id="id97">10.15&nbsp;&nbsp;&nbsp;version 2.2.12:</a></li>
<li><a class="reference internal" href="#version-2-2-11" id="id98">10.16&nbsp;&nbsp;&nbsp;version 2.2.11:</a></li>
<li><a class="reference internal" href="#version-2-2-10" id="id99">10.17&nbsp;&nbsp;&nbsp;version 2.2.10:</a></li>
<li><a class="reference internal" href="#version-2-2-9" id="id100">10.18&nbsp;&nbsp;&nbsp;version 2.2.9:</a></li>
<li><a class="reference internal" href="#version-2-2-8" id="id101">10.19&nbsp;&nbsp;&nbsp;version 2.2.8:</a></li>
<li><a class="reference internal" href="#version-2-2-7" id="id102">10.20&nbsp;&nbsp;&nbsp;version 2.2.7:</a></li>
<li><a class="reference internal" href="#id1" id="id103">10.21&nbsp;&nbsp;&nbsp;version 2.2.11:</a></li>
<li><a class="reference internal" href="#id2" id="id104">10.22&nbsp;&nbsp;&nbsp;version 2.2.10:</a></li>
<li><a class="reference internal" href="#id3" id="id105">10.23&nbsp;&nbsp;&nbsp;version 2.2.9:</a></li>
<li><a class="reference internal" href="#id4" id="id106">10.24&nbsp;&nbsp;&nbsp;version 2.2.8:</a></li>
<li><a class="reference internal" href="#id5" id="id107">10.25&nbsp;&nbsp;&nbsp;version 2.2.7:</a></li>
<li><a class="reference internal" href="#version-2-2-6" id="id108">10.26&nbsp;&nbsp;&nbsp;version 2.2.6:</a></li>
<li><a class="reference internal" href="#version-2-2-5" id="id109">10.27&nbsp;&nbsp;&nbsp;version 2.2.5:</a></li>
<li><a class="reference internal" href="#version-2-2-4" id="id110">10.28&nbsp;&nbsp;&nbsp;version 2.2.4:</a></li>
<li><a class="reference internal" href="#version-2-2-3" id="id111">10.29&nbsp;&nbsp;&nbsp;version 2.2.3:</a></li>
<li><a class="reference internal" href="#version-2-2-2" id="id112">10.30&nbsp;&nbsp;&nbsp;version 2.2.2:</a></li>
<li><a class="reference internal" href="#version-2-2-1" id="id113">10.31&nbsp;&nbsp;&nbsp;version 2.2.1:</a></li>
<li><a class="reference internal" href="#version-2-2" id="id114">10.32&nbsp;&nbsp;&nbsp;version 2.2:</a></li>
<li><a class="reference internal" href="#version-2-1-0" id="id115">10.33&nbsp;&nbsp;&nbsp;version 2.1.0:</a></li>
<li><a class="reference internal" href="#version-2-0-2" id="id116">10.34&nbsp;&nbsp;&nbsp;version 2.0.2:</a></li>
<li><a class="reference internal" href="#version-2-0-0" id="id117">10.35&nbsp;&nbsp;&nbsp;version 2.0.0:</a></li>
</ul>
</li>
</ul>
</div>
<hr class="docutils" />
<div class="section" id="oar-capabilities">
<h1><a class="toc-backref" href="#id6">1&nbsp;&nbsp;&nbsp;OAR capabilities</a></h1>
<p>Oar is an opensource batch scheduler which provides a simple and flexible
exploitation of a cluster.</p>
<p>It manages resources of clusters as a traditional batch scheduler
(as PBS / Torque / LSF / SGE). In other words, it doesn't execute your job on
the resources but manages them (reservation, acces granting) in order to allow
you to connect these resources and use them.</p>
<dl class="docutils">
<dt>Its design is based on high level tools:</dt>
<dd><ul class="first last simple">
<li>relational database engine MySQL or PostgreSQL,</li>
<li>scripting language Perl,</li>
<li>confinement system mechanism cpuset,</li>
<li>scalable exploiting tool Taktuk.</li>
</ul>
</dd>
</dl>
<p>It is flexible enough to be suitable for production clusters and research
experiments.
It currently manages over than 5000 nodes and has executed more than 5 million
jobs.</p>
<dl class="docutils">
<dt>OAR advantages:</dt>
<dd><ul class="first last simple">
<li>No specific daemon on nodes.</li>
<li>No dependence on specific computing libraries like MPI. We support all
sort of parallel user applications.</li>
<li>Upgrades are made on the servers, nothing to do on computing nodes.</li>
<li>CPUSET (2.6 linux kernel) integration which restricts the jobs on
assigned resources (also useful to clean completely a job, even
parallel jobs).</li>
<li>All administration tasks are performed with the taktuk command (a large
scale remote execution deployment): <a class="reference external" href="http://taktuk.gforge.inria.fr/">http://taktuk.gforge.inria.fr/</a>.</li>
<li>Hierarchical resource requests (handle heterogeneous clusters).</li>
<li>Gantt scheduling (so you can visualize the internal scheduler decisions).</li>
<li>Full or partial time-sharing.</li>
<li>Checkpoint/resubmit.</li>
<li>Licences servers management support.</li>
<li>Best effort jobs : if another job wants the same resources then it is
deleted automatically (useful to execute programs like <em>SETI&#64;home</em>).</li>
<li>Environment deployment support (Kadeploy):
<a class="reference external" href="http://kadeploy.imag.fr/">http://kadeploy.imag.fr/</a>.</li>
</ul>
</dd>
<dt>Other more <em>common</em> features:</dt>
<dd><ul class="first last simple">
<li>Batch and Interactive jobs.</li>
<li>Admission rules.</li>
<li>Walltime.</li>
<li>Multi-schedulers support.</li>
<li>Multi-queues with priority.</li>
<li>Backfilling.</li>
<li>First-Fit Scheduler.</li>
<li>Reservation.</li>
<li>Support of moldable tasks.</li>
<li>Check compute nodes.</li>
<li>Epilogue/Prologue scripts.</li>
<li>Support of dynamic nodes.</li>
<li>Logging/Accounting.</li>
<li>Suspend/resume jobs.</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="installing-the-oar-batch-system">
<h1><a class="toc-backref" href="#id7">2&nbsp;&nbsp;&nbsp;Installing the OAR batch system</a></h1>
<dl class="docutils">
<dt>What do you need?</dt>
<dd><ul class="first last simple">
<li>a cluster</li>
<li>to be an admin of this cluster</li>
<li>to get the install package of OAR (normally you have already done that)</li>
</ul>
</dd>
</dl>
<div class="section" id="requirements">
<h2><a class="toc-backref" href="#id8">2.1&nbsp;&nbsp;&nbsp;Requirements</a></h2>
<p>There a three kinds of nodes, each requiring a specific software configuration.</p>
<p>These are :</p>
<blockquote>
<ul class="simple">
<li>the server node, which will hold all of OAR &quot;smartness&quot; ;</li>
<li>the login nodes, on which you will be allowed to login, then reserve some
computational nodes ;</li>
<li>the computational nodes (a.k.a. the nodes), on which the jobs will run.</li>
</ul>
</blockquote>
<p>On every nodes (server, login, computational), the following packages must be
installed :</p>
<blockquote>
<ul class="simple">
<li>Perl</li>
<li>Perl-base</li>
<li>openssh (server and client)</li>
</ul>
</blockquote>
<p>On the OAR server and on the login nodes, the following packages must be
installed:</p>
<blockquote>
<ul class="simple">
<li>Perl-Mysql | Perl-PostgreSQL</li>
<li>Perl-DBI</li>
<li>MySQL | PostgreSQL</li>
<li>libmysql | libpostgres</li>
</ul>
</blockquote>
<p>From now on, we will suppose all the packages are correctly installed and
configured and the database is started.</p>
</div>
<div class="section" id="configuration-of-the-cluster">
<h2><a class="toc-backref" href="#id9">2.2&nbsp;&nbsp;&nbsp;Configuration of the cluster</a></h2>
<p>The following steps have to be done, prior to installing OAR:</p>
<blockquote>
<ul>
<li><p class="first">add a user named &quot;oar&quot; in the group &quot;oar&quot; on every node</p>
</li>
<li><p class="first">let the user &quot;oar&quot; connect through ssh from any node to any node WITHOUT
password. To achieve this, here is some standard procedure for OpenSSH:</p>
<blockquote>
<ul>
<li><p class="first">create a set of ssh keys for the user &quot;oar&quot; with ssh-keygen (for
instance 'id_dsa.pub' and 'id_dsa')</p>
</li>
<li><p class="first">copy these keys on each node of the cluster in the &quot;.ssh&quot; folder of
the user &quot;oar&quot;</p>
</li>
<li><p class="first">append the contents of 'id_dsa.pub' to the file
&quot;~/.ssh/authorized_keys&quot;</p>
</li>
<li><p class="first">the default oar ssh public key in the authorized_keys file must be
tagged for the security. So this prefix must be set in front of
the public key:</p>
<pre class="literal-block">
environment=&quot;OAR_KEY=1&quot;
</pre>
<p>So if the oar public key is:</p>
<pre class="literal-block">
ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAsatv3+4HjaP91oLdZu68JVvYcHKl/u5avb4b
zkc3ut3W6FXz5qZYknDW99/R7VYaaZ+VFG5vt6ZCZvJReyM268p00D00ic4fuDwZADpgZMPW
FOGHJM5ga8cTPaczg88XMUx/cVGfnm1LaK5nSrymHZdMsxXr
</pre>
<p>then it must be switched into:</p>
<pre class="literal-block">
environment=&quot;OAR_KEY=1&quot; ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAsatv3+4HjaP9
1oLdZu68JVvYcHKl/u5avb4bzkc3ut3W6FXz5qZYknDW99/R7VYaaZ+VFG5vt6ZCZvJReyM2
68p00D00ic4fuDwZADpgZMPWFOGHJM5ga8cTPaczg88XMUx/cVGfnm1LaK5nSrymHZdMsxXr
</pre>
</li>
<li><p class="first">in &quot;~/.ssh/config&quot; add the lines:</p>
<pre class="literal-block">
Host *
    ForwardX11 no
    StrictHostKeyChecking no
    PasswordAuthentication no
    AddressFamily inet
</pre>
</li>
<li><p class="first">test the ssh connection between (every) two nodes : there should not
be any prompt.</p>
</li>
</ul>
</blockquote>
</li>
</ul>
</blockquote>
<p>There are three different flavors of installation :</p>
<blockquote>
<ul>
<li><p class="first">server:  install the daemon which must be running on the server</p>
</li>
<li><p class="first">user:    install all the tools needed to submit and manage jobs for the
users (oarsub, oarstat, oarnodes, ...)</p>
</li>
<li><dl class="first docutils">
<dt>node:    install the tools for a computing node (check that the oar</dt>
<dd><p class="first last">user ssh key is prefixed by environment=&quot;OAR_KEY=1&quot;, see
<a class="reference internal" href="#important-notes">Important notes</a>)</p>
</dd>
</dl>
</li>
</ul>
</blockquote>
<p>The installation is straightforward:</p>
<blockquote>
<ul>
<li><p class="first">become root</p>
</li>
<li><p class="first">go to OAR source repository</p>
</li>
<li><p class="first">You can set Makefile variables in the command line to suit your configuration
(change &quot;OARHOMEDIR&quot; to the home of your user oar and &quot;PREFIX&quot; where you want
to copy all OAR files).</p>
</li>
<li><dl class="first docutils">
<dt>run make  &lt;module&gt; [module] ...</dt>
<dd><dl class="first last docutils">
<dt>where module := { server-install | user-install | node-install | doc-install | debian-package }</dt>
<dd><p class="first last">OPTIONS := { OARHOMEDIR | OARCONFDIR | OARUSER | PREFIX | MANDIR | OARDIR | BINDIR | SBINDIR | DOCDIR }</p>
</dd>
</dl>
</dd>
</dl>
</li>
<li><p class="first">Edit /etc/oar/oar.conf file to match your cluster configuration.</p>
</li>
<li><p class="first">Make sure that the PATH environment variable contains $PREFIX/$BINDIR of
your installation (default is /usr/local/bin).</p>
</li>
</ul>
</blockquote>
<p>Initialization of OAR database (MySQL) is achieved using oar_mysql_db_init
script provided with the server module installation and located in $PREFIX/sbin
(/usr/local/sbin in default Makefile).</p>
<p>If you want to use a postgres SQL server then you can call the oar_psql_db_init.pl
script that will do all the users and tables creation for you.
If you want to do this by yourself, you have to add a  new user which can
connect on a new oar database (use the commands <em>createdb</em> and <em>createuser</em>).
After that, you have to authorize network connections on the postgresql server
in the <em>postgresql.conf</em> (uncomment <em>tcpip_socket = true</em>).  Then you can import
the database scheme stored in <em>oar_postgres.sql</em> (use <em>psql</em> and the SQL command
&quot;\i&quot;).</p>
<p>Here is an example to perform all the potgres database install(there is
certainly other ways to do that):</p>
<pre class="literal-block">
sudo su - postgres

createuser -P
    Enter name of role to add: oar
    Enter password for new role:
    Enter it again:
    Shall the new role be a superuser? (y/n) n
    Shall the new role be allowed to create databases? (y/n) n
    Shall the new role be allowed to create more new roles? (y/n) n
    CREATE ROLE

createuser -P
    Enter name of role to add: oar_ro
    Enter password for new role:
    Enter it again:
    Shall the new role be a superuser? (y/n) n
    Shall the new role be allowed to create databases? (y/n) n
    Shall the new

createdb oar

sudo vi /etc/postgresql/8.1/main/pg_hba.conf
    host    oar         oar_ro            127.0.0.1          255.255.255.255    md5
    host    oar         oar               127.0.0.1          255.255.255.255    md5
# Be careful to put these two lines at the top of the file or it won't work

sudo /etc/init.d/postgresql-8.1 reload

psql -Uoar -h127.0.0.1 oar
    \i /usr/lib/oar/pg_structure.sql
    \i /usr/lib/oar/pg_default_admission_rules.sql
    \i /usr/lib/oar/default_data.sql
    \q

psql oar
    GRANT ALL PRIVILEGES ON schema,accounting,admission_rules,assigned_resources,
    challenges,event_log_hostnames,event_logs,files,frag_jobs,gantt_jobs_predictions,
    gantt_jobs_predictions_visu,gantt_jobs_resources,gantt_jobs_resources_visu,
    job_dependencies,job_resource_descriptions,job_resource_groups,
    job_state_logs,job_types,jobs,moldable_job_descriptions,queues,
    resource_logs,resources,admission_rules_id_seq,event_logs_event_id_seq,
    files_file_id_seq,job_resource_groups_res_group_id_seq,
    job_state_logs_job_state_log_id_seq,job_types_job_type_id_seq,
    moldable_job_descriptions_moldable_id_seq,resource_logs_resource_log_id_seq,
    resources_resource_id_seq,jobs_job_id_seq TO oar;

    GRANT SELECT ON schema,accounting,admission_rules,assigned_resources,event_log_hostnames,
    event_logs,files,frag_jobs,gantt_jobs_predictions,gantt_jobs_predictions_visu,
    gantt_jobs_resources,gantt_jobs_resources_visu,job_dependencies,
    job_resource_descriptions,job_resource_groups,job_state_logs,job_types,
    jobs,moldable_job_descriptions,queues,resource_logs,resources,admission_rules_id_seq,
    event_logs_event_id_seq,files_file_id_seq,job_resource_groups_res_group_id_seq,
    job_state_logs_job_state_log_id_seq,job_types_job_type_id_seq,
    moldable_job_descriptions_moldable_id_seq,resource_logs_resource_log_id_seq,
    resources_resource_id_seq,jobs_job_id_seq TO oar_ro;
    \q

# You can test it with
psql oar oar_ro -h127.0.0.1
</pre>
<p>IMPORTANT: be sure to activate the &quot;autovacuum&quot; feature in the
&quot;postgresql.conf&quot; file (OAR creates and deletes a lot of records and this
setting cleans the postgres database from unneeded records).
Better performances are achieved by adding the vacuum into the crontab of the
postgres user like this (&quot;crontab -e&quot; to edit and add the line)</p>
<pre class="literal-block">
postgres$ crontab -l
# m h  dom mon dow   command
02 02 * * * vacuumdb -a -f -z
</pre>
<p>For more information about postgresql, go to <a class="reference external" href="http://www.postgresql.org/">http://www.postgresql.org/</a>.</p>
<p><strong>Security issue</strong>: For security reasons it is hardly <strong>recommended</strong> to
configure a read only account for the OAR database (like the above example).
Thus you will be able to add this data in <a class="reference internal" href="#db-base-login-ro">DB_BASE_LOGIN_RO</a> and
<a class="reference internal" href="#db-base-passwd-ro">DB_BASE_PASSWD_RO</a> in <em>oar.conf</em>.</p>
<p>Note: The same machine may host several or even all modules.</p>
<p>Note about X11: The easiest and scalable way to use X11 application on cluster
nodes is to open X11 ports and set the right DISPLAY environment variable by
hand.  Otherwise users can use X11 forwarding via ssh to access cluster
frontal. After that you must configure ssh server on this frontal with</p>
<pre class="literal-block">
X11Forwarding yes
X11UseLocalhost no
</pre>
<p>With this configuration, users can launch X11 applications after a 'oarsub -I'
on the given node.</p>
</div>
<div class="section" id="cpuset-installation">
<h2><a class="toc-backref" href="#id10">2.3&nbsp;&nbsp;&nbsp;CPUSET installation</a></h2>

<div class="section" id="what-are-oarsh-and-oarsh-shell-scripts">
<h3><a class="toc-backref" href="#id11">2.3.1&nbsp;&nbsp;&nbsp;What are &quot;oarsh&quot; and &quot;oarsh_shell&quot; scripts ?</a></h3>
<p>&quot;oarsh&quot; and &quot;oarsh_shell&quot; are two scripts that can restrict user processes to
stay in the same cpuset on all nodes.</p>
<p>This feature is very usefull to restrict processor consumption on
multiprocessors computers and to kill all processes of a same
OAR job on several nodes.</p>
<p>If you want to configure this feature into OAR then take a look also in CPUSET
and <a class="reference internal" href="#resources">resources</a>.</p>
</div>
<div class="section" id="cpuset-definition">
<h3><a class="toc-backref" href="#id12">2.3.2&nbsp;&nbsp;&nbsp;CPUSET definition</a></h3>
<p>CPUSET is a module integrated in the Linux kernel since 2.6.x.
In the kernel documentation, you can read:</p>
<pre class="literal-block">
Cpusets provide a mechanism for assigning a set of CPUs and Memory
Nodes to a set of tasks.

Cpusets constrain the CPU and Memory placement of tasks to only
the resources within a tasks current cpuset.  They form a nested
hierarchy visible in a virtual file system.  These are the essential
hooks, beyond what is already present, required to manage dynamic
job placement on large systems.

Each task has a pointer to a cpuset.  Multiple tasks may reference
the same cpuset.  Requests by a task, using the sched_setaffinity(2)
system call to include CPUs in its CPU affinity mask, and using the
mbind(2) and set_mempolicy(2) system calls to include Memory Nodes
in its memory policy, are both filtered through that tasks cpuset,
filtering out any CPUs or Memory Nodes not in that cpuset.  The
scheduler will not schedule a task on a CPU that is not allowed in
its cpus_allowed vector, and the kernel page allocator will not
allocate a page on a node that is not allowed in the requesting tasks
mems_allowed vector.

If a cpuset is cpu or mem exclusive, no other cpuset, other than a direct
ancestor or descendent, may share any of the same CPUs or Memory Nodes.
A cpuset that is cpu exclusive has a sched domain associated with it.
The sched domain consists of all cpus in the current cpuset that are not
part of any exclusive child cpusets.
This ensures that the scheduler load balacing code only balances
against the cpus that are in the sched domain as defined above and not
all of the cpus in the system. This removes any overhead due to
load balancing code trying to pull tasks outside of the cpu exclusive
cpuset only to be prevented by the tasks' cpus_allowed mask.

A cpuset that is mem_exclusive restricts kernel allocations for
page, buffer and other data commonly shared by the kernel across
multiple users.  All cpusets, whether mem_exclusive or not, restrict
allocations of memory for user space.  This enables configuring a
system so that several independent jobs can share common kernel
data, such as file system pages, while isolating each jobs user
allocation in its own cpuset.  To do this, construct a large
mem_exclusive cpuset to hold all the jobs, and construct child,
non-mem_exclusive cpusets for each individual job.  Only a small
amount of typical kernel memory, such as requests from interrupt
handlers, is allowed to be taken outside even a mem_exclusive cpuset.

User level code may create and destroy cpusets by name in the cpuset
virtual file system, manage the attributes and permissions of these
cpusets and which CPUs and Memory Nodes are assigned to each cpuset,
specify and query to which cpuset a task is assigned, and list the
task pids assigned to a cpuset.
</pre>
</div>
<div class="section" id="oarsh">
<h3><a class="toc-backref" href="#id13">2.3.3&nbsp;&nbsp;&nbsp;OARSH</a></h3>
<p>&quot;oarsh&quot; is a wrapper around the &quot;ssh&quot; command (tested with openSSH).
Its goal is to propagate two environment variables:</p>
<blockquote>
<ul class="simple">
<li>OAR_CPUSET : The name of the OAR job cpuset</li>
<li>OAR_JOB_USER : The name of the user corresponding to the job</li>
</ul>
</blockquote>
<p>So &quot;oarsh&quot; must be run by oar and a simple user must run it via the
&quot;sudowrapper&quot; script to become oar.
In this way each cluster user who can execute &quot;oarsh&quot; via &quot;sudowrapper&quot; can
connect himself on each cluster nodes (if oarsh is installed everywhere).</p>
</div>
<div class="section" id="oarsh-shell">
<h3><a class="toc-backref" href="#id14">2.3.4&nbsp;&nbsp;&nbsp;OARSH_SHELL</a></h3>
<p>&quot;oarsh_shell&quot; must be the shell of the oar user on each nodes where you want
oarsh to work.
This script takes &quot;OAR_CPUSET&quot; and &quot;OAR_JOB_USER&quot; environment variables and
adds its PID in OAR_CPUSET cpuset. Then it searches user shell and home and
executes the right command (like ssh).</p>
</div>
<div class="section" id="important-notes">
<h3><a class="toc-backref" href="#id15">2.3.5&nbsp;&nbsp;&nbsp;Important notes</a></h3>
<blockquote>
<ul>
<li><p class="first">On each node you must add in the SSH server configuration file (you
have to install an openssh server with a version &gt;= <strong>3.9</strong>):</p>
<pre class="literal-block">
AcceptEnv OAR_CPUSET OAR_JOB_USER
PermitUserEnvironment yes
UseLogin no
AllowUsers oar
</pre>
<p>In Debian the file is &quot;/etc/ssh/sshd_config&quot;.</p>
<p><em>AllowUsers</em> restricts the users which can connect directly on the
nodes. With cpuset enabled, only the user oar is needed but other logins
can be added with this syntax(it is safer):</p>
<pre class="literal-block">
AllowUsers oar admin1 admin2 ...
</pre>
<p>After that you have to restart the SSH server.</p>
</li>
<li><p class="first">the command &quot;scp&quot; can be used with oarsh. The syntax is:</p>
<pre class="literal-block">
scp -S /path/to/oarsh ...
</pre>
</li>
<li><p class="first">If you want to use oarsh from the user frontale, you can. You have to
define the environment OAR_JOB_ID and then launch oarsh on a node used
by your OAR job. This feature works only where the oarstat command is
configured:</p>
<pre class="literal-block">
OAR_JOB_ID=42 oarsh node12
</pre>
<p>or:</p>
<pre class="literal-block">
export OAR_JOB_ID=42
oarsh node12
</pre>
<p>This command gives you a shell on the &quot;node12&quot; from the OAR job 42.</p>
<p>You can also copy files with a syntax like:</p>
<pre class="literal-block">
OAR_JOB_ID=42 scp -S /path/to/oarsh ...
</pre>
</li>
<li><p class="first">You can restrict the use of oarsh with the sudo configuration:</p>
<pre class="literal-block">
%oarsh ALL=(oar) NOPASSWD: /path/to/oarsh
</pre>
<p>Here only users from oarsh group can execute oarsh</p>
</li>
<li><p class="first">You can disable the cpuset security mechanism by setting the
OARSH_BYPASS_WHOLE_SECURITY field to 1 in your oar.conf file.
WARNING: this is a critical functionality (this is only useful
if users want to have a command to connect on every nodes without
taking care of their ssh configuration and act like a ssh).</p>
</li>
</ul>
</blockquote>
</div>
</div>
<div class="section" id="using-taktuk">
<h2><a class="toc-backref" href="#id16">2.4&nbsp;&nbsp;&nbsp;Using Taktuk</a></h2>
<p>If you want to use taktuk to manage remote admnistration commands, you have to
install it. You can find information about taktuk from its website:
<a class="reference external" href="http://taktuk.gforge.inria.fr">http://taktuk.gforge.inria.fr</a>.
Then, you have to edit your oar configuration file and to fill in the different
related parameters:</p>
<blockquote>
<ul class="simple">
<li>TAKTUK_CMD (the path to the taktuk command)</li>
<li>PINGCHECKER_TAKTUK_ARG_COMMAND (the command used to check resources states)</li>
<li>SCHEDULER_NODE_MANAGER_SLEEP_CMD (command used for halting nodes)</li>
</ul>
</blockquote>
</div>
<div class="section" id="visualization-tools-installation">
<h2><a class="toc-backref" href="#id17">2.5&nbsp;&nbsp;&nbsp;Visualization tools installation</a></h2>
<p>There are two different tools. One, named Monika, displays the current cluster
state with all active and waiting jobs. The other, named drawgantt, displays
node occupation in a lapse of time.  These tools are CGI scripts and generate
HTML pages.</p>
<dl class="docutils">
<dt>You can install these in this way:</dt>
<dd><p class="first">drawgantt:</p>
<blockquote>
<ul class="simple">
<li>Make sure you installed &quot;ruby&quot;, &quot;libdbd-mysql-ruby&quot; or
&quot;libdbd-pg-ruby&quot; and &quot;libgd-ruby1.8&quot; packages.</li>
<li>Copy &quot;drawgantt.cgi&quot; and &quot;drawgantt.conf&quot; in the CGI folder of your
web server
(ex: /usr/lib/cgi-bin/ for Debian).</li>
<li>Copy all icons and javascript files in a folder that web server can
find them
(ex: /var/www/oar/Icons and /var/www/oar/Icons).</li>
<li>Make sure that these files can be read by the web server user.</li>
<li>Edit &quot;drawgantt.conf&quot; and change tags to fit your configuration.</li>
</ul>
</blockquote>
<p>Monika:</p>
<blockquote class="last">
<ul class="simple">
<li>The packages &quot;libdbd-mysql-perl&quot; or
&quot;libdbd-pg-perl&quot; and &quot;perl-AppConfig&quot; are required.</li>
<li>Read INSTALL file in the monika repository.</li>
</ul>
</blockquote>
</dd>
</dl>
</div>
<div class="section" id="debian-packages">
<h2><a class="toc-backref" href="#id18">2.6&nbsp;&nbsp;&nbsp;Debian packages</a></h2>
<p>OAR is also released under Debian packages (or Ubuntu). You can find them at
<a class="reference external" href="https://gforge.inria.fr/frs/?group_id=125">https://gforge.inria.fr/frs/?group_id=125</a>.</p>
<p>If you want to add it as a new source in your /etc/apt/sources.list then add
the line:</p>
<pre class="literal-block">
deb http://oar.imag.fr/download ./
</pre>

<p><strong>IMPORTANT</strong> : if you want to use the cpuset features then you have to install
the oar-node package on computing nodes otherwise this is not mandatory.  But
if this is performed then the configuration of <a class="reference internal" href="#important-notes">Important notes</a> must be set
on these nodes.</p>
<p>After installing packages, you have to edit the <a class="reference internal" href="#configuration-file">configuration file</a> on the
server, submission nodes and computing nodes to fit your needs.</p>
</div>
<div class="section" id="starting">
<h2><a class="toc-backref" href="#id19">2.7&nbsp;&nbsp;&nbsp;Starting</a></h2>
<p>First, you must start OAR daemon on the server (its name is &quot;Almighty&quot;).</p>
<blockquote>
<ul class="simple">
<li>if you have installed OAR from sources, become root user and launch
command &quot;Almighty&quot; (it stands in $PREFIX/sbin).</li>
<li>if you have installed OAR from Debian packages, use the script
&quot;/etc/init.d/oar-server&quot; to start the daemon.</li>
</ul>
</blockquote>
<p>Then you have to insert new resources in the database via the command
<a class="reference internal" href="#oarnodesetting">oarnodesetting</a>.</p>
<p>If you want to <strong>automatically</strong> initialize your cluster then you just need to
launch <em>oar_resources_init</em>. It will detect the resources from the nodes that
you put in a file and store right OAR commands to initialize the database with
the appropriate values for the memory and the cpuset properties. Just try...</p>
<p>A tool is now available to help you managing your oar resources and admission
rules : oaradmin.
Take a look at the oaradmin documentation in the administrator commands section
for more details.</p>
</div>
<div class="section" id="energy-saving">
<h2><a class="toc-backref" href="#id20">2.8&nbsp;&nbsp;&nbsp;Energy saving</a></h2>
<p>Starting with version 2.4.3, OAR provides a module responsible of advanced
management of wake-up/shut-down of nodes when they are not used.
To activate this feature, you have to:</p>
<blockquote>
<ul class="simple">
<li>provide 2 commands or scripts that are to be executed on the oar server
to shutdown (or set into standby) some nodes and to wake-up some nodes
(configure the path of those commands into the ENERGY_SAVING_NODE_MANAGER_WAKE_UP_CMD
and ENERGY_SAVING_NODE_MANAGER_SHUT_DOWN_CMD variables into oar.conf)</li>
<li>configure the available_upto property of all your nodes:<ul>
<li>available_upto=0           : to disable the wake-up and halt</li>
<li>available_upto=1           : to disable the wake-up (but not the halt)</li>
<li>available_upto=2147483647  : to disable the halt (but not the wake-up)</li>
<li>available_upto=2147483646  : to enable wake-up/halt forever</li>
<li>available_upto=&lt;timestamp&gt; : to enable the halt, and the wake-up until
the date given by &lt;timestamp&gt;</li>
</ul>
</li>
<li>activate the energy saving module by setting ENERGY_SAVING_INTERNAL=&quot;yes&quot;
and configuring the ENERGY_* variables into oar.conf</li>
<li>configure the metascheduler time values into SCHEDULER_NODE_MANAGER_IDLE_TIME,
SCHEDULER_NODE_MANAGER_SLEEP_TIME and SCHEDULER_NODE_MANAGER_WAKEUP_TIME
variables of the oar.conf file.</li>
<li>restart the oar server (you should see an &quot;Almighty&quot; process more).</li>
</ul>
</blockquote>
<p>You need to restart OAR each time you change an ENERGY_* variable.
More informations are available inside the oar.conf file itself. For more
details about the mechanism, take a look at the &quot;Hulot&quot; module documentation.</p>
</div>
<div class="section" id="further-informations">
<h2><a class="toc-backref" href="#id21">2.9&nbsp;&nbsp;&nbsp;Further informations</a></h2>
<p>For further information, please check <a class="reference external" href="http://oar.imag.fr/">http://oar.imag.fr/</a>.</p>
</div>
</div>
<div class="section" id="security-aspects-in-oar">
<h1><a class="toc-backref" href="#id22">3&nbsp;&nbsp;&nbsp;Security aspects in OAR</a></h1>
<p>In OAR2, security and user switching is managed by the &quot;oardodo&quot; script.
It is a suid script executable only by root and the oar group members that
is used to launch a command, a terminal or a script with
the privileges of a particular user.
When &quot;oardodo&quot; is called, it checks the value of an environment variable:
OARDO_BECOME_USER.</p>
<blockquote>
<ul class="simple">
<li>If this variable is empty, &quot;oardodo&quot; will execute the command with the
privileges of the superuser (root).</li>
<li>Else, this variable contains the name of the user that will be used to
execute the command.</li>
</ul>
</blockquote>
<p>Here are the scripts/modules where &quot;oardodo&quot; is called and which user is used
during this call:</p>
<blockquote>
<ul>
<li><dl class="first docutils">
<dt>oar_Judas:</dt>
<dd><p class="first last">this module is used for logging and notification.</p>
</dd>
</dl>
<ul class="simple">
<li>user notification: email or command execution.OARDO_BECOME_USER = user</li>
</ul>
</li>
<li><dl class="first docutils">
<dt>oarsub:</dt>
<dd><p class="first last">this script is used for submitting jobs or reservations.</p>
</dd>
</dl>
<ul>
<li><p class="first">read user script</p>
</li>
<li><p class="first">connection to the job and the remote shell</p>
</li>
<li><p class="first">keys management</p>
</li>
<li><p class="first">job key export</p>
<blockquote>
<p>for all these functions, the user used in the OARDO_BECOME_USER variable is
the user that submits the job.</p>
</blockquote>
</li>
</ul>
</li>
<li><dl class="first docutils">
<dt>pingchecker:</dt>
<dd><p class="first last">this module is used to check resources health. Here, the user is root.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>oarexec:</dt>
<dd><p class="first last">executed on the first reserved node, oarexec executes the job prologue and
initiate the job.</p>
</dd>
</dl>
<ul class="simple">
<li>the &quot;clean&quot; method kills every oarsub connection process in superuser mode</li>
<li>&quot;kill_children&quot; method kills every child of the process in superuser mode</li>
<li>execution of a passive job in user mode</li>
<li>getting of the user shell in user mode</li>
<li>checkpointing in superuser mode</li>
</ul>
</li>
<li><dl class="first docutils">
<dt>job_resource_manager:</dt>
<dd><p class="first last">The job_resource_manager script is a perl script that oar server deploys on
nodes to manage cpusets, users, job keys...</p>
</dd>
</dl>
<ul class="simple">
<li>cpuset creation and clean is executed in superuser mode</li>
</ul>
</li>
<li><dl class="first docutils">
<dt>oarsh_shell:</dt>
<dd><p class="first last">shell program used with the oarsh script. It adds its own process in the
cpuset and launches the shell or the script of the user.</p>
</dd>
</dl>
<ul class="simple">
<li>cpuset filling, &quot;nice&quot; and display management are executed as root.</li>
<li>TTY login is executed as user.</li>
</ul>
</li>
<li><dl class="first docutils">
<dt>oarsh:</dt>
<dd><p class="first last">oar's ssh wrapper to connect from node to node. It contains all the context
variables usefull for this connection.</p>
</dd>
</dl>
<ul>
<li><dl class="first docutils">
<dt>display management and connection with a user job key file are executed</dt>
<dd><p class="first last">as user.</p>
</dd>
</dl>
</li>
</ul>
</li>
</ul>
</blockquote>
</div>
<div class="section" id="administrator-commands">
<h1><a class="toc-backref" href="#id23">4&nbsp;&nbsp;&nbsp;Administrator commands</a></h1>
<div class="section" id="oarproperty">
<h2><a class="toc-backref" href="#id24">4.1&nbsp;&nbsp;&nbsp;<em>oarproperty</em></a></h2>
<p>This command manages OAR resource properties stored in the database.</p>
<p>Options are:</p>
<pre class="literal-block">
-l : list properties
-a NAME : add a property
  -c : sql new field of type VARCHAR(255) (default is integer)
-d NAME : delete a property
-r &quot;OLD_NAME,NEW_NAME&quot; : rename property OLD_NAME into NEW_NAME
</pre>
<p>Examples:</p>
<pre class="literal-block">
# oarproperty -a cpu_freq
# oarproperty -a type
# oarproperty -r &quot;cpu_freq,freq&quot;
</pre>
</div>
<div class="section" id="oarnodesetting">
<h2><a class="toc-backref" href="#id25">4.2&nbsp;&nbsp;&nbsp;<em>oarnodesetting</em></a></h2>
<p>This command permits to change the state or a property of a node or of several
resources resources.</p>
<p>By default the node name used by <a class="reference internal" href="#oarnodesetting">oarnodesetting</a> is the result of the command
<em>hostname</em>.</p>
<p>Options are:</p>
<pre class="literal-block">
-a    : add a new resource
-s    : state to assign to the node:
        * &quot;Alive&quot; : a job can be run on the node.
        * &quot;Absent&quot; : administrator wants to remove the node from the pool
           for a moment.
        * &quot;Dead&quot; : the node will not be used and will be deleted.
-h    : specify the node name (override hostname).
-r    : specify the resource number
--sql : get resource identifiers which respond to the
        SQL where clause on the table jobs
        (ex: &quot;type = 'default'&quot;)
-p    : change the value of a property specified resources.
-n    : specify this option if you do not want to wait the end of jobs running
        on this node when you change its state into &quot;Absent&quot; or &quot;Dead&quot;.
</pre>
</div>
<div class="section" id="oaradmin">
<h2><a class="toc-backref" href="#id26">4.3&nbsp;&nbsp;&nbsp;<em>oaradmin</em></a></h2>
<p>This command permits to create resources and manage admission rules easily. An optional feature permits versioning changes in admission rules and conf files.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Requirements:</th><td class="field-body"></td>
</tr>
</tbody>
</table>
<p>For oaradmin, the following packages must be installed:</p>
<blockquote>
<ul class="simple">
<li>Perl-Yaml</li>
<li>Ruby 1.8 or greater</li>
<li>Ruby-Yaml</li>
<li>Ruby-DBI</li>
<li>Subversion for the optional versioning feature</li>
</ul>
</blockquote>
<p>Options for resources subcommand are:</p>
<pre class="literal-block">
-a, --add                        Add new resources
    --cpusetproperty=prop        Property name for cpuset numbers
-s, --select                     Select resources for update
-p, --property                   Set value for a property
-d, --delete                     Delete resources
-c, --commit                     Commit in oar database
</pre>
<p>Examples:</p>
<pre class="literal-block">
# oaradmin resources -a /node=mycluster{12}.domain/cpu={2}/core={2}
# oaradmin resources -a /node=mycluster-[1-250].domain/cpu={2}
# oaradmin resources -a /node=mycluster-[1-250].domain/cpu={2} -p memnode=1024 -p cpufreq=3.2 -p cputype=xeon
</pre>
<p>Options for rules subcommand are:</p>
<pre class="literal-block">
-l, --list                       List admission rules
-a, --add                        Add an admission rule
-f, --file                       File which contains script for admission rule
-d, --delete                     Delete admission rules
-x, --export                     Export admission rules
-e, --edit                       Edit an admission rule
-1, --enable                     Enable the admission rule (removing comments)
-0, --disable                    Disable the admission rule (commenting the code)
-H, --history                    Show all changes made on the admission rule
-R, --revert                     Revert to the admission rule as it existed in a revision number
</pre>
<p>Examples:</p>
<pre class="literal-block">
# oaradmin rules -l
# oaradmin rules -lll 3
# oaradmin rules -e 3
</pre>
<p>Options for conf subcommand are:</p>
<pre class="literal-block">
-e, --edit                       Edit the conf file
-H, --history                    Show all changes made on the conf file
-R, --revert                     Revert to the conf file as it existed in a revision number
</pre>
<p>Examples:</p>
<pre class="literal-block">
# oaradmin conf -e /etc/oar/oar.conf
# oaradmin conf -R /etc/oar/oar.conf 3
</pre>
</div>
<div class="section" id="oarremoveresource">
<h2><a class="toc-backref" href="#id27">4.4&nbsp;&nbsp;&nbsp;<em>oarremoveresource</em></a></h2>
<p>This command permits to remove a resource from the database.</p>
<p>The node must be in the state &quot;Dead&quot; (use <a class="reference internal" href="#oarnodesetting">oarnodesetting</a> to do this) and then
you can use this command to delete it.</p>
</div>
<div class="section" id="oaraccounting">
<h2><a class="toc-backref" href="#id28">4.5&nbsp;&nbsp;&nbsp;<em>oaraccounting</em></a></h2>
<p>This command permits to update the <a class="reference internal" href="#accounting">accounting</a> table for jobs ended since the
last launch.</p>
<p>Option &quot;--reinitialize&quot; removes everything in the <a class="reference internal" href="#accounting">accounting</a> table and
switches the &quot;accounted&quot; field of the table <a class="reference internal" href="#jobs">jobs</a> into &quot;NO&quot;. So when you will
launch the <a class="reference internal" href="#oaraccounting">oaraccounting</a> command again, it will take the whole jobs.</p>
<p>Option &quot;--delete_before&quot; removes records from the <a class="reference internal" href="#accounting">accounting</a> table that are
older than the amount of time specified. So if the table becomes too big you
can shrink old data; for example:</p>
<pre class="literal-block">
oaraccounting --delete_before 2678400
</pre>
<p>(Remove everything older than 31 days)</p>
</div>
<div class="section" id="oarnotify">
<h2><a class="toc-backref" href="#id29">4.6&nbsp;&nbsp;&nbsp;<em>oarnotify</em></a></h2>
<p>This command sends commands to the <a class="reference internal" href="#almighty">Almighty</a> module and manages scheduling
queues.</p>
<p>Option are:</p>
<pre class="literal-block">
    Almighty_tag    send this tag to the Almighty (default is TERM)
-e                  active an existing queue
-d                  inactive an existing queue
-E                  active all queues
-D                  inactive all queues
--add_queue         add a new queue; syntax is name,priority,scheduler
                    (ex: &quot;name,3,&quot;oar_sched_gantt_with_timesharing&quot;
--remove_queue      remove an existing queue
-l                  list all queues and there status
-h                  show this help screen
-v                  print OAR version number
</pre>
</div>
<div class="section" id="oarmonitor">
<h2><a class="toc-backref" href="#id30">4.7&nbsp;&nbsp;&nbsp;<em>oarmonitor</em></a></h2>
<p>This command collects monitoring data from compute nodes and stores them into
the database.</p>
<p>The <a class="reference internal" href="#taktuk-cmd">TAKTUK_CMD</a> is mandatory in the <em>oar.conf</em> and data comes from the sensor
file <a class="reference internal" href="#oarmonitor-sensor-file">OARMONITOR_SENSOR_FILE</a> (parse <em>/proc</em> filesystem for example) and print
it in the right way.</p>
<p>For example, the user &quot;oar&quot; or &quot;root&quot; can run the following command on the
server:</p>
<blockquote>
oarmonitor -j 4242 -f 10</blockquote>
<p>(Retrieve data from compute nodes of the job 4242 every 10 seconds and store
them into database tables monitoring_*)</p>
<p>For now, there is just a very minimalist command for the user to view these
data. It creates PNG images and a movie...</p>
<blockquote>
oarmonitor_graph_gen.pl -j 4242</blockquote>
<p>Then the user can look into the directory <em>OAR.1653.monitoring</em> in the current
directory.</p>
</div>
</div>
<div class="section" id="database-scheme">
<h1><a class="toc-backref" href="#id31">5&nbsp;&nbsp;&nbsp;Database scheme</a></h1>
<div class="figure">
<a class="reference external image-reference" href="../schemas/db_scheme.svg"><img alt="Database scheme" src="../schemas/db_scheme.png" /></a>
<p class="caption">Database scheme
(red lines seem PRIMARY KEY,
blue lines seem INDEX)</p>
</div>
<p>Note : all dates and duration are stored in an integer manner (number of
seconds since the EPOCH).</p>
<div class="section" id="accounting">
<h2><a class="toc-backref" href="#id32">5.1&nbsp;&nbsp;&nbsp;<em>accounting</em></a></h2>
<table border="1" class="docutils">
<colgroup>
<col width="23%" />
<col width="26%" />
<col width="51%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Fields</th>
<th class="head">Types</th>
<th class="head">Descriptions</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>window_start</td>
<td>INT UNSIGNED</td>
<td>start date of the accounting interval</td>
</tr>
<tr><td>window_stop</td>
<td>INT UNSIGNED</td>
<td>stop date of the accounting interval</td>
</tr>
<tr><td>accounting_user</td>
<td>VARCHAR(20)</td>
<td>user name</td>
</tr>
<tr><td>accounting_project</td>
<td>VARCHAR(255)</td>
<td>name of the related project</td>
</tr>
<tr><td>queue_name</td>
<td>VARCHAR(100)</td>
<td>queue name</td>
</tr>
<tr><td>consumption_type</td>
<td>ENUM(&quot;ASKED&quot;,
&quot;USED&quot;)</td>
<td>&quot;ASKED&quot; corresponds to the walltimes
specified by the user. &quot;USED&quot;
corresponds to the effective time
used by the user.</td>
</tr>
<tr><td>consumption</td>
<td>INT UNSIGNED</td>
<td>number of seconds used</td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Primary key:</th><td class="field-body">window_start, window_stop, accounting_user, queue_name,
accounting_project, consumption_type</td>
</tr>
<tr class="field"><th class="field-name">Index fields:</th><td class="field-body">window_start, window_stop, accounting_user, queue_name,
accounting_project, consumption_type</td>
</tr>
</tbody>
</table>
<p>This table is a summary of the consumption for each user on each queue. This
increases the speed of queries about user consumptions and statistic
generation.</p>
<p>Data are inserted through the command <a class="reference internal" href="#oaraccounting">oaraccounting</a> (when a job is treated
the field <em>accounted</em> in table jobs is passed into &quot;YES&quot;). So it is possible to
regenerate this table completely in this way :</p>
<blockquote>
<ul>
<li><p class="first">Delete all data of the table:</p>
<pre class="literal-block">
DELETE FROM accounting;
</pre>
</li>
<li><p class="first">Set the field <em>accounted</em> in the table jobs to &quot;NO&quot; for each row:</p>
<pre class="literal-block">
UPDATE jobs SET accounted = &quot;NO&quot;;
</pre>
</li>
<li><p class="first">Run the <a class="reference internal" href="#oaraccounting">oaraccounting</a> command.</p>
</li>
</ul>
</blockquote>
<p>You can change the amount of time for each window : edit the oar configuration
file and change the value of the tag <a class="reference internal" href="#accounting-window">ACCOUNTING_WINDOW</a>.</p>
</div>
<div class="section" id="admission-rules">
<h2><a class="toc-backref" href="#id33">5.2&nbsp;&nbsp;&nbsp;<em>admission_rules</em></a></h2>
<table border="1" class="docutils">
<colgroup>
<col width="21%" />
<col width="27%" />
<col width="52%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Fields</th>
<th class="head">Types</th>
<th class="head">Descriptions</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>id</td>
<td>INT UNSIGNED</td>
<td>id number</td>
</tr>
<tr><td>rule</td>
<td>TEXT</td>
<td>rule written in Perl applied when a
job is going to be registered</td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Primary key:</th><td class="field-body">id</td>
</tr>
<tr class="field"><th class="field-name">Index fields:</th><td class="field-body"><em>None</em></td>
</tr>
</tbody>
</table>
<p>You can use these rules to change some values of some properties when a job is
submitted. So each admission rule is executed in the order of the id field and
it can set several variables. If one of them exits then the others will not
be evaluated and oarsub returns an error.</p>
<p>Some examples are better than a long description :</p>
<blockquote>
<ul>
<li><p class="first">Specify the default value for queue parameter</p>
<pre class="literal-block">
INSERT INTO admission_rules (rule) VALUES('
  if (not defined($queue_name)) {
      $queue_name=&quot;default&quot;;
  }
');
</pre>
</li>
<li><p class="first">Avoid users except oar to go in the admin queue</p>
<pre class="literal-block">
INSERT INTO admission_rules (rule) VALUES ('
  if (($queue_name eq &quot;admin&quot;) &amp;&amp; ($user ne &quot;oar&quot;)) {
    die(&quot;[ADMISSION RULE] Only oar user can submit jobs in the admin queue\\n&quot;);
  }
');
</pre>
</li>
<li><p class="first">Restrict the maximum of the walltime for interactive jobs</p>
<pre class="literal-block">
INSERT INTO admission_rules (rule) VALUES ('
  my $max_walltime = iolib::sql_to_duration(&quot;12:00:00&quot;);
  if ($jobType eq &quot;INTERACTIVE&quot;){
    foreach my $mold (&#64;{$ref_resource_list}){
      if (
        (defined($mold-&gt;[1])) and
        ($max_walltime &lt; $mold-&gt;[1])
      ){
        print(&quot;[ADMISSION RULE] Walltime to big for an INTERACTIVE job so it is set to $max_walltime.\\n&quot;);
        $mold-&gt;[1] = $max_walltime;
      }
    }
  }
');
</pre>
</li>
<li><p class="first">Specify the default walltime</p>
<pre class="literal-block">
INSERT INTO admission_rules (rule) VALUES ('
  my $default_wall = iolib::sql_to_duration(&quot;2:00:00&quot;);
  foreach my $mold (&#64;{$ref_resource_list}){
    if (!defined($mold-&gt;[1])){
      print(&quot;[ADMISSION RULE] Set default walltime to $default_wall.\\n&quot;);
      $mold-&gt;[1] = $default_wall;
    }
  }
');
</pre>
</li>
<li><p class="first">How to perform actions if the user name is in a file</p>
<pre class="literal-block">
INSERT INTO admission_rules (rule) VALUES ('
  open(FILE, &quot;/tmp/users.txt&quot;);
  while (($queue_name ne &quot;admin&quot;) and ($_ = &lt;FILE&gt;)){
    if ($_ =~ m/^\\s*$user\\s*$/m){
      print(&quot;[ADMISSION RULE] Change assigned queue into admin\\n&quot;);
      $queue_name = &quot;admin&quot;;
    }
  }
  close(FILE);
');
</pre>
</li>
</ul>
</blockquote>
</div>
<div class="section" id="event-logs">
<h2><a class="toc-backref" href="#id34">5.3&nbsp;&nbsp;&nbsp;<em>event_logs</em></a></h2>
<table border="1" class="docutils">
<colgroup>
<col width="21%" />
<col width="27%" />
<col width="52%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Fields</th>
<th class="head">Types</th>
<th class="head">Descriptions</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>event_id</td>
<td>INT UNSIGNED</td>
<td>event identifier</td>
</tr>
<tr><td>type</td>
<td>VARCHAR(50)</td>
<td>event type</td>
</tr>
<tr><td>job_id</td>
<td>INT UNSIGNED</td>
<td>job related of the event</td>
</tr>
<tr><td>date</td>
<td>INT UNSIGNED</td>
<td>event date</td>
</tr>
<tr><td>description</td>
<td>VARCHAR(255)</td>
<td>textual description of the event</td>
</tr>
<tr><td>to_check</td>
<td>ENUM('YES', 'NO')</td>
<td>specify if the module <em>NodeChangeState</em>
must check this event to Suspect or not
some nodes</td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Primary key:</th><td class="field-body">event_id</td>
</tr>
<tr class="field"><th class="field-name">Index fields:</th><td class="field-body">type, to_check</td>
</tr>
</tbody>
</table>
<p>The different event types are:</p>
<blockquote>
<ul class="simple">
<li>&quot;PING_CHECKER_NODE_SUSPECTED&quot; : the system detected via the module &quot;finaud&quot;
that a node is not responding.</li>
<li>&quot;PROLOGUE_ERROR&quot; : an error occurred during the execution of the job
prologue (exit code != 0).</li>
<li>&quot;EPILOGUE_ERROR&quot; : an error occurred during the execution of the job
epilogue (exit code != 0).</li>
<li>&quot;CANNOT_CREATE_TMP_DIRECTORY&quot; : OAR cannot create the directory where all
information files will be stored.</li>
<li>&quot;CAN_NOT_WRITE_NODE_FILE&quot; : the system was not able to write file which had
to contain the node list on the first node (<em>/tmp/OAR_job_id</em>).</li>
<li>&quot;CAN_NOT_WRITE_PID_FILE&quot; : the system was not able to write the file which
had to contain the pid of oarexec process on the first node
(<em>/tmp/pid_of_oarexec_for_job_id</em>).</li>
<li>&quot;USER_SHELL&quot; : the system was not able to get informations about the user
shell on the first node.</li>
<li>&quot;EXIT_VALUE_OAREXEC&quot; : the oarexec process terminated with an unknown exit
code.</li>
<li>&quot;SEND_KILL_JOB&quot; : signal that OAR has transmitted a kill signal to the
oarexec of the specified job.</li>
<li>&quot;LEON_KILL_BIPBIP_TIMEOUT&quot; : Leon module has detected that something wrong
occurred during the kill of a job and so kill the local <em>bipbip</em> process.</li>
<li>&quot;EXTERMINATE_JOB&quot; : Leon module has detected that something wrong occurred
during the kill of a job and so clean the database and terminate the job
artificially.</li>
<li>&quot;WORKING_DIRECTORY&quot; : the directory from which the job was submitted does
not exist on the node assigned by the system.</li>
<li>&quot;OUTPUT_FILES&quot; : OAR cannot write the output files (stdout and stderr) in
the working directory.</li>
<li>&quot;CANNOT_NOTIFY_OARSUB&quot; : OAR cannot notify the <cite>oarsub</cite> process for an
interactive job (maybe the user has killed this process).</li>
<li>&quot;WALLTIME&quot; : the job has reached its walltime.</li>
<li>&quot;SCHEDULER_REDUCE_NB_NODES_FOR_RESERVATION&quot; : this means that there is not
enough nodes for the reservation and so the scheduler do the best and
gives less nodes than the user wanted (this occurres when nodes become
Suspected or Absent).</li>
<li>&quot;BESTEFFORT_KILL&quot; : the job is of the type <em>besteffort</em> and was killed
because a normal job wanted the nodes.</li>
<li>&quot;FRAG_JOB_REQUEST&quot; : someone wants to delete a job.</li>
<li>&quot;CHECKPOINT&quot; : the checkpoint signal was sent to the job.</li>
<li>&quot;CHECKPOINT_ERROR&quot; : OAR cannot send the signal to the job.</li>
<li>&quot;CHECKPOINT_SUCCESS&quot; : system has sent the signal correctly.</li>
<li>&quot;SERVER_EPILOGUE_TIMEOUT&quot; : epilogue server script has time outed.</li>
<li>&quot;SERVER_EPILOGUE_EXIT_CODE_ERROR&quot; : epilogue server script did not return 0.</li>
<li>&quot;SERVER_EPILOGUE_ERROR&quot; : cannot find epilogue server script file.</li>
<li>&quot;SERVER_PROLOGUE_TIMEOUT&quot; : prologue server script has time outed.</li>
<li>&quot;SERVER_PROLOGUE_EXIT_CODE_ERROR&quot; : prologue server script did not return 0.</li>
<li>&quot;SERVER_PROLOGUE_ERROR&quot; : cannot find prologue server script file.</li>
<li>&quot;CPUSET_CLEAN_ERROR&quot; : OAR cannot clean correctly cpuset files for a job
on the remote node.</li>
<li>&quot;MAIL_NOTIFICATION_ERROR&quot; : a mail cannot be sent.</li>
<li>&quot;USER_MAIL_NOTIFICATION&quot; : user mail notification cannot be performed.</li>
<li>&quot;USER_EXEC_NOTIFICATION_ERROR&quot; : user script execution notification cannot
be performed.</li>
<li>&quot;BIPBIP_BAD_JOBID&quot; : error when retrieving informations about a running job.</li>
<li>&quot;BIPBIP_CHALLENGE&quot; : OAR is configured to detach jobs when they are launched
on compute nodes and the job return a bad challenge number.</li>
<li>&quot;RESUBMIT_JOB_AUTOMATICALLY&quot; : the job was automatically resubmitted.</li>
<li>&quot;WALLTIME&quot; : the job reached its walltime.</li>
<li>&quot;REDUCE_RESERVATION_WALLTIME&quot; : the reservation job was shrunk.</li>
<li>&quot;SSH_TRANSFER_TIMEOUT&quot; : node OAR part script was too long to transfer.</li>
<li>&quot;BAD_HASHTABLE_DUMP&quot; : OAR transfered a bad hashtable.</li>
<li>&quot;LAUNCHING_OAREXEC_TIMEOUT&quot; : oarexec was too long to initialize itself.</li>
<li>&quot;RESERVATION_NO_NODE&quot; : All nodes were detected as bad for the reservation
job.</li>
</ul>
</blockquote>
</div>
<div class="section" id="event-log-hostnames">
<h2><a class="toc-backref" href="#id35">5.4&nbsp;&nbsp;&nbsp;<em>event_log_hostnames</em></a></h2>
<table border="1" class="docutils">
<colgroup>
<col width="21%" />
<col width="27%" />
<col width="52%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Fields</th>
<th class="head">Types</th>
<th class="head">Descriptions</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>event_id</td>
<td>INT UNSIGNED</td>
<td>event identifier</td>
</tr>
<tr><td>hostname</td>
<td>VARCHAR(255)</td>
<td>name of the node where the event
has occured</td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Primary key:</th><td class="field-body">event_id</td>
</tr>
<tr class="field"><th class="field-name">Index fields:</th><td class="field-body">hostname</td>
</tr>
</tbody>
</table>
<p>This table stores hostnames related to events like
&quot;PING_CHECKER_NODE_SUSPECTED&quot;.</p>
</div>
<div class="section" id="files">
<h2><a class="toc-backref" href="#id36">5.5&nbsp;&nbsp;&nbsp;<em>files</em></a></h2>
<table border="1" class="docutils">
<colgroup>
<col width="21%" />
<col width="27%" />
<col width="52%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Fields</th>
<th class="head">Types</th>
<th class="head">Descriptions</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>idFile</td>
<td>INT UNSIGNED</td>
<td>&nbsp;</td>
</tr>
<tr><td>md5sum</td>
<td>VARCHAR(255)</td>
<td>&nbsp;</td>
</tr>
<tr><td>location</td>
<td>VARCHAR(255)</td>
<td>&nbsp;</td>
</tr>
<tr><td>method</td>
<td>VARCHAR(255)</td>
<td>&nbsp;</td>
</tr>
<tr><td>compression</td>
<td>VARCHAR(255)</td>
<td>&nbsp;</td>
</tr>
<tr><td>size</td>
<td>INT UNSIGNED</td>
<td>&nbsp;</td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Primary key:</th><td class="field-body">idFile</td>
</tr>
<tr class="field"><th class="field-name">Index fields:</th><td class="field-body">md5sum</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="frag-jobs">
<h2><a class="toc-backref" href="#id37">5.6&nbsp;&nbsp;&nbsp;<em>frag_jobs</em></a></h2>
<table border="1" class="docutils">
<colgroup>
<col width="21%" />
<col width="35%" />
<col width="44%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Fields</th>
<th class="head">Types</th>
<th class="head">Descriptions</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>frag_id_job</td>
<td>INT UNSIGNED</td>
<td>job id</td>
</tr>
<tr><td>frag_date</td>
<td>INT UNSIGNED</td>
<td>kill job decision date</td>
</tr>
<tr><td>frag_state</td>
<td>ENUM('LEON', 'TIMER_ARMED'
, 'LEON_EXTERMINATE',
'FRAGGED')
DEFAULT 'LEON'</td>
<td>state to tell Leon what to do</td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Primary key:</th><td class="field-body">frag_id_job</td>
</tr>
<tr class="field"><th class="field-name">Index fields:</th><td class="field-body">frag_state</td>
</tr>
</tbody>
</table>
<p>What do these states mean:</p>
<blockquote>
<ul class="simple">
<li>&quot;LEON&quot; : the Leon module must try to kill the job and change the state into
&quot;TIMER_ARMED&quot;.</li>
<li>&quot;TIMER_ARMED&quot; : the Sarko module must wait a response from the job during
a timeout (default is 60s)</li>
<li>&quot;LEON_EXTERMINATE&quot; : the Sarko module has decided that the job time outed and
asked Leon to clean up the database.</li>
<li>&quot;FRAGGED&quot; : job is fragged.</li>
</ul>
</blockquote>
</div>
<div class="section" id="gantt-jobs-resources">
<h2><a class="toc-backref" href="#id38">5.7&nbsp;&nbsp;&nbsp;<em>gantt_jobs_resources</em></a></h2>
<table border="1" class="docutils">
<colgroup>
<col width="21%" />
<col width="27%" />
<col width="52%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Fields</th>
<th class="head">Types</th>
<th class="head">Descriptions</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>moldable_job_id</td>
<td>INT UNSIGNED</td>
<td>moldable job id</td>
</tr>
<tr><td>resource_id</td>
<td>INT UNSIGNED</td>
<td>resource assigned to the job</td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Primary key:</th><td class="field-body">moldable_job_id, resource_id</td>
</tr>
<tr class="field"><th class="field-name">Index fields:</th><td class="field-body"><em>None</em></td>
</tr>
</tbody>
</table>
<p>This table specifies which resources are attributed to which jobs.</p>
</div>
<div class="section" id="gantt-jobs-resources-visu">
<h2><a class="toc-backref" href="#id39">5.8&nbsp;&nbsp;&nbsp;<em>gantt_jobs_resources_visu</em></a></h2>
<table border="1" class="docutils">
<colgroup>
<col width="21%" />
<col width="27%" />
<col width="52%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Fields</th>
<th class="head">Types</th>
<th class="head">Descriptions</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>moldable_job_id</td>
<td>INT UNSIGNED</td>
<td>moldable job id</td>
</tr>
<tr><td>resource_id</td>
<td>INT UNSIGNED</td>
<td>resource assigned to the job</td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Primary key:</th><td class="field-body">moldable_job_id, resource_id</td>
</tr>
<tr class="field"><th class="field-name">Index fields:</th><td class="field-body"><em>None</em></td>
</tr>
</tbody>
</table>
<p>This table is the same as <a class="reference internal" href="#gantt-jobs-resources">gantt_jobs_resources</a> and is used by visualisation
tools. It is updated atomically (a lock is used).</p>
</div>
<div class="section" id="gantt-jobs-predictions">
<h2><a class="toc-backref" href="#id40">5.9&nbsp;&nbsp;&nbsp;<em>gantt_jobs_predictions</em></a></h2>
<table border="1" class="docutils">
<colgroup>
<col width="21%" />
<col width="27%" />
<col width="52%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Fields</th>
<th class="head">Types</th>
<th class="head">Descriptions</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>moldable_job_id</td>
<td>INT UNSIGNED</td>
<td>job id</td>
</tr>
<tr><td>start_time</td>
<td>INT UNSIGNED</td>
<td>date when the job is scheduled to start</td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Primary key:</th><td class="field-body">moldable_job_id</td>
</tr>
<tr class="field"><th class="field-name">Index fields:</th><td class="field-body"><em>None</em></td>
</tr>
</tbody>
</table>
<p>With this table and <a class="reference internal" href="#gantt-jobs-resources">gantt_jobs_resources</a> you can know exactly what are the
decisions taken by the schedulers for each waiting jobs.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">note:</th><td class="field-body">The special job id &quot;0&quot; is used to store the scheduling reference date.</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="gantt-jobs-predictions-visu">
<h2><a class="toc-backref" href="#id41">5.10&nbsp;&nbsp;&nbsp;<em>gantt_jobs_predictions_visu</em></a></h2>
<table border="1" class="docutils">
<colgroup>
<col width="21%" />
<col width="27%" />
<col width="52%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Fields</th>
<th class="head">Types</th>
<th class="head">Descriptions</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>moldable_job_id</td>
<td>INT UNSIGNED</td>
<td>job id</td>
</tr>
<tr><td>start_time</td>
<td>INT UNSIGNED</td>
<td>date when the job is scheduled to start</td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Primary key:</th><td class="field-body">job_id</td>
</tr>
<tr class="field"><th class="field-name">Index fields:</th><td class="field-body"><em>None</em></td>
</tr>
</tbody>
</table>
<p>This table is the same as <a class="reference internal" href="#gantt-jobs-predictions">gantt_jobs_predictions</a> and is used by visualisation
tools. It is made up to date in an atomic action (with a lock).</p>
</div>
<div class="section" id="jobs">
<h2><a class="toc-backref" href="#id42">5.11&nbsp;&nbsp;&nbsp;<em>jobs</em></a></h2>
<table border="1" class="docutils">
<colgroup>
<col width="26%" />
<col width="27%" />
<col width="48%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Fields</th>
<th class="head">Types</th>
<th class="head">Descriptions</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>job_id</td>
<td>INT UNSIGNED</td>
<td>job identifier</td>
</tr>
<tr><td>job_name</td>
<td>VARCHAR(100)</td>
<td>name given by the user</td>
</tr>
<tr><td>cpuset_name</td>
<td>VARCHAR(255)</td>
<td>name of the cpuset directory used for
this job on each nodes</td>
</tr>
<tr><td>job_type</td>
<td>ENUM('INTERACTIVE',
'PASSIVE') DEFAULT
'PASSIVE'</td>
<td>specify if the user wants to launch a
program or get an interactive shell</td>
</tr>
<tr><td>info_type</td>
<td>VARCHAR(255)</td>
<td>some informations about <cite>oarsub</cite>
command</td>
</tr>
<tr><td>state</td>
<td>ENUM('Waiting','Hold',
'toLaunch', 'toError',
'toAckReservation',
'Launching', 'Running'
'Suspended',
'Resuming',
, 'Finishing',
'Terminated', 'Error')</td>
<td>job state</td>
</tr>
<tr><td>reservation</td>
<td>ENUM('None',
'toSchedule',
'Scheduled') DEFAULT
'None'</td>
<td>specify if the job is a reservation
and the state of this one</td>
</tr>
<tr><td>message</td>
<td>VARCHAR(255)</td>
<td>readable information message for the
user</td>
</tr>
<tr><td>job_user</td>
<td>VARCHAR(255)</td>
<td>user name</td>
</tr>
<tr><td>command</td>
<td>TEXT</td>
<td>program to run</td>
</tr>
<tr><td>queue_name</td>
<td>VARCHAR(100)</td>
<td>queue name</td>
</tr>
<tr><td>properties</td>
<td>TEXT</td>
<td>properties that assigned nodes must
match</td>
</tr>
<tr><td>launching_directory</td>
<td>TEXT</td>
<td>path of the directory where to launch
the user process</td>
</tr>
<tr><td>submission_time</td>
<td>INT UNSIGNED</td>
<td>date when the job was submitted</td>
</tr>
<tr><td>start_time</td>
<td>INT UNSIGNED</td>
<td>date when the job was launched</td>
</tr>
<tr><td>stop_time</td>
<td>INT UNSIGNED</td>
<td>date when the job was stopped</td>
</tr>
<tr><td>file_id</td>
<td>INT UNSIGNED</td>
<td>&nbsp;</td>
</tr>
<tr><td>accounted</td>
<td>ENUM(&quot;YES&quot;, &quot;NO&quot;)
DEFAULT &quot;NO&quot;</td>
<td>specify if the job was considered by
the accounting mechanism or not</td>
</tr>
<tr><td>notify</td>
<td>VARCHAR(255)</td>
<td>gives the way to notify the user about
the job (mail or script )</td>
</tr>
<tr><td>assigned_moldable_job</td>
<td>INT UNSIGNED</td>
<td>moldable job chosen by the scheduler</td>
</tr>
<tr><td>checkpoint</td>
<td>INT UNSIGNED</td>
<td>number of seconds before the walltime
to send the checkpoint signal to the
job</td>
</tr>
<tr><td>checkpoint_signal</td>
<td>INT UNSIGNED</td>
<td>signal to use when checkpointing the
job</td>
</tr>
<tr><td>stdout_file</td>
<td>TEXT</td>
<td>file name where to redirect program
STDOUT</td>
</tr>
<tr><td>stderr_file</td>
<td>TEXT</td>
<td>file name where to redirect program
STDERR</td>
</tr>
<tr><td>resubmit_job_id</td>
<td>INT UNSIGNED</td>
<td>if a job is resubmitted then the new
one store the previous</td>
</tr>
<tr><td>project</td>
<td>VARCHAR(255)</td>
<td>arbitrary name given by the user or an
admission rule</td>
</tr>
<tr><td>suspended</td>
<td>ENUM(&quot;YES&quot;,&quot;NO&quot;)</td>
<td>specify if the job was suspended
(oarhold)</td>
</tr>
<tr><td>job_env</td>
<td>TEXT</td>
<td>environment variables to set for the
job</td>
</tr>
<tr><td>exit_code</td>
<td>INT DEFAULT 0</td>
<td>exit code for passive jobs</td>
</tr>
<tr><td>job_group</td>
<td>VARCHAR(255)</td>
<td>not used</td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Primary key:</th><td class="field-body">job_id</td>
</tr>
<tr class="field"><th class="field-name">Index fields:</th><td class="field-body">state, reservation, queue_name, accounted, suspended</td>
</tr>
</tbody>
</table>
<p>Explications about the &quot;state&quot; field:</p>
<blockquote>
<ul class="simple">
<li>&quot;Waiting&quot; : the job is waiting OAR scheduler decision.</li>
<li>&quot;Hold&quot; : user or administrator wants to hold the job (<cite>oarhold</cite> command).
So it will not be scheduled by the system.</li>
<li>&quot;toLaunch&quot; : the OAR scheduler has attributed some nodes to the job. So it
will be launched.</li>
<li>&quot;toError&quot; : something wrong occurred and the job is going into the error
state.</li>
<li>&quot;toAckReservation&quot; : the OAR scheduler must say &quot;YES&quot; or &quot;NO&quot; to the waiting
<cite>oarsub</cite> command because it requested a reservation.</li>
<li>&quot;Launching&quot; : OAR has launched the job and will execute the user command
on the first node.</li>
<li>&quot;Running&quot; : the user command is executing on the first node.</li>
<li>&quot;Suspended&quot; : the job was in Running state and there was a request
(<cite>oarhold</cite> with &quot;-r&quot; option) to suspend this job. In this state other jobs
can be scheduled on the same resources (these resources has the
&quot;suspended_jobs&quot; field to &quot;YES&quot;).</li>
<li>&quot;Finishing&quot; : the user command has terminated and OAR is doing work internally</li>
<li>&quot;Terminated&quot; : the job has terminated normally.</li>
<li>&quot;Error&quot; : a problem has occurred.</li>
</ul>
</blockquote>
<p>Explications about the &quot;reservation&quot; field:</p>
<blockquote>
<ul class="simple">
<li>&quot;None&quot; : the job is not a reservation.</li>
<li>&quot;toSchedule&quot; : the job is a reservation and must be approved by the
scheduler.</li>
<li>&quot;Scheduled&quot; : the job is a reservation and is scheduled by OAR.</li>
</ul>
</blockquote>
</div>
<div class="section" id="job-dependencies">
<h2><a class="toc-backref" href="#id43">5.12&nbsp;&nbsp;&nbsp;<em>job_dependencies</em></a></h2>
<table border="1" class="docutils">
<colgroup>
<col width="21%" />
<col width="27%" />
<col width="52%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Fields</th>
<th class="head">Types</th>
<th class="head">Descriptions</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>job_id</td>
<td>INT UNSIGNED</td>
<td>job identifier</td>
</tr>
<tr><td>job_id_required</td>
<td>INT UNSIGNED</td>
<td>job needed to be completed before
launching job_id</td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Primary key:</th><td class="field-body">job_id, job_id_required</td>
</tr>
<tr class="field"><th class="field-name">Index fields:</th><td class="field-body">job_id, job_id_required</td>
</tr>
</tbody>
</table>
<p>This table is feeded by <cite>oarsub</cite> command with the &quot;-a&quot; option.</p>
</div>
<div class="section" id="moldable-job-descriptions">
<h2><a class="toc-backref" href="#id44">5.13&nbsp;&nbsp;&nbsp;<em>moldable_job_descriptions</em></a></h2>
<table border="1" class="docutils">
<colgroup>
<col width="22%" />
<col width="26%" />
<col width="51%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Fields</th>
<th class="head">Types</th>
<th class="head">Descriptions</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>moldable_id</td>
<td>INT UNSIGNED</td>
<td>moldable job identifier</td>
</tr>
<tr><td>moldable_job_id</td>
<td>INT UNSIGNED</td>
<td>corresponding job identifier</td>
</tr>
<tr><td>moldable_walltime</td>
<td>INT UNSIGNED</td>
<td>instance duration</td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Primary key:</th><td class="field-body">moldable_id</td>
</tr>
<tr class="field"><th class="field-name">Index fields:</th><td class="field-body">moldable_job_id</td>
</tr>
</tbody>
</table>
<p>A job can be described with several instances. Thus OAR scheduler can choose one
of them. For example it can calculate which instance will finish first.
So this table stores all instances for all jobs.</p>
</div>
<div class="section" id="job-resource-groups">
<h2><a class="toc-backref" href="#id45">5.14&nbsp;&nbsp;&nbsp;<em>job_resource_groups</em></a></h2>
<table border="1" class="docutils">
<colgroup>
<col width="26%" />
<col width="25%" />
<col width="49%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Fields</th>
<th class="head">Types</th>
<th class="head">Descriptions</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>res_group_id</td>
<td>INT UNSIGNED</td>
<td>group identifier</td>
</tr>
<tr><td>res_group_moldable_id</td>
<td>INT UNSIGNED</td>
<td>corresponding moldable job identifier</td>
</tr>
<tr><td>res_group_property</td>
<td>TEXT</td>
<td>SQL constraint properties</td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Primary key:</th><td class="field-body">res_group_id</td>
</tr>
<tr class="field"><th class="field-name">Index fields:</th><td class="field-body">res_group_moldable_id</td>
</tr>
</tbody>
</table>
<p>As you can specify job global properties with <cite>oarsub</cite> and the &quot;-p&quot; option,
you can do the same thing for each resource groups that you define with
the &quot;-l&quot; option.</p>
</div>
<div class="section" id="job-resource-descriptions">
<h2><a class="toc-backref" href="#id46">5.15&nbsp;&nbsp;&nbsp;<em>job_resource_descriptions</em></a></h2>
<table border="1" class="docutils">
<colgroup>
<col width="26%" />
<col width="25%" />
<col width="49%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Fields</th>
<th class="head">Types</th>
<th class="head">Descriptions</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>res_job_group_id</td>
<td>INT UNSIGNED</td>
<td>corresponding group identifier</td>
</tr>
<tr><td>res_job_resource_type</td>
<td>VARCHAR(255)</td>
<td>resource type (name of a field in
resources)</td>
</tr>
<tr><td>res_job_value</td>
<td>INT</td>
<td>wanted resource number</td>
</tr>
<tr><td>res_job_order</td>
<td>INT UNSIGNED</td>
<td>order of the request</td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Primary key:</th><td class="field-body">res_job_group_id, res_job_resource_type, res_job_order</td>
</tr>
<tr class="field"><th class="field-name">Index fields:</th><td class="field-body">res_job_group_id</td>
</tr>
</tbody>
</table>
<p>This table store the hierarchical resource description given with <cite>oarsub</cite> and
the &quot;-l&quot; option.</p>
</div>
<div class="section" id="job-state-logs">
<h2><a class="toc-backref" href="#id47">5.16&nbsp;&nbsp;&nbsp;<em>job_state_logs</em></a></h2>
<table border="1" class="docutils">
<colgroup>
<col width="22%" />
<col width="26%" />
<col width="51%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Fields</th>
<th class="head">Types</th>
<th class="head">Descriptions</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>job_state_log_id</td>
<td>INT UNSIGNED</td>
<td>identifier</td>
</tr>
<tr><td>job_id</td>
<td>INT UNSIGNED</td>
<td>corresponding job identifier</td>
</tr>
<tr><td>job_state</td>
<td>ENUM('Waiting',
'Hold', 'toLaunch',
'toError',
'toAckReservation',
'Launching',
'Finishing',
'Running',
'Suspended',
'Resuming',
'Terminated',
'Error')</td>
<td>job state during the interval</td>
</tr>
<tr><td>date_start</td>
<td>INT UNSIGNED</td>
<td>start date of the interval</td>
</tr>
<tr><td>date_stop</td>
<td>INT UNSIGNED</td>
<td>end date of the interval</td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Primary key:</th><td class="field-body">job_state_log_id</td>
</tr>
<tr class="field"><th class="field-name">Index fields:</th><td class="field-body">job_id, job_state</td>
</tr>
</tbody>
</table>
<p>This table keeps informations about state changes of jobs.</p>
</div>
<div class="section" id="job-types">
<h2><a class="toc-backref" href="#id48">5.17&nbsp;&nbsp;&nbsp;<em>job_types</em></a></h2>
<table border="1" class="docutils">
<colgroup>
<col width="21%" />
<col width="27%" />
<col width="52%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Fields</th>
<th class="head">Types</th>
<th class="head">Descriptions</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>job_type_id</td>
<td>INT UNSIGNED</td>
<td>identifier</td>
</tr>
<tr><td>job_id</td>
<td>INT UNSIGNED</td>
<td>corresponding job identifier</td>
</tr>
<tr><td>type</td>
<td>VARCHAR(255)</td>
<td>job type like &quot;deploy&quot;, &quot;timesharing&quot;,
...</td>
</tr>
<tr><td>type_index</td>
<td>ENUM('CURRENT',
'LOG')</td>
<td>index field</td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Primary key:</th><td class="field-body">job_type_id</td>
</tr>
<tr class="field"><th class="field-name">Index fields:</th><td class="field-body">job_id, type</td>
</tr>
</tbody>
</table>
<p>This table stores job types given with the <cite>oarsub</cite> command and &quot;-t&quot; options.</p>
</div>
<div class="section" id="resources">
<h2><a class="toc-backref" href="#id49">5.18&nbsp;&nbsp;&nbsp;<em>resources</em></a></h2>
<table border="1" class="docutils">
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="49%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Fields</th>
<th class="head">Types</th>
<th class="head">Descriptions</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>resource_id</td>
<td>INT UNSIGNED</td>
<td>resource identifier</td>
</tr>
<tr><td>type</td>
<td>VARCHAR(100)
DEFAULT &quot;default&quot;</td>
<td>resource type (used for licence
resources for example)</td>
</tr>
<tr><td>network_address</td>
<td>VARCHAR(100)</td>
<td>node name (used to connect via SSH)</td>
</tr>
<tr><td>state</td>
<td>ENUM('Alive', 'Dead'
, 'Suspected',
'Absent')</td>
<td>resource state</td>
</tr>
<tr><td>next_state</td>
<td>ENUM('UnChanged',
'Alive', 'Dead',
'Absent',
'Suspected') DEFAULT
'UnChanged'</td>
<td>state for the resource to switch</td>
</tr>
<tr><td>finaud_decision</td>
<td>ENUM('YES', 'NO')
DEFAULT 'NO'</td>
<td>tell if the actual state results in a
&quot;finaud&quot; module decision</td>
</tr>
<tr><td>next_finaud_decision</td>
<td>ENUM('YES', 'NO')
DEFAULT 'NO'</td>
<td>tell if the next node state results in
a &quot;finaud&quot; module decision</td>
</tr>
<tr><td>state_num</td>
<td>INT</td>
<td>corresponding state number (useful
with the SQL &quot;ORDER&quot; query)</td>
</tr>
<tr><td>suspended_jobs</td>
<td>ENUM('YES','NO')</td>
<td>specify if there is at least one
suspended job on the resource</td>
</tr>
<tr><td>scheduler_priority</td>
<td>INT UNSIGNED</td>
<td>arbitrary number given by the system
to select resources with more
intelligence</td>
</tr>
<tr><td>switch</td>
<td>VARCHAR(50)</td>
<td>name of the switch</td>
</tr>
<tr><td>cpu</td>
<td>INT UNSIGNED</td>
<td>global cluster cpu number</td>
</tr>
<tr><td>cpuset</td>
<td>INT UNSIGNED</td>
<td>field used with the
<a class="reference internal" href="#job-resource-manager-property-db-field">JOB_RESOURCE_MANAGER_PROPERTY_DB_FIELD</a></td>
</tr>
<tr><td>besteffort</td>
<td>ENUM('YES','NO')</td>
<td>accept or not besteffort jobs</td>
</tr>
<tr><td>deploy</td>
<td>ENUM('YES','NO')</td>
<td>specify if the resource is deployable</td>
</tr>
<tr><td>expiry_date</td>
<td>INT UNSIGNED</td>
<td>field used for the desktop computing
feature</td>
</tr>
<tr><td>desktop_computing</td>
<td>ENUM('YES','NO')</td>
<td>tell if it is a desktop computing
resource (with an agent)</td>
</tr>
<tr><td>last_job_date</td>
<td>INT UNSIGNED</td>
<td>store the date when the resource
was used for the last time</td>
</tr>
<tr><td>available_upto</td>
<td>INT UNSIGNED</td>
<td>used with compute mode features to
know if an Absent resource can be
switch on</td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Primary key:</th><td class="field-body">resource_id</td>
</tr>
<tr class="field"><th class="field-name">Index fields:</th><td class="field-body">state, next_state, type, suspended_jobs</td>
</tr>
</tbody>
</table>
<p>State explications:</p>
<blockquote>
<ul class="simple">
<li>&quot;Alive&quot; : the resource is ready to accept a job.</li>
<li>&quot;Absent&quot; : the oar administrator has decided to pull out the resource. This
computer can come back.</li>
<li>&quot;Suspected&quot; : OAR system has detected a problem on this resource and so has
suspected it (you can look in the <a class="reference internal" href="#event-logs">event_logs</a> table to know what has
happened). This computer can come back (automatically if this is a
&quot;finaud&quot; module decision).</li>
<li>&quot;Dead&quot; : The oar administrator considers that the resource will not come back
and will be removed from the pool.</li>
</ul>
</blockquote>
<p>This table permits to specify different properties for each resources. These can
be used with the <cite>oarsub</cite> command (&quot;-p&quot; and &quot;-l&quot; options).</p>
<p>You can add your own properties with <a class="reference internal" href="#oarproperty">oarproperty</a> command.</p>
<p>These properties can be updated with the <a class="reference internal" href="#oarnodesetting">oarnodesetting</a> command (&quot;-p&quot; option).</p>
<p>Several properties are added by default:</p>
<blockquote>
<ul class="simple">
<li>switch : you have to register the name of the switch where the node is
plugged.</li>
<li>cpu : this is a unique name given to each cpus. This enables OAR scheduler
to distinguish all cpus.</li>
<li>cpuset : this is the name of the cpu on the node. The Linux kernel sets this
to an integer beginning at 0. This field is linked to the configuration tag
<a class="reference internal" href="#job-resource-manager-property-db-field">JOB_RESOURCE_MANAGER_PROPERTY_DB_FIELD</a>.</li>
</ul>
</blockquote>
</div>
<div class="section" id="resource-logs">
<h2><a class="toc-backref" href="#id50">5.19&nbsp;&nbsp;&nbsp;<em>resource_logs</em></a></h2>
<table border="1" class="docutils">
<colgroup>
<col width="22%" />
<col width="26%" />
<col width="51%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Fields</th>
<th class="head">Types</th>
<th class="head">Descriptions</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>resource_log_id</td>
<td>INT UNSIGNED</td>
<td>unique id</td>
</tr>
<tr><td>resource_id</td>
<td>INT UNSIGNED</td>
<td>resource identifier</td>
</tr>
<tr><td>attribute</td>
<td>VARCHAR(255)</td>
<td>name of corresponding field in
resources</td>
</tr>
<tr><td>value</td>
<td>VARCHAR(255)</td>
<td>value of the field</td>
</tr>
<tr><td>date_start</td>
<td>INT UNSIGNED</td>
<td>interval start date</td>
</tr>
<tr><td>date_stop</td>
<td>INT UNSIGNED</td>
<td>interval stop date</td>
</tr>
<tr><td>finaud_decision</td>
<td>ENUM('YES','NO')</td>
<td>store if this is a system change or a
human one</td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Primary key:</th><td class="field-body"><em>None</em></td>
</tr>
<tr class="field"><th class="field-name">Index fields:</th><td class="field-body">resource_id, attribute</td>
</tr>
</tbody>
</table>
<p>This table permits to keep a trace of every property changes (consequence of
the <a class="reference internal" href="#oarnodesetting">oarnodesetting</a> command with the &quot;-p&quot; option).</p>
</div>
<div class="section" id="assigned-resources">
<h2><a class="toc-backref" href="#id51">5.20&nbsp;&nbsp;&nbsp;<em>assigned_resources</em></a></h2>
<table border="1" class="docutils">
<colgroup>
<col width="21%" />
<col width="27%" />
<col width="52%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Fields</th>
<th class="head">Types</th>
<th class="head">Descriptions</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>moldable_job_id</td>
<td>INT UNSIGNED</td>
<td>job id</td>
</tr>
<tr><td>resource_id</td>
<td>INT UNSIGNED</td>
<td>resource assigned to the job</td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Primary key:</th><td class="field-body">moldable_job_id, resource_id</td>
</tr>
<tr class="field"><th class="field-name">Index fields:</th><td class="field-body">moldable_job_id</td>
</tr>
</tbody>
</table>
<p>This table keeps informations for jobs on which resources they were
scheduled.</p>
</div>
<div class="section" id="queues">
<h2><a class="toc-backref" href="#id52">5.21&nbsp;&nbsp;&nbsp;<em>queues</em></a></h2>
<table border="1" class="docutils">
<colgroup>
<col width="21%" />
<col width="27%" />
<col width="52%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Fields</th>
<th class="head">Types</th>
<th class="head">Descriptions</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>queue_name</td>
<td>VARCHAR(100)</td>
<td>queue name</td>
</tr>
<tr><td>priority</td>
<td>INT UNSIGNED</td>
<td>the scheduling priority</td>
</tr>
<tr><td>scheduler_policy</td>
<td>VARCHAR(100)</td>
<td>path of the associated scheduler</td>
</tr>
<tr><td>state</td>
<td>ENUM('Active',
'notActive')
DEFAULT 'Active'</td>
<td>permits to stop the scheduling for a
queue</td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Primary key:</th><td class="field-body">queue_name</td>
</tr>
<tr class="field"><th class="field-name">Index fields:</th><td class="field-body"><em>None</em></td>
</tr>
</tbody>
</table>
<p>This table contains the schedulers executed by the <em>oar_meta_scheduler</em> module.
Executables are launched one after one in the specified priority.</p>
</div>
<div class="section" id="challenges">
<h2><a class="toc-backref" href="#id53">5.22&nbsp;&nbsp;&nbsp;<em>challenges</em></a></h2>
<table border="1" class="docutils">
<colgroup>
<col width="21%" />
<col width="27%" />
<col width="52%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Fields</th>
<th class="head">Types</th>
<th class="head">Descriptions</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>job_id</td>
<td>INT UNSIGNED</td>
<td>job identifier</td>
</tr>
<tr><td>challenge</td>
<td>VARCHAR(255)</td>
<td>challenge string</td>
</tr>
<tr><td>ssh_private_key</td>
<td>TEXT DEFAULT NULL</td>
<td>ssh private key given by the user
(in grid usage it enables to connect
onto all nodes of the job of all
clusers with <a class="reference internal" href="#oarsh">oarsh</a>)</td>
</tr>
<tr><td>ssh_public_key</td>
<td>TEXT DEFAULT NULL</td>
<td>ssh public key</td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Primary key:</th><td class="field-body">job_id</td>
</tr>
<tr class="field"><th class="field-name">Index fields:</th><td class="field-body"><em>None</em></td>
</tr>
</tbody>
</table>
<p>This table is used to share a secret between OAR server and oarexec process on
computing nodes (avoid a job id being stolen/forged by malicious user).</p>
<p>For security reasons, this table <strong>must not be readable</strong> for a database
account given to users who want to access OAR internal informations(like statistics).</p>
</div>
</div>
<div class="section" id="configuration-file">
<h1><a class="toc-backref" href="#id54">6&nbsp;&nbsp;&nbsp;Configuration file</a></h1>
<p>Be careful, the syntax of this file must be bash compliant(so after editing
you must be able to launch in bash 'source /etc/oar.conf' and have variables
assigned).
Each configuration tag found in /etc/oar.conf is now described:</p>
<blockquote>
<ul>
<li><p class="first">Database type : you can use a MySQL or a PostgreSQL database (tags are
&quot;mysql&quot; or &quot;Pg&quot;):</p>
<pre class="literal-block">
DB_TYPE=mysql
</pre>
</li>
<li><p class="first">Database hostname:</p>
<pre class="literal-block">
DB_HOSTNAME=localhost

  - Database port::

DB_PORT=3306
</pre>
</li>
<li><p class="first">Database base name:</p>
<pre class="literal-block">
DB_BASE_NAME=oar
</pre>
</li>
<li><p class="first">DataBase user name:</p>
<pre class="literal-block">
DB_BASE_LOGIN=oar
</pre>
</li>
<li><p class="first">DataBase user password:</p>
<pre class="literal-block">
DB_BASE_PASSWD=oar
</pre>
</li>
</ul>
</blockquote>
<blockquote id="db-base-login-ro">
<ul>
<li><p class="first">DataBase read only user name:</p>
<pre class="literal-block">
DB_BASE_LOGIN_RO=oar_ro
</pre>
</li>
</ul>
</blockquote>
<blockquote id="db-base-passwd-ro">
<ul>
<li><p class="first">DataBase read only user password:</p>
<pre class="literal-block">
DB_BASE_PASSWD_RO=oar_ro
</pre>
</li>
<li><p class="first">OAR server hostname:</p>
<pre class="literal-block">
SERVER_HOSTNAME=localhost
</pre>
</li>
</ul>
</blockquote>
<blockquote id="server-port">
<ul>
<li><p class="first">OAR server port:</p>
<pre class="literal-block">
SERVER_PORT=6666
</pre>
</li>
<li><p class="first">When the user does not specify a -l option then oar use this:</p>
<pre class="literal-block">
OARSUB_DEFAULT_RESOURCES=&quot;/resource_id=1&quot;
</pre>
</li>
<li><p class="first">Force use of job key even if --use-job-key or -k is not set in oarsub:</p>
<pre class="literal-block">
OARSUB_FORCE_JOB_KEY=&quot;no&quot;
</pre>
</li>
</ul>
</blockquote>
<blockquote id="deploy-hostname">
<ul>
<li><p class="first">Specify where we are connected in the deploy queue(the node to connect
to when the job is in the deploy queue):</p>
<pre class="literal-block">
DEPLOY_HOSTNAME=&quot;127.0.0.1&quot;
</pre>
</li>
</ul>
</blockquote>
<blockquote id="cosystem-hostname">
<ul>
<li><p class="first">Specify where we are connected with a job of the cosystem type:</p>
<pre class="literal-block">
COSYSTEM_HOSTNAME=&quot;127.0.0.1&quot;
</pre>
</li>
</ul>
</blockquote>
<blockquote id="detach-job-from-server">
<ul>
<li><p class="first">Set DETACH_JOB_FROM_SERVER to 1 if you do not want to keep a ssh
connection between the node and the server. Otherwise set this tag to 0:</p>
<pre class="literal-block">
DETACH_JOB_FROM_SERVER=1
</pre>
</li>
<li><p class="first">Set the directory where OAR will store its temporary files on each nodes
of the cluster. This value MUST be the same in all oar.conf on
all nodes:</p>
<pre class="literal-block">
OAR_RUNTIME_DIRECTORY=&quot;/tmp/oar_runtime&quot;
</pre>
</li>
<li><p class="first">Specify the database field to use to fill the file on the first node of
the job in $OAR_NODE_FILE (default is 'network_address'). Only resources
with type=default are displayed in this file:</p>
<pre class="literal-block">
NODE_FILE_DB_FIELD=&quot;network_address&quot;
</pre>
</li>
<li><p class="first">Specify the database field that will be considered to fill the node file
used by the user on the first node of the job. for each different value
of this field then OAR will put 1 line in the node file(by default &quot;cpu&quot;):</p>
<pre class="literal-block">
NODE_FILE_DB_FIELD_DISTINCT_VALUES=&quot;core&quot;
</pre>
</li>
<li><p class="first">By default OAR uses the ping command to detect if nodes are down or not.
To enhance this diagnostic you can specify one of these other methods (
give the complete command path):</p>
<blockquote>
<ul>
<li><p class="first">OAR taktuk:</p>
<pre class="literal-block">
PINGCHECKER_TAKTUK_ARG_COMMAND=&quot;-t 3 broadcast exec [ true ]&quot;
</pre>
<p>If you use sentinelle.pl then you must use this tag:</p>
<pre class="literal-block">
PINGCHECKER_SENTINELLE_SCRIPT_COMMAND=&quot;/var/lib/oar/sentinelle.pl -t 5 -w 20&quot;
</pre>
</li>
<li><p class="first">OAR fping:</p>
<pre class="literal-block">
PINGCHECKER_FPING_COMMAND=&quot;/usr/bin/fping -q&quot;
</pre>
</li>
<li><p class="first">OAR nmap : it will test to connect on the ssh port (22):</p>
<pre class="literal-block">
PINGCHECKER_NMAP_COMMAND=&quot;/usr/bin/nmap -p 22 -n -T5&quot;
</pre>
</li>
<li><p class="first">OAR generic : a specific script may be used instead of ping to check
aliveness of nodes. The script must return bad nodes on STDERR (1 line
for a bad node and it must have exactly the same name that OAR has
given in argument of the command):</p>
<pre class="literal-block">
PINGCHECKER_GENERIC_COMMAND=&quot;/path/to/command arg1 arg2&quot;
</pre>
</li>
</ul>
</blockquote>
</li>
<li><p class="first">OAR log level: 3(debug+warnings+errors), 2(warnings+errors), 1(errors):</p>
<pre class="literal-block">
LOG_LEVEL=2
</pre>
</li>
<li><p class="first">OAR log file:</p>
<pre class="literal-block">
LOG_FILE=&quot;/var/log/oar.log&quot;
</pre>
</li>
<li><p class="first">If you want to debug oarexec on nodes then affect 1 (only effective if
DETACH_JOB_FROM_SERVER = 1):</p>
<pre class="literal-block">
OAREXEC_DEBUG_MODE=0
</pre>
</li>
</ul>
</blockquote>
<blockquote id="accounting-window">
<ul>
<li><p class="first">Set the granularity of the OAR accounting feature (in seconds). Default is
1 day (86400s):</p>
<pre class="literal-block">
ACCOUNTING_WINDOW=&quot;86400&quot;
</pre>
</li>
</ul>
</blockquote>
<blockquote id="mail">
<ul>
<li><p class="first">OAR informations may be notified by email to the administror.
Set accordingly to your configuration the next lines to activate
this feature:</p>
<pre class="literal-block">
MAIL_SMTP_SERVER=&quot;smtp.serveur.com&quot;
MAIL_RECIPIENT=&quot;user&#64;domain.com&quot;
MAIL_SENDER=&quot;oar&#64;domain.com&quot;
</pre>
</li>
<li><p class="first">Set the timeout for the prologue and epilogue execution on computing
nodes:</p>
<pre class="literal-block">
PROLOGUE_EPILOGUE_TIMEOUT=60
</pre>
</li>
<li><p class="first">Files to execute before and after each job on the first computing node
(by default nothing is executed):</p>
<pre class="literal-block">
PROLOGUE_EXEC_FILE=&quot;/path/to/prog&quot;
EPILOGUE_EXEC_FILE=&quot;/path/to/prog&quot;
</pre>
</li>
<li><p class="first">Set the timeout for the prologue and epilogue execution on the OAR server:</p>
<pre class="literal-block">
SERVER_PROLOGUE_EPILOGUE_TIMEOUT=60
</pre>
</li>
</ul>
</blockquote>
<blockquote id="server-script-exec-file">
<ul>
<li><p class="first">Files to execute before and after each job on the OAR server
(by default nothing is executed):</p>
<pre class="literal-block">
SERVER_PROLOGUE_EXEC_FILE=&quot;/path/to/prog&quot;
SERVER_EPILOGUE_EXEC_FILE=&quot;/path/to/prog&quot;
</pre>
</li>
<li><p class="first">Set the frequency for checking Alive and Suspected resources:</p>
<pre class="literal-block">
FINAUD_FREQUENCY=300
</pre>
</li>
</ul>
</blockquote>
<blockquote id="dead-switch-time">
<ul>
<li><p class="first">Set time after which resources become Dead (default is 0 and it means
never):</p>
<pre class="literal-block">
DEAD_SWITCH_TIME=600
</pre>
</li>
</ul>
</blockquote>
<blockquote id="scheduler-timeout">
<ul>
<li><p class="first">Maximum of seconds used by a scheduler:</p>
<pre class="literal-block">
SCHEDULER_TIMEOUT=10
</pre>
</li>
<li><p class="first">Time to wait when a reservation has not got all resources that it has
reserved (some resources could have become Suspected or Absent since the
job submission) before to launch the job in the remaining resources:</p>
<pre class="literal-block">
RESERVATION_WAITING_RESOURCES_TIMEOUT=300
</pre>
</li>
</ul>
</blockquote>
<blockquote id="scheduler-job-security-time">
<ul>
<li><p class="first">Time to add between each jobs (time for administration tasks or time to
let computers to reboot):</p>
<pre class="literal-block">
SCHEDULER_JOB_SECURITY_TIME=1
</pre>
</li>
</ul>
</blockquote>
<blockquote id="scheduler-gantt-hole-minimum-time">
<ul>
<li><p class="first">Minimum time in seconds that can be considered like a hole where a job
could be scheduled in:</p>
<pre class="literal-block">
SCHEDULER_GANTT_HOLE_MINIMUM_TIME=300
</pre>
</li>
</ul>
</blockquote>
<blockquote id="scheduler-resource-order">
<ul>
<li><p class="first">You can add an order preference on resource assigned by the system(SQL
ORDER syntax):</p>
<pre class="literal-block">
SCHEDULER_RESOURCE_ORDER=&quot;switch ASC, network_address DESC, resource_id ASC&quot;
</pre>
</li>
</ul>
</blockquote>
<blockquote id="scheduler-resources-always-assigned-type">
<ul>
<li><p class="first">You can specify resources from a resource type that will be always assigned for
each job (for example: enable all jobs to be able to log on the cluster
frontales).
For more information, see the FAQ:</p>
<pre class="literal-block">
SCHEDULER_RESOURCES_ALWAYS_ASSIGNED_TYPE=&quot;42 54 12 34&quot;
</pre>
</li>
<li><p class="first">This says to the scheduler to treate resources of these types, where there is
a suspended job, like free ones. So some other jobs can be scheduled on these
resources. (list resource types separate with spaces; Default value is
nothing so no other job can be scheduled on suspended job resources):</p>
<pre class="literal-block">
SCHEDULER_AVAILABLE_SUSPENDED_RESOURCE_TYPE=&quot;default licence vlan&quot;
</pre>
</li>
<li><p class="first">Name of the perl script that manages suspend/resume. You have to install your
script in $OARDIR and give only the name of the file without the entire path.
(default is suspend_resume_manager.pl):</p>
<pre class="literal-block">
SUSPEND_RESUME_FILE=&quot;suspend_resume_manager.pl&quot;
</pre>
</li>
</ul>
</blockquote>
<blockquote id="just-before-resume-exec-file">
<span id="just-after-suspend-exec-file"></span><ul>
<li><p class="first">Files to execute just after a job was suspended and just before a job was
resumed:</p>
<pre class="literal-block">
JUST_AFTER_SUSPEND_EXEC_FILE=&quot;/path/to/prog&quot;
JUST_BEFORE_RESUME_EXEC_FILE=&quot;/path/to/prog&quot;
</pre>
</li>
<li><p class="first">Timeout for the two previous scripts:</p>
<pre class="literal-block">
SUSPEND_RESUME_SCRIPT_TIMEOUT=60
</pre>
</li>
</ul>
</blockquote>
<blockquote id="job-resource-manager-property-db-field">
<ul>
<li><p class="first">Indicate the name of the database field that contains the cpu number of
the node. If this option is set then users must use <a class="reference internal" href="#oarsh">OARSH</a> instead of
ssh to walk on each nodes that they have reserved via oarsub.</p>
<pre class="literal-block">
JOB_RESOURCE_MANAGER_PROPERTY_DB_FIELD=cpuset
</pre>
</li>
</ul>
</blockquote>
<blockquote id="job-resource-manager-file">
<ul>
<li><p class="first">Name of the perl script that manages cpuset. You have to install your
script in $OARDIR and give only the name of the file without the
entire path.
(default is cpuset_manager.pl which handles the linux kernel cpuset)</p>
<pre class="literal-block">
JOB_RESOURCE_MANAGER_FILE=&quot;cpuset_manager.pl&quot;
</pre>
</li>
</ul>
</blockquote>
<blockquote id="job-resource-manager-job-uid-type">
<ul>
<li><p class="first">Resource &quot;type&quot; DB field to use if you want to enable the job uid feature.
(create a unique user id per job on each nodes of the job)</p>
<pre class="literal-block">
JOB_RESOURCE_MANAGER_JOB_UID_TYPE=&quot;userid&quot;
</pre>
</li>
</ul>
</blockquote>
<blockquote id="taktuk-cmd">
<ul>
<li><p class="first">If you have installed taktuk and want to use it to manage cpusets
then give the full command path (with your options except &quot;-m&quot; and &quot;-o&quot;
and &quot;-c&quot;).
You don't also have to give any taktuk command.(taktuk version must be &gt;=
3.6)</p>
<pre class="literal-block">
TAKTUK_CMD=&quot;/usr/bin/taktuk -s&quot;
</pre>
</li>
<li><p class="first">If you want to manage nodes to be started and stoped. OAR gives you this
API:</p>
</li>
</ul>
</blockquote>
<blockquote id="scheduler-node-manager-wake-up-cmd">
<ul>
<li><p class="first">When OAR scheduler wants some nodes to wake up then it launches this
command and puts on its STDIN the list of nodes to wake up (one hostname
by line).The scheduler looks at <em>available_upto</em> field in the <a class="reference internal" href="#resources">resources</a>
table to know if the node will be started for enough time:</p>
<pre class="literal-block">
SCHEDULER_NODE_MANAGER_WAKE_UP_CMD=&quot;/path/to/the/command with your args&quot;
</pre>
</li>
</ul>
</blockquote>
<blockquote id="scheduler-node-manager-sleep-cmd">
<ul>
<li><p class="first">When OAR considers that some nodes can be shut down, it launches this
command and puts the node list on its STDIN(one hostname by line):</p>
<pre class="literal-block">
SCHEDULER_NODE_MANAGER_SLEEP_CMD=&quot;/path/to/the/command args&quot;
</pre>
</li>
</ul>
</blockquote>
<blockquote id="scheduler-node-manager-idle-time">
<ul>
<li><p class="first">Parameters for the scheduler to decide when a node is idle(number of
seconds since the last job was terminated on the nodes):</p>
<pre class="literal-block">
SCHEDULER_NODE_MANAGER_IDLE_TIME=600
</pre>
</li>
</ul>
</blockquote>
<blockquote id="scheduler-node-manager-sleep-time">
<ul>
<li><p class="first">Parameters for the scheduler to decide if a node will have enough time
to sleep(number of seconds before the next job):</p>
<pre class="literal-block">
SCHEDULER_NODE_MANAGER_SLEEP_TIME=600
</pre>
</li>
</ul>
</blockquote>
<blockquote id="openssh-cmd">
<ul>
<li><p class="first">Command to use to connect to other nodes (default is &quot;ssh&quot; in the PATH)</p>
<pre class="literal-block">
OPENSSH_CMD=&quot;/usr/bin/ssh&quot;
</pre>
</li>
<li><p class="first">These are configuration tags for OAR in the desktop-computing mode:</p>
<pre class="literal-block">
DESKTOP_COMPUTING_ALLOW_CREATE_NODE=0
DESKTOP_COMPUTING_EXPIRY=10
STAGEOUT_DIR=&quot;/var/lib/oar/stageouts/&quot;
STAGEIN_DIR=&quot;/var/lib/oar/stageins&quot;
STAGEIN_CACHE_EXPIRY=144
</pre>
</li>
<li><p class="first">This variable must be set to enable the use of oarsh from a frontale node.
Otherwise you must not set this variable if you are not on a frontale:</p>
<pre class="literal-block">
OARSH_OARSTAT_CMD=&quot;/usr/bin/oarstat&quot;
</pre>
</li>
</ul>
</blockquote>
<blockquote id="oarsh-openssh-default-options">
<ul>
<li><p class="first">The following variable adds options to ssh. If one option is not handled
by your ssh version just remove it BUT be careful because these options are
there for security reasons:</p>
<pre class="literal-block">
OARSH_OPENSSH_DEFAULT_OPTIONS=&quot;-oProxyCommand=none -oPermitLocalCommand=no&quot;
</pre>
</li>
</ul>
</blockquote>
<blockquote id="oarmonitor-sensor-file">
<ul>
<li><p class="first">Name of the perl script the retrive monitoring data from compute nodes.
This is used in oarmonitor command.</p>
<blockquote>
<p>OARMONITOR_SENSOR_FILE=&quot;/etc/oar/oarmonitor_sensor.pl&quot;</p>
</blockquote>
</li>
</ul>
</blockquote>
</div>
<div class="section" id="modules-descriptions">
<h1><a class="toc-backref" href="#id55">7&nbsp;&nbsp;&nbsp;Modules descriptions</a></h1>
<p>OAR can be decomposed into several modules which perform different tasks.</p>
<div class="section" id="almighty">
<h2><a class="toc-backref" href="#id56">7.1&nbsp;&nbsp;&nbsp;Almighty</a></h2>
<p>This module is the OAR server. It decides what actions must be performed. It
is divided into 2 processes:</p>
<blockquote>
<ul class="simple">
<li>One listens to a TCP/IP socket. It waits informations or commands from OAR
user program or from the other modules.</li>
<li>Another one deals with commands thanks to an automaton and launch right
modules one after one.</li>
</ul>
</blockquote>
<p>It's behaviour is represented in these schemes.</p>
<blockquote>
<ul class="simple">
<li>General schema:</li>
</ul>
<img alt="../schemas/almighty_automaton_general.png" src="../schemas/almighty_automaton_general.png" />
</blockquote>
<p>When the Almighty automaton starts it will first open a socket and creates a
pipe for the process communication with it's forked son. Then, Almighty will
fork itself in a process called &quot;appendice&quot; which role is to listen to incoming
connections on the socket and catch clients messages. These messages will be
thereafter piped to Almighty. Then, the automaton will change it's state
according to what message has been received.</p>
<hr class="docutils" />
<blockquote>
<ul class="simple">
<li>Scheduler schema:</li>
</ul>
<img alt="../schemas/almighty_automaton_scheduler_part.png" src="../schemas/almighty_automaton_scheduler_part.png" />
</blockquote>
<hr class="docutils" />
<blockquote>
<ul class="simple">
<li>Finaud schema:</li>
</ul>
<img alt="../schemas/almighty_automaton_finaud_part.png" src="../schemas/almighty_automaton_finaud_part.png" />
</blockquote>
<hr class="docutils" />
<blockquote>
<ul class="simple">
<li>Leon schema:</li>
</ul>
<img alt="../schemas/almighty_automaton_leon_part.png" src="../schemas/almighty_automaton_leon_part.png" />
</blockquote>
<hr class="docutils" />
<blockquote>
<ul class="simple">
<li>Sarko schema:</li>
</ul>
<img alt="../schemas/almighty_automaton_villains_part.png" src="../schemas/almighty_automaton_villains_part.png" />
</blockquote>
<hr class="docutils" />
<blockquote>
<ul class="simple">
<li>ChangeNode schema:</li>
</ul>
<img alt="../schemas/almighty_automaton_changenode_part.png" src="../schemas/almighty_automaton_changenode_part.png" />
</blockquote>
</div>
<div class="section" id="sarko">
<h2><a class="toc-backref" href="#id57">7.2&nbsp;&nbsp;&nbsp;Sarko</a></h2>
<p>This module is executed periodically by the Almighty (default is every
30 seconds).</p>
<p>The jobs of Sarko are :</p>
<blockquote>
<ul class="simple">
<li>Look at running job walltimes and ask to frag them if they had expired.</li>
<li>Detect if fragged jobs are really fragged otherwise asks to exterminate
them.</li>
<li>In &quot;Desktop Computing&quot; mode, it detects if a node date has expired and
asks to change its state into &quot;Suspected&quot;.</li>
<li>Can change &quot;Suspected&quot; resources into &quot;Dead&quot; after <a class="reference internal" href="#dead-switch-time">DEAD_SWITCH_TIME</a> seconds.</li>
</ul>
</blockquote>
</div>
<div class="section" id="judas">
<h2><a class="toc-backref" href="#id58">7.3&nbsp;&nbsp;&nbsp;Judas</a></h2>
<p>This is the module dedicated to print and log every debugging, warning and
error messages.</p>
<p>The notification functions are the following:</p>
<blockquote>
<ul class="simple">
<li>send_mail(mail_recipient_address, object, body, job_id) that sends
emails to the OAR admin</li>
<li>notify_user(base, method, host, user, job_id, job_name, tag, comments)
that parses the notify method. This method can be a user script or a
mail to send. If the &quot;method&quot; field begins with
&quot;mail:&quot;, notify_user will send an email to the user. If the
beginning is &quot;exec:&quot;, it will execute the script as the &quot;user&quot;.</li>
</ul>
</blockquote>
<p>The main logging functions are the following:</p>
<blockquote>
<ul class="simple">
<li>redirect_everything() this function redirects STDOUT and STDERR into
the log file</li>
<li>oar_debug(message)</li>
<li>oar_warn(message)</li>
<li>oar_error(message)</li>
</ul>
</blockquote>
<p>The three last functions are used to set the log level of the message.</p>
</div>
<div class="section" id="leon">
<h2><a class="toc-backref" href="#id59">7.4&nbsp;&nbsp;&nbsp;Leon</a></h2>
<p>This module is in charge to delete the jobs. Other OAR modules or commands
can ask to kill a job and this is Leon which performs that.</p>
<p>There are 2 frag types :</p>
<blockquote>
<ul class="simple">
<li><em>normal</em> : Leon tries to connect to the first node allocated for the job and
terminates the job.</li>
<li><em>exterminate</em> : after a timeout if the <em>normal</em> method did not succeed
then Leon notifies this case and clean up the database for these jobs. So
OAR doesn't know what occured on the node and Suspects it.</li>
</ul>
</blockquote>
</div>
<div class="section" id="runner">
<h2><a class="toc-backref" href="#id60">7.5&nbsp;&nbsp;&nbsp;Runner</a></h2>
<p>This module launches OAR effective jobs. These processes are run asynchronously
with all modules.</p>
<p>For each job, the <a class="reference internal" href="#runner">Runner</a> uses <a class="reference internal" href="#openssh-cmd">OPENSSH_CMD</a> to connect to the first node of the
reservation and propagate a Perl script which handles the execution of the user
command.</p>
<blockquote>
<ul class="simple">
<li>for each job in &quot;toError&quot; state, answer to the oarsub client: &quot;BAD JOB&quot;.
This will exit the client with an error code.</li>
<li>for each job in &quot;toAckReservation&quot; state, try to acknowledge the
oarsub client reservation. If runner cannot contact the client, it will
frag the job.</li>
<li>for each job to launch, launch job's bipbip.</li>
</ul>
</blockquote>
<hr class="docutils" />
<blockquote>
<ul class="simple">
<li>Runner schema:</li>
</ul>
<img alt="../schemas/runner.png" src="../schemas/runner.png" />
</blockquote>
<hr class="docutils" />
<blockquote>
<ul class="simple">
<li>bipbip schema:</li>
</ul>
<img alt="../schemas/bipbip.png" src="../schemas/bipbip.png" />
</blockquote>
</div>
<hr class="docutils" />
<div class="section" id="nodechangestate">
<h2><a class="toc-backref" href="#id61">7.6&nbsp;&nbsp;&nbsp;NodeChangeState</a></h2>
<p>This module is in charge of changing resource states and checking if there are
jobs on these.</p>
<p>It also checks all pending events in the table <a class="reference internal" href="#event-logs">event_logs</a>.</p>
</div>
<div class="section" id="scheduler">
<h2><a class="toc-backref" href="#id62">7.7&nbsp;&nbsp;&nbsp;Scheduler</a></h2>
<p>This module checks for each reservation jobs if it is valid and launches them
at the right time.</p>
<p><a class="reference internal" href="#scheduler">Scheduler</a> launches all gantt scheduler in the order of the priority specified
in the database and update all visualization tables
(<a class="reference internal" href="#gantt-jobs-predictions-visu">gantt_jobs_predictions_visu</a> and <a class="reference internal" href="#gantt-jobs-resources-visu">gantt_jobs_resources_visu</a>).</p>
<div class="section" id="oar-sched-gantt-with-timesharing">
<h3><a class="toc-backref" href="#id63">7.7.1&nbsp;&nbsp;&nbsp;oar_sched_gantt_with_timesharing</a></h3>
<p>This is the default OAR scheduler. It implements all functionalities like
timesharing, moldable jobs, <cite>besteffort jobs</cite>, ...</p>
<p>By default, this scheduler is used by all default queues.</p>
<p>We have implemented the FIFO with backfilling algorithm. Some parameters
can be changed in the <a class="reference internal" href="#configuration-file">configuration file</a> (see <a class="reference internal" href="#scheduler-timeout">SCHEDULER_TIMEOUT</a>,
<a class="reference internal" href="#scheduler-job-security-time">SCHEDULER_JOB_SECURITY_TIME</a>, <a class="reference internal" href="#scheduler-gantt-hole-minimum-time">SCHEDULER_GANTT_HOLE_MINIMUM_TIME</a>,
<a class="reference internal" href="#scheduler-resource-order">SCHEDULER_RESOURCE_ORDER</a>).</p>
</div>
<div class="section" id="oar-sched-gantt-with-timesharing-and-fairsharing">
<h3><a class="toc-backref" href="#id64">7.7.2&nbsp;&nbsp;&nbsp;oar_sched_gantt_with_timesharing_and_fairsharing</a></h3>
<p>This scheduler is the same than <a class="reference internal" href="#oar-sched-gantt-with-timesharing">oar_sched_gantt_with_timesharing</a> but it looks
at the consumption past and try to order waiting jobs with fairsharing in mind.</p>
<p>Some parameters can be changed directly in the file:</p>
<pre class="literal-block">
###############################################################################
# Fairsharing parameters #
##########################
# Avoid problems if there are too many waiting jobs
my $Karma_max_number_of_jobs_treated = 1000;
# number of seconds to consider for the fairsharing
my $Karma_window_size = 3600 * 30;
# specify the target percentages for project names (0 if not specified)
my $Karma_project_targets = {
    first =&gt; 75,
    default =&gt; 25
};

# specify the target percentages for users (0 if not specified)
my $Karma_user_targets = {
    oar =&gt; 100
};
# weight given to each criteria
my $Karma_coeff_project_consumption = 3;
my $Karma_coeff_user_consumption = 2;
my $Karma_coeff_user_asked_consumption = 1;
###############################################################################
</pre>
<p>This scheduler takes its historical data in the <a class="reference internal" href="#accounting">accounting</a> table. To fill this,
the command <a class="reference internal" href="#oaraccounting">oaraccounting</a> have to be run periodically (in a cron job for
example). Otherwise the scheduler cannot be aware of new user consumptions.</p>
</div>
</div>
<div class="section" id="hulot">
<h2><a class="toc-backref" href="#id65">7.8&nbsp;&nbsp;&nbsp;Hulot</a></h2>
<p>This module is responsible of the advanced management of the standby mode of the
nodes. It's related to the energy saving features of OAR. It is an optional module
activated with the ENERGY_SAVING_INTERNAL=yes configuration variable.</p>
<p>It runs as a fourth &quot;Almighty&quot; daemon and opens a pipe on which it receives commands
from the MetaScheduler. It also communicates with a library called &quot;WindowForker&quot;
that is responsible of forking shut-down/wake-up commands in a way that not too much
commands are started at a time.</p>
<hr class="docutils" />
<blockquote>
<ul class="simple">
<li>Hulot general commands process schema:</li>
</ul>
<img alt="../schemas/hulot_general_commands_process.png" src="../schemas/hulot_general_commands_process.png" />
</blockquote>
<p>When Hulot is activated, the metascheduler sends, each time it is executed, a
list of nodes that need to be woken-up or may be halted. Hulot maintains a
list of commands that have already been sent to the nodes and asks to the
windowforker to actually execute the commands only when it is appropriate.
A special feature is the &quot;keepalive&quot; of nodes depending on some properties:
even if the metascheduler asks to shut-down some nodes, it's up to Hulot to
check if the keepalive constraints are still satisfied. If not, Hulot refuses
to halt the corresponding nodes.</p>
<hr class="docutils" />
<blockquote>
<ul class="simple">
<li>Hulot checking process schema:</li>
</ul>
<img alt="../schemas/hulot_checking_process.png" src="../schemas/hulot_checking_process.png" />
</blockquote>
<p>Hulot is called each time the metascheduler is called, to do all the checking
process. This process is also executed when Hulot receives normal halt or wake-up
commands from the scheduler. Hulot checks if waking-up nodes are actually Alive
or not and suspects the nodes if they haven't woken-up before the timeout.
It also checks keepalive constraints and decides to wake-up nodes if a constraint
is no more satisfied (for example because new jobs are running on nodes that are
now busy, and no more idle).
Hulot also checks the results of the commands sent by the windowforker and may
also suspect a node if the command exited with non-zero status.</p>
<hr class="docutils" />
<blockquote>
<ul class="simple">
<li>Hulot wake-up process schema</li>
</ul>
<img alt="../schemas/hulot_wakeup_process.png" src="../schemas/hulot_wakeup_process.png" />
</blockquote>
<hr class="docutils" />
<blockquote>
<ul class="simple">
<li>Hulot shutdown process schema</li>
</ul>
<img alt="../schemas/hulot_shutdown_process.png" src="../schemas/hulot_shutdown_process.png" />
</blockquote>
</div>
</div>
<hr class="docutils" />
<div class="section" id="internal-mechanisms">
<h1><a class="toc-backref" href="#id66">8&nbsp;&nbsp;&nbsp;Internal mechanisms</a></h1>
<div class="section" id="job-execution">
<h2><a class="toc-backref" href="#id67">8.1&nbsp;&nbsp;&nbsp;Job execution</a></h2>
<div class="figure">
<img alt="../schemas/job_execution.png" src="../schemas/job_execution.png" />
</div>
</div>
<hr class="docutils" />
<div class="section" id="scheduling">
<h2><a class="toc-backref" href="#id68">8.2&nbsp;&nbsp;&nbsp;Scheduling</a></h2>
<blockquote>
<div class="figure">
<img alt="../schemas/scheduling.png" src="../schemas/scheduling.png" />
</div>
</blockquote>
</div>
</div>
<div class="section" id="faq-admin">
<h1><a class="toc-backref" href="#id69">9&nbsp;&nbsp;&nbsp;FAQ - ADMIN</a></h1>
<div class="section" id="release-policy">
<h2><a class="toc-backref" href="#id70">9.1&nbsp;&nbsp;&nbsp;Release policy</a></h2>
<dl class="docutils">
<dt>Since the version 2.2, release numbers are divided into 3 parts:</dt>
<dd><ul class="first last simple">
<li>The first represents the design and the implementation used.</li>
<li>The second represents a set of OAR functionalities.</li>
<li>The third is incremented after bug fixes.</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="what-means-the-error-bad-configuration-option-permitlocalcommand-when-i-am-using-oarsh">
<h2><a class="toc-backref" href="#id71">9.2&nbsp;&nbsp;&nbsp;What means the error &quot;Bad configuration option: PermitLocalCommand&quot; when I am using oarsh?</a></h2>
<p>For security reasons, on the latest OpenSSH releases you are able to execute
a local command when you are connecting to the remote host and we must
deactivate this option because the <a class="reference internal" href="#oarsh">oarsh</a> wrapper executes the <em>ssh</em> command
into the user oar.</p>
<p>So if you encounter this error message it means that your OpenSSH does
not know this option and you have to remove it from the oar.conf.
There is a variable named <a class="reference internal" href="#oarsh-openssh-default-options">OARSH_OPENSSH_DEFAULT_OPTIONS</a> in oar.conf used by oarsh.
So you have just to remove the not yet implemented option.</p>
</div>
<div class="section" id="how-to-manage-start-stop-of-the-nodes">
<h2><a class="toc-backref" href="#id72">9.3&nbsp;&nbsp;&nbsp;How to manage start/stop of the nodes?</a></h2>
<p>You have to add a script in /etc/init.d which switches resources of the node
into the &quot;Alive&quot; or &quot;Absent&quot; state.
So when this script is called at boot time, it will change the state into
&quot;Alive&quot;. And when it is called at halt time, it will change into &quot;Absent&quot;.</p>
<p>There two ways to perform this action:</p>
<blockquote>
<ol class="arabic">
<li><p class="first">Install OAR &quot;oar-libs&quot; part on all nodes. Thus you will be able to launch
the command <a class="reference internal" href="#oarnodesetting">oarnodesetting</a> (be careful to right configure &quot;oar.conf&quot; with
database login and password AND to allow network connections on this
database).
So you can execute:</p>
<pre class="literal-block">
oarnodesetting -s Alive -h node_hostname
    or
oarnodesetting -s Absent -h node_hostname
</pre>
</li>
<li><p class="first">You do not want to install anything else on each node. So you have to
enable oar user to connect to the server via ssh (for security you
can use another SSH key with restrictions on the command that oar can
launch with this one). Thus you will have in you init script
something like:</p>
<pre class="literal-block">
sudo -u oar ssh oar-server &quot;oarnodesetting -s Alive -h node_hostname&quot;
    or
sudo -u oar ssh oar-server &quot;oarnodesetting -s Absent -h node_hostname&quot;
</pre>
<p>In this case, further OAR software upgrade will be more painless.</p>
</li>
</ol>
</blockquote>
</div>
<div class="section" id="how-can-i-manage-scheduling-queues">
<h2><a class="toc-backref" href="#id73">9.4&nbsp;&nbsp;&nbsp;How can I manage scheduling queues?</a></h2>
<p>see <a class="reference internal" href="#oarnotify">oarnotify</a>.</p>
</div>
<div class="section" id="how-can-i-handle-licence-tokens">
<h2><a class="toc-backref" href="#id74">9.5&nbsp;&nbsp;&nbsp;How can I handle licence tokens?</a></h2>
<p>OAR does not manage resources with an empty &quot;network_address&quot;. So you can
define resources that are not linked with a real node.</p>
<p>So the steps to configure OAR with the possibility to reserve licences (or
whatever you want that are other notions):</p>
<blockquote>
<ol class="arabic">
<li><p class="first">Add a new field in the table <a class="reference internal" href="#resources">resources</a> to specify the licence name.</p>
<pre class="literal-block">
oarproperty -a licence -c
</pre>
</li>
<li><p class="first">Add your licence name resources with <a class="reference internal" href="#oarnodesetting">oarnodesetting</a>.</p>
<pre class="literal-block">
oarnodesetting -a -h &quot;&quot; -p type=mathlab -p licence=l1
oarnodesetting -a -h &quot;&quot; -p type=mathlab -p licence=l2
oarnodesetting -a -h &quot;&quot; -p type=fluent -p licence=l1
...
</pre>
</li>
<li><p class="first">Now you have to write an admission rule to force oarsub &quot;-l&quot; option on
resources of the type &quot;default&quot; (node resources) if there is no other
specifications.</p>
<pre class="literal-block">
INSERT INTO admission_rules (rule) VALUES ('
foreach my $mold (&#64;{$ref_resource_list}){
    foreach my $r (&#64;{$mold-&gt;[0]}){
        my $prop = $r-&gt;{property};
        if (($prop !~ /[\\s\\(]type[\\s=]/) and ($prop !~ /^type[\\s=]/)){
            if (!defined($prop)){
                $r-&gt;{property} = &quot;type = \\\'default\\\'&quot;;
            }else{
                $r-&gt;{property} = &quot;($r-&gt;{property}) AND type = \\\'default\\\'&quot;;
            }
        }
    }
}
print(&quot;[ADMISSION RULE] Modify resource description with type constraints\\n&quot;);
');
</pre>
</li>
</ol>
</blockquote>
<p>After this configuration, users can perform submissions like</p>
<pre class="literal-block">
oarsub -I -l &quot;/switch=2/nodes=10+{type = 'mathlab'}/licence=20&quot;
</pre>
<p>So users ask OAR to give them some other resource types but nothing block
their program to take more licences than they asked.
You can resolve this problem with the <a class="reference internal" href="#server-script-exec-file">SERVER_SCRIPT_EXEC_FILE</a> configuration.
In these files you have to bind OAR allocated resources to the licence servers
to restrict user consumptions to what they asked. This is very dependant of
the licence management.</p>
</div>
<div class="section" id="how-can-i-handle-multiple-clusters-with-one-oar">
<h2><a class="toc-backref" href="#id75">9.6&nbsp;&nbsp;&nbsp;How can I handle multiple clusters with one OAR?</a></h2>
<p>These are the steps to follow:</p>
<blockquote>
<ol class="arabic">
<li><p class="first">create a resource property to identify the corresponding cluster (like &quot;cluster&quot;):</p>
<pre class="literal-block">
oarproperty -a cluster
</pre>
<p>(you can see this new property when you use oarnodes)</p>
</li>
<li><p class="first">with <a class="reference internal" href="#oarnodesetting">oarnodesetting</a> you have to fill this field for all resources; for example:</p>
<pre class="literal-block">
oarnodesetting -h node42.cluster1.com -p cluster=1
oarnodesetting -h node43.cluster1.com -p cluster=1
oarnodesetting -h node2.cluster2.com -p cluster=2
...
</pre>
</li>
<li><p class="first">Then you have to restrict properties for new job type.
So an admission rule performs this job (this is a SQL syntax to use in a database interpreter):</p>
<pre class="literal-block">
INSERT IGNORE INTO admission_rules (rule) VALUES ('
    my $cluster_constraint = 0;
    if (grep(/^cluster1$/, &#64;{$type_list})){
        $cluster_constraint = 1;
    }elsif (grep(/^cluster2$/, &#64;{$type_list})){
        $cluster_constraint = 2;
    }
if ($cluster_constraint &gt; 0){
    if ($jobproperties ne &quot;&quot;){
        $jobproperties = &quot;($jobproperties) AND cluster = $cluster_constraint&quot;;
    }else{
        $jobproperties = &quot;cluster = $cluster_constraint&quot;;
    }
    print(&quot;[ADMISSION RULE] Added automatically cluster resource constraint\\n&quot;);
}
');
</pre>
</li>
<li><p class="first">Edit the admission rule which checks the right job types and add
&quot;cluster1&quot; and &quot;cluster2&quot; in.</p>
</li>
</ol>
</blockquote>
<p>So when you will use oarsub to submit a &quot;cluster2&quot; job type only resources
with the property &quot;cluster=2&quot; is used. This is the same when you will use the
&quot;cluster1&quot; type.</p>
</div>
<div class="section" id="how-to-configure-a-more-ecological-cluster-or-how-to-make-some-power-consumption-economies">
<h2><a class="toc-backref" href="#id76">9.7&nbsp;&nbsp;&nbsp;How to configure a more ecological cluster (or how to make some power consumption economies)?</a></h2>
<p>This feature can be performed with the <cite>Dynamic nodes coupling features</cite>.</p>
<p>First you have to make sure that you have a command to wake up a computer
that is stopped. For example you can use the WoL (Wake on Lan) feature
(generally you have to right configure the BIOS and add right options to the
Linux Ethernet driver; see &quot;ethtool&quot;).</p>
<p>If you want to enable a node to be woke up the next 12 hours:</p>
<pre class="literal-block">
((DATE=$(date +%s)+3600*12))
oarnodesetting -h host_name -p cm_availability=$DATE
</pre>
<p>Otherwise you can disable the wake up of nodes (but not the halt) by:</p>
<pre class="literal-block">
oarnodesetting -h host_name -p cm_availability=1
</pre>
<p>If you want to disable the halt on a node (but not the wakeup):</p>
<pre class="literal-block">
oarnodesetting -h host_name -p cm_availability=2147483647
</pre>
<p>2147483647 = 2^31 - 1 : we take this value as infinite and it is used to
disable the halt mechanism.</p>
<p>And if you want to disable the halt and the wakeup:</p>
<pre class="literal-block">
oarnodesetting -h host_name -p cm_availability=0
</pre>
<p>Note: In the unstable 2.4 OAR version, cm_availability has been renamed
into available_upto.</p>
<p>Your <a class="reference internal" href="#scheduler-node-manager-wake-up-cmd">SCHEDULER_NODE_MANAGER_WAKE_UP_CMD</a> must be a script that read node
names and translate them into the right wake up command.</p>
<p>So with the right OAR and node configurations you can optimize the power
consumption of your cluster (and your air conditioning infrastructure)
without drawback for the users.</p>
<p>Take a look at your cluster occupation and your electricity bill to know if it
could be interesting for you ;-)</p>
</div>
<div class="section" id="how-to-configure-temporary-uid-for-each-job">
<h2><a class="toc-backref" href="#id77">9.8&nbsp;&nbsp;&nbsp;How to configure temporary UID for each job?</a></h2>
<p>For a better way to handle job processes we introduce the temporary user id
feature.</p>
<p>This feature creates a user for each job on assigned nodes. Hence it is
possible to clean temporary files, IPC, every generated processes, ...
Furthermore a lot of system features could be used like bandwidth management
(iptables rules on the user id).</p>
<p>To configure this feature, CPUSET must be activated and the tag
JOB_RESOURCE_MANAGER_JOB_UID_TYPE has to be configured in the oar.conf file.
The value is the content of the &quot;type&quot; field into the <a class="reference internal" href="#resources">resources</a> table. After
that you have to add resources in the database with this type and fill the
cpuset field with a unique UID (not used by real users). The maximum number of
concurrent jobs is the number of resources of this type.</p>
<p>For example, if you put this in your oar.onf:</p>
<pre class="literal-block">
JOB_RESOURCE_MANAGER_PROPERTY_DB_FIELD=&quot;cpuset&quot;
JOB_RESOURCE_MANAGER_JOB_UID_TYPE=&quot;user&quot;
</pre>
<p>Then you can add temporary UID:</p>
<pre class="literal-block">
oarnodesetting -a -h fake -p cpuset=23000 -p type=user
oarnodesetting -a -h fake -p cpuset=23001 -p type=user
oarnodesetting -a -h fake -p cpuset=23002 -p type=user
...
</pre>
<p>You can put what you want in the place of the hostname (here &quot;fake&quot;).</p>
<p>The drawback of this feature is that users haven't their UID only their GID.</p>
</div>
<div class="section" id="how-to-enable-jobs-to-connect-to-the-frontales-from-the-nodes-using-oarsh">
<h2><a class="toc-backref" href="#id78">9.9&nbsp;&nbsp;&nbsp;How to enable jobs to connect to the frontales from the nodes using oarsh?</a></h2>
<p>First you have to install the node part of OAR on the wanted nodes.</p>
<p>After that you have to register the frontales into the database using
oarnodesetting with the &quot;frontal&quot; (for example) type and assigned the desired
cpus into the cpuset field; for example:</p>
<pre class="literal-block">
oarnodesetting -a -h frontal1 -p type=frontal -p cpuset=0
oarnodesetting -a -h frontal1 -p type=frontal -p cpuset=1
oarnodesetting -a -h frontal2 -p type=frontal -p cpuset=0
...
</pre>
<p>Thus you will be able to see resources identifier of these resources with
oarnodes; try to type:</p>
<pre class="literal-block">
oarnodes --sql &quot;type='frontal'&quot;
</pre>
<p>Then put this type name (here &quot;frontal&quot;) into the <em>oar.conf</em> file on the OAR
server into the tag <a class="reference internal" href="#scheduler-resources-always-assigned-type">SCHEDULER_RESOURCES_ALWAYS_ASSIGNED_TYPE</a>.</p>
<dl class="docutils">
<dt>Notes:</dt>
<dd><ul class="first last simple">
<li>if one of these resources become &quot;Suspected&quot; then the scheduling will
stop.</li>
<li>you can disable this feature with <a class="reference internal" href="#oarnodesetting">oarnodesetting</a> and put these resources
into the &quot;Absent&quot; state.</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="a-job-remains-in-the-finishing-state-what-can-i-do">
<h2><a class="toc-backref" href="#id79">9.10&nbsp;&nbsp;&nbsp;A job remains in the &quot;Finishing&quot; state, what can I do?</a></h2>
<p>If you have waited more than a couple of minutes (10mn for example) then
something wrong occurred (frontal has crashed, out of memory, ...).</p>
<p>So you are able to turn manually a job into the &quot;Error&quot; state by typing in
the OAR install directory with the root user (example with a bash shell):</p>
<pre class="literal-block">
export OARCONFFILE=/etc/oar/oar.conf
perl -e 'use oar_iolib; $db = iolib::connect(); iolib::set_job_state($db,42,&quot;Error&quot;)'
</pre>
<p>(Replace 42 by your job identifier)</p>
</div>
<div class="section" id="how-can-i-write-my-own-scheduler">
<h2><a class="toc-backref" href="#id80">9.11&nbsp;&nbsp;&nbsp;How can I write my own scheduler?</a></h2>
<p>Definition of the scheduler's interface:</p>
<p>The scheduler can be any executable file, stored in the schedulers repository
(/usr/lib/oar/schedulers/ for installations using the Debian packages).
The scheduler is executed by the &quot;metascheduler&quot;, that handles sequentially the
different queues by priority order and make the glue between them (besteffort
jobs to kill, etc).
THe scheduler executable has the following interface:</p>
<blockquote>
<ul>
<li><dl class="first docutils">
<dt>it gets 3 arguments:</dt>
<dd><ol class="first last arabic simple">
<li>queue name</li>
<li>reference time in second</li>
<li>reference time in sql format (for conveniance only)</li>
</ol>
</dd>
</dl>
</li>
<li><p class="first">only jobs of your queue and with the state &quot;Waiting&quot; or &quot;Reservation =
None&quot; should be manipulated</p>
</li>
<li><p class="first">any information stored in the database can however be taken into account
(read).</p>
</li>
<li><p class="first">previous decisions of the other schedulers (other queues) should be taken
into account: information from tables <a class="reference internal" href="#gantt-jobs-predictions">gantt_jobs_predictions</a> and
<a class="reference internal" href="#gantt-jobs-resources">gantt_jobs_resources</a>, in order to avoid conflicts between jobs of
different queues.</p>
</li>
<li><p class="first">decisions must be stored in the tables <a class="reference internal" href="#gantt-jobs-predictions">gantt_jobs_predictions</a> and
<a class="reference internal" href="#gantt-jobs-resources">gantt_jobs_resources</a></p>
</li>
<li><p class="first">the state of some jobs can be set to &quot;toError&quot; so that OAR delete them
after the scheduler's run. If any job is set to that state, the scheduler
must return an exit code equal to 1, in order to notify the metascheduler,
otherwise exit code must be 0.</p>
</li>
</ul>
</blockquote>
<p>As an example, you can look at the default OAR scheduler
&quot;oar_sched_gantt_with_timesharing&quot;.
It uses a gantt and a resource tree libraries that are essential to take some
decisions.</p>
</div>
<div class="section" id="what-is-the-syntax-of-this-documentation">
<h2><a class="toc-backref" href="#id81">9.12&nbsp;&nbsp;&nbsp;What is the syntax of this documentation?</a></h2>
<p>We are using the RST format from the <a class="reference external" href="http://docutils.sourceforge.net/">Docutils</a> project. This syntax is easily readable
and can be converted into HTML, LaTex or XML.</p>
<p>You can find basic informations on
<a class="reference external" href="http://docutils.sourceforge.net/docs/user/rst/quickref.html">http://docutils.sourceforge.net/docs/user/rst/quickref.html</a></p>
</div>
</div>
<div class="section" id="oar-changelog">
<h1><a class="toc-backref" href="#id82">10&nbsp;&nbsp;&nbsp;OAR CHANGELOG</a></h1>
<div class="section" id="next-version">
<h2><a class="toc-backref" href="#id83">10.1&nbsp;&nbsp;&nbsp;next version</a></h2>
<blockquote>
<ul class="simple">
<li>Bugfix: Fix a regression (only for PostgreSQL) in Drawgantt introduced in
the version 2.4.6 (thanks to Yann Genevois).</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-4-7">
<h2><a class="toc-backref" href="#id84">10.2&nbsp;&nbsp;&nbsp;version 2.4.7:</a></h2>
<blockquote>
<ul class="simple">
<li>Backport: Debug checkpoint feature with cosystem or deploy jobs.</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-4-6">
<h2><a class="toc-backref" href="#id85">10.3&nbsp;&nbsp;&nbsp;version 2.4.6:</a></h2>
<blockquote>
<ul class="simple">
<li>Fix the user variable used in oarsh. When using oarsh from the frontal, the
variable OAR_USER was not defined in the environment, and make oarsh unable
to read the user private key file.</li>
<li>Backport: Bugfix #13434: reservation were not handled correctly with the
energy saving feature</li>
<li>Draw-Gantt: Do not display Absent node in the future that are in the stanby &quot;sub-state&quot;.</li>
<li>Bugfix: spelling error (network_addess &gt; network_address)</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-4-5">
<h2><a class="toc-backref" href="#id86">10.4&nbsp;&nbsp;&nbsp;version 2.4.5:</a></h2>
<blockquote>
<ul class="simple">
<li>backport: node_change_state: do not Suspect the first node of a job which
was EXTERMINATED by Leon if the cpuset feature is configured (let do the
job by the cpuset)</li>
<li>backport: OAREXEC: ESRF detected that sometime oarexec think that he
notified the Almighty with it exit code but nothing was seen on the server.
So try to resend the exit code until oarexec is killed.</li>
<li>backport: oar_Tools: add in notify_almighty a check on the print and on the
close of the socket connected to Almighty.</li>
<li>backport: switch to /bin/bash as default (some scripts currently need bash).</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-4-4">
<h2><a class="toc-backref" href="#id87">10.5&nbsp;&nbsp;&nbsp;version 2.4.4:</a></h2>
<blockquote>
<ul class="simple">
<li>Bug 10999: memory leak into Hulot when used with postgresql. The leak has been
minimized, but it is still there (DBD::Pg bug)</li>
<li>Almighty cleans ipcs used by oar on exit</li>
<li>Bugfix #10641 and #10999 : Hulot is automatically and periodically restarted</li>
<li>oar_resource_init: bad awk delimiter. There's a space and if the property
is the first one then there is not a ','.</li>
<li>job suspend: oardo does not exist anymore (long long time ago). Replace it
with oardodo.</li>
<li>Bugfix: oaradmin rules edition/add was broken</li>
<li>Bug #11599: missing pingchecker line into Leon</li>
<li>Bug #10567: enabling to bypass window mechanism of hulot (Backport from 2.5)</li>
<li>Bug #10568: Wake up timeout changing with the number of nodes
(Backport from 2.5)</li>
<li>oarsub: when an admission rule died micheline returns an integer and not an
array ref. Now oarsub ends nicely.</li>
<li>Monika: add a link on each jobid on the node display area.</li>
<li>sshd_config: with nodes with a lot of core, 10 // connections could be too
few</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-4-3">
<h2><a class="toc-backref" href="#id88">10.6&nbsp;&nbsp;&nbsp;version 2.4.3:</a></h2>
<blockquote>
<ul class="simple">
<li>Hulot module now has customizable keepalive feature
(backport from 2.5)</li>
<li>Added a hook to launch a healing command when nodes are suspected
(backport from 2.5)</li>
<li>Bugfix #9995: oaraccouting script doesn't freeze anymore when db is unreachable.</li>
<li>Bugfix #9990: prevent from inserting jobs with invalid username (like an empty username)</li>
<li>Oarnodecheck improvements: node is not checked if a job is already running</li>
<li>New oaradmin option: --auto-offset</li>
<li>Feature request #10565: add the possibility to check the aliveness of the
nodes of a job at the end of this one (pingchecker)</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-4-2">
<h2><a class="toc-backref" href="#id89">10.7&nbsp;&nbsp;&nbsp;version 2.4.2:</a></h2>
<blockquote>
<ul class="simple">
<li>New &quot;Hulot&quot; module for intelligent and configurable energy saving</li>
<li>Bug #9906: fix bad optimization in the gantt lib (so bad scheduling</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-4-1">
<h2><a class="toc-backref" href="#id90">10.8&nbsp;&nbsp;&nbsp;version 2.4.1:</a></h2>
<blockquote>
<ul class="simple">
<li>Bug #9038: Security flaw in oarsub --notify option</li>
<li>Bug #9601: Cosystem jobs are no more killed when a resource is set to Absent</li>
<li>Fixed some packaging bugs</li>
<li>API bug fixes in job submission parsing</li>
<li>Added standby info into <cite>oarnodes -s</cite> and available_upto info into
/resources uri of the API</li>
<li>Bug Grid'5000 #2687 Fix possible crashes of the scheduler.</li>
<li>Bug fix: with MySQL DB Finaud suspected resources which are not of the
&quot;default&quot; type.</li>
<li>Signed debian packages (install oar-keyring package)</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-4-0">
<h2><a class="toc-backref" href="#id91">10.9&nbsp;&nbsp;&nbsp;version 2.4.0:</a></h2>
<blockquote>
<ul>
<li><p class="first">Fix bug in oarnodesetting command generated by oar_resources_init (detect_resources)</p>
</li>
<li><p class="first">Added a --state option to oarstat to only get the status of specified jobs
(optimized query, to allow scripting)</p>
</li>
<li><p class="first">Added a REST API for OAR and OARGRID</p>
</li>
<li><p class="first">Added JSON support into oarnodes, oarstat and oarsub</p>
</li>
<li><p class="first">New Makefile adapted to build packages as non-root user</p>
</li>
<li><p class="first">add the command &quot;oar_resources_init&quot; to easily detect and initialize the
whole resources of a cluster.</p>
</li>
<li><p class="first">&quot;oaradmin version&quot; : now retrieve the most recent database schema number</p>
</li>
<li><p class="first">Fix rights on the &quot;schema&quot; table in postgresql.</p>
</li>
<li><p class="first">Bug #7509: fix bug in add_micheline_subjob for array jobs + jobtypes</p>
</li>
<li><p class="first">Ctrl-C was not working anymore in oarsub.
It seems that the signal handler does not handle the previous syntax
($SIG = 'qdel')</p>
</li>
<li><p class="first">Fix bug in oarsh with the &quot;-l&quot; option</p>
</li>
<li><p class="first">Bug #7487: bad initialisation of the gnatt for the container jobs.</p>
</li>
<li><p class="first">Scheduler: move the &quot;delete_unnecessary_subtrees&quot; directly into
&quot;find_first_hole&quot;. Thus this is possible to query a job like:</p>
<pre class="literal-block">
oarsub -I -l nodes=1/core=1+nodes=4/core=2
(no hard separation between each group)
</pre>
<dl class="docutils">
<dt>For the same behaviour as before, you can query:</dt>
<dd><p class="first last">oarsub -I -l {prop=1}/nodes=1/core=1+{prop=2}/nodes=4/core=2</p>
</dd>
</dl>
</li>
<li><p class="first">Bug #7634: test if the resource property value is effectively defined
otherwise print a ''</p>
</li>
<li><p class="first">Optional script to take into account cpu/core topology of the nodes at boot
time (to activate inside oarnodesetting_ssh)</p>
</li>
<li><p class="first">Bug #7174: Cleaned default PATH from &quot;./&quot; into oardodo</p>
</li>
<li><p class="first">Bug #7674: remove the computation of the scheduler_priority field for
besteffort jobs from the asynchronous OAR part. Now the value is set when
the jobs are turned into toLaunch state and in Error/Terminated.</p>
</li>
<li><p class="first">Bug #7691: add --array and --array-param-file options parsing into the
submitted script. Fix also some parsing errors.</p>
</li>
<li><p class="first">Bug #7962: enable resource property &quot;cm_availability&quot; to be manipulated by
the oarnodesetting command</p>
</li>
<li><dl class="first docutils">
<dt>Added the (standby) information to a node state in oarnodes when it's state</dt>
<dd><p class="first last">is Absent and cm_availability != 0</p>
</dd>
</dl>
</li>
<li><p class="first">Changed the name of cm_availability to available_upto which is more relevant</p>
</li>
<li><p class="first">add a --maintenance option to oarnodesetting that sets the state of a resource
to Absent and its available_upto to 0 if maintenance is on and resets previous
values if maintenance is off.</p>
</li>
<li><p class="first">added a --signal option to oardel that allow a user to send a signal to one of
his jobs</p>
</li>
<li><p class="first">added a name field in the schema table that will refer to the OAR version name</p>
</li>
<li><p class="first">added a table containing scheduler name, script and description</p>
</li>
<li><p class="first">Bug #8559: Almighty: Moved OAREXEC_XXXX management code out of the queue for
immediate action, to prevent potential problems in case of scheduler timeouts.</p>
</li>
<li><p class="first">oarnodes, oarstat and the REST API are no more making retry connections to the
database in case of failure, but exit with an error instead. The retry behavior
is left for daemons.</p>
</li>
<li><p class="first">improved packaging (try to install files in more standard places)</p>
</li>
<li><p class="first">improved init script for Almighty (into deb and rpm packages)</p>
</li>
<li><p class="first">fixed performance issue on oarstat (array_id index missing)</p>
</li>
<li><p class="first">fixed performance issue (job_id index missing in event_log table)</p>
</li>
<li><p class="first">fixed a performance issue at job submission (optimized a query and added an
index on challenges table)
decisions).</p>
</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-3-5">
<h2><a class="toc-backref" href="#id92">10.10&nbsp;&nbsp;&nbsp;version 2.3.5:</a></h2>
<blockquote>
<ul class="simple">
<li>Bug #8139: Drawgantt nil error (Add condition to test the presence of nil
value in resources table.)</li>
<li>Bug #8416: When a the automatic halt/wakeup feature is enabled then there
was a problem to determine idle nodes.</li>
<li>Debug a mis-initialization of the Gantt with running jobs in the
metascheduler (concurrency access to PG database)</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-3-4">
<h2><a class="toc-backref" href="#id93">10.11&nbsp;&nbsp;&nbsp;version 2.3.4:</a></h2>
<blockquote>
<ul class="simple">
<li>add the command &quot;oar_resources_init&quot; to easily detect and initialize the
whole resources of a cluster.</li>
<li>&quot;oaradmin version&quot; : now retrieve the most recent database schema number</li>
<li>Fix rights on the &quot;schema&quot; table in postgresql.</li>
<li>Bug #7509: fix bug in add_micheline_subjob for array jobs + jobtypes</li>
<li>Ctrl-C was not working anymore in oarsub.
It seems that the signal handler does not handle the previous syntax
($SIG = 'qdel')</li>
<li>Bug #7487: bad initialisation of the gnatt for the container jobs.</li>
<li>Fix bug in oarsh with the &quot;-l&quot; option</li>
<li>Bug #7634: test if the resource property value is effectively defined
otherwise print a ''</li>
<li>Bug #7674: remove the computation of the scheduler_priority field for
besteffort jobs from the asynchronous OAR part. Now the value is set when
the jobs are turned into toLaunch state and in Error/Terminated.</li>
<li>Bug #7691: add --array and --array-param-file options parsing into the
submitted script. Fix also some parsing errors.</li>
<li>Bug #7962: enable resource property &quot;cm_availability&quot; to be manipulated by
the oarnodesetting command</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-3-3">
<h2><a class="toc-backref" href="#id94">10.12&nbsp;&nbsp;&nbsp;version 2.3.3:</a></h2>
<blockquote>
<ul>
<li><p class="first">Fix default admission rules: case unsensitive check for properties used in
oarsub</p>
</li>
<li><p class="first">Add new oaradmin subcommand : oaradmin conf. Useful to edit conf files and
keep changes in a Subversion repository.</p>
</li>
<li><p class="first">Kill correctly each taktuk command children in case of a timeout.</p>
</li>
<li><p class="first">New feature: array jobs (option --array)  (on oarsub, oarstat oardel,
oarhold and oarresume) and file-based parametric array jobs
(oarsub --array-param-file)
/!in this version the DB scheme has changed. If you want to upgrade your
installation from a previous 2.3 release then you have to execute in your
database one of these SQL script (stop OAR before):</p>
<pre class="literal-block">
mysql:
    DB/mysql_structure_upgrade_2.3.1-2.3.3.sql

postgres:
    DB/pg_structure_upgrade_2.3.1-2.3.3.sql
</pre>
</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-3-2">
<h2><a class="toc-backref" href="#id95">10.13&nbsp;&nbsp;&nbsp;version 2.3.2:</a></h2>
<blockquote>
<ul class="simple">
<li>Change scheduler timeout implementation to schedule the maximum of jobs.</li>
<li>Bug #5879: do not show initial_request in oarstat when it is not a job of
the user who launched the oarstat command (oar or root).</li>
<li>Add a --event option to oarnodes and oarstat to display events recorded for
a job or node</li>
<li>Display reserved resources for a validated waiting reservation, with a hint
in their state</li>
<li>Fix oarproperty: property names are lowercase</li>
<li>Fix OAR_JOB_PROPERTIES_FILE: do not display system properties</li>
<li>Add a new user command: oarprint which allow to pretty print resource
properties of a job</li>
<li>Debug temporary job UID feature</li>
<li>Add 'kill -9' on subprocesses that reached a timeout (avoid Perl to
wait something)</li>
<li>desktop computing feature is now available again. (ex: oarsub -t
desktop_computing date)</li>
<li>Add versioning feature for admission rules with Subversion</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-3-1">
<h2><a class="toc-backref" href="#id96">10.14&nbsp;&nbsp;&nbsp;version 2.3.1:</a></h2>
<blockquote>
<ul class="simple">
<li>Add new oarmonitor command. This will permit to monitor OAR jobs on compute
nodes.</li>
<li>Remove sudo dependency and replace it by the commands &quot;oardo&quot; and
&quot;oardodo&quot;.</li>
<li>Add possibility to create a temporary user for each jobs on compute nodes.
So you can perform very strong restrictions for each job (ex: bandwidth
restrictions with iptable, memory management, ... everything that can be
handled with a user id)</li>
<li>Debian packaging: Run OAR specific sshd with root privileges (under heavy
load, kernel may be more responsive for root processes...)</li>
<li>Remove ALLOWED_NETWORKS tag in oar.conf (added more complexeity than
resolving problems)</li>
<li>/!change database scheme for the field <em>exit_code</em> in the table <em>jobs</em>.
Now <em>oarstat</em> <em>exit_code</em> line reflects the right exit code of the user
passive job (before, even when the user script was not launched the
<em>exit_code</em> was 0 which was BAD)</li>
<li>/!add DB field <em>initial_request</em> in the table <em>jobs</em> that stores the
oarsub line of the user</li>
<li>Feature Request #4868: Add a parameter to specify what the &quot;nodes&quot; resource
is a synomym for. Network_address must be seen as an internal data and not
used.</li>
<li>Scheduler: add timeout for each job == 1/4 of the remaining scheduler
timeout.</li>
<li>Bug #4866: now the whole node is Suspected instead of just the par where
there is no job onto. So it is possible to have a job on Suspected nodes.</li>
<li>Add job walltime (in seconds) in parameter of prologue and epilogue on
compute nodes.</li>
<li>oarnodes does not show system properties anymore.</li>
<li>New feature: container job type now allows to submit inner jobs for a
scheduling within the container job</li>
<li>Monika refactoring and now in the oar packaging.</li>
<li>Added a table schema in the db with the field version, reprensenting the
version of the db schema.</li>
<li>Added a field DB_PORT in the oar config file.</li>
<li>Bug #5518: add right initialization of the job user name.</li>
<li>Add new oaradmin command. This will permit to create resources and
manage admission rules more easily.</li>
<li>Bug #5692: change source code into a right Perl 5.10 syntax.</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-2-12">
<h2><a class="toc-backref" href="#id97">10.15&nbsp;&nbsp;&nbsp;version 2.2.12:</a></h2>
<blockquote>
<ul class="simple">
<li>Bug #5239: fix the bug if there are spaces into job name or project</li>
<li>Fix the bug in Iolib if DEAD_SWITCH_TIME &gt;0</li>
<li>Fix a bug in bipbip when calling the cpuset_manager to clean jobs in error</li>
<li>Bug #5469: fix the bug with reservations and Dead resources</li>
<li>Bug #5535: checks for reservations made at a same time was wrong.</li>
<li>New feature: local checks on nodes can be plugged in the oarnodecheck
mechanism. Results can be asynchronously checked from the server (taktuk
ping checker)</li>
<li>Add 2 new tables to keep track of the scheduling decisions
(gantt_jobs_predictions_log and gantt_jobs_resources_log). This will help
debugging scheduling troubles (see SCHEDULER_LOG_DECISIONS in oar.conf)</li>
<li>Now reservations are scheduled only once (at submission time). Resources
allocated to a reservations are definitively set once the validated is
done and won't change in next scheduler's pass.</li>
<li>Fix DrawGantt to not display besteffort jobs in the future which is
meaningless.</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-2-11">
<h2><a class="toc-backref" href="#id98">10.16&nbsp;&nbsp;&nbsp;version 2.2.11:</a></h2>
<blockquote>
<ul class="simple">
<li>Fix Debian package dependency on a CGI web server.</li>
<li>Fix little bug: remove notification (scheduled start time) for Interactive
reservation.</li>
<li>Fix bug in reservation: take care of the SCHEDULER_JOB_SECURITY_TIME for
reservations to check.</li>
<li>Fix bug: add a lock around the section which creates and feed the OAR
cpuset.</li>
<li>Taktuk command line API has changed (we need taktuk &gt;= 3.6).</li>
<li>Fix extra ' in the name of output files when using a job name.</li>
<li>Bug #4740: open the file in oarsub with user privileges (-S option)</li>
<li>Bug #4787: check if the remote socket is defined (problem of timing with
nmap)</li>
<li>Feature Request #4874: check system names when renaming properties</li>
<li>DrawGantt can export charts to be reused to build a global multi-OAR view
(e.g. DrawGridGantt).</li>
<li>Bug #4990: DrawGantt now uses the database localtime as its time reference.</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-2-10">
<h2><a class="toc-backref" href="#id99">10.17&nbsp;&nbsp;&nbsp;version 2.2.10:</a></h2>
<blockquote>
<ul class="simple">
<li>Job dependencies: if the required jobs do not have an exit code == 0 and in
the state Terminated then the schedulers refuse to schedule this job.</li>
<li>Add the possibility to disable the halt command on nodes with
cm_availability value.</li>
<li>Enhance oarsub &quot;-S&quot; option (more #OAR parsed).</li>
<li>Add the possibility to use oarsh without configuring the CPUSETs (can be
useful for users that don't want to configure there ssh keys)</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-2-9">
<h2><a class="toc-backref" href="#id100">10.18&nbsp;&nbsp;&nbsp;version 2.2.9:</a></h2>
<blockquote>
<ul class="simple">
<li>Bug 4225: Dump only 1 data structure when using -X or -Y or -D.</li>
<li>Bug fix in Finishing sequence (Suspect right nodes).</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-2-8">
<h2><a class="toc-backref" href="#id101">10.19&nbsp;&nbsp;&nbsp;version 2.2.8:</a></h2>
<blockquote>
<ul class="simple">
<li>Bug 4159: remove unneeded Dump print from oarstat.</li>
<li>Bug 4158: replace XML::Simple module by XML::Dumper one.</li>
<li>Bug fix for reservation (recalculate the right walltime).</li>
<li>Print job dependencies in oarstat.</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-2-7">
<h2><a class="toc-backref" href="#id102">10.20&nbsp;&nbsp;&nbsp;version 2.2.7:</a></h2>
</div>
<div class="section" id="id1">
<h2><a class="toc-backref" href="#id103">10.21&nbsp;&nbsp;&nbsp;version 2.2.11:</a></h2>
<blockquote>
<ul class="simple">
<li>Fix Debian package dependency on a CGI web server.</li>
<li>Fix little bug: remove notification (scheduled start time) for Interactive
reservation.</li>
<li>Fix bug in reservation: take care of the SCHEDULER_JOB_SECURITY_TIME for
reservations to check.</li>
<li>Fix bug: add a lock around the section which creates and feed the OAR
cpuset.</li>
<li>Taktuk command line API has changed (we need taktuk &gt;= 3.6).</li>
<li>Fix extra ' in the name of output files when using a job name.</li>
<li>Bug #4740: open the file in oarsub with user privileges (-S option)</li>
<li>Bug #4787: check if the remote socket is defined (problem of timing with
nmap)</li>
<li>Feature Request #4874: check system names when renaming properties</li>
<li>DrawGantt can export charts to be reused to build a global multi-OAR view
(e.g. DrawGridGantt).</li>
<li>Bug #4990: DrawGantt now uses the database localtime as its time reference.</li>
</ul>
</blockquote>
</div>
<div class="section" id="id2">
<h2><a class="toc-backref" href="#id104">10.22&nbsp;&nbsp;&nbsp;version 2.2.10:</a></h2>
<blockquote>
<ul class="simple">
<li>Job dependencies: if the required jobs do not have an exit code == 0 and in
the state Terminated then the schedulers refuse to schedule this job.</li>
<li>Add the possibility to disable the halt command on nodes with
cm_availability value.</li>
<li>Enhance oarsub &quot;-S&quot; option (more #OAR parsed).</li>
<li>Add the possibility to use oarsh without configuring the CPUSETs (can be
useful for users that don't want to configure there ssh keys)</li>
</ul>
</blockquote>
</div>
<div class="section" id="id3">
<h2><a class="toc-backref" href="#id105">10.23&nbsp;&nbsp;&nbsp;version 2.2.9:</a></h2>
<blockquote>
<ul class="simple">
<li>Bug 4225: Dump only 1 data structure when using -X or -Y or -D.</li>
<li>Bug fix in Finishing sequence (Suspect right nodes).</li>
</ul>
</blockquote>
</div>
<div class="section" id="id4">
<h2><a class="toc-backref" href="#id106">10.24&nbsp;&nbsp;&nbsp;version 2.2.8:</a></h2>
<blockquote>
<ul class="simple">
<li>Bug 4159: remove unneeded Dump print from oarstat.</li>
<li>Bug 4158: replace XML::Simple module by XML::Dumper one.</li>
<li>Bug fix for reservation (recalculate the right walltime).</li>
<li>Print job dependencies in oarstat.</li>
</ul>
</blockquote>
</div>
<div class="section" id="id5">
<h2><a class="toc-backref" href="#id107">10.25&nbsp;&nbsp;&nbsp;version 2.2.7:</a></h2>
<blockquote>
<ul class="simple">
<li>Bug 4106: fix oarsh and oarcp issue with some options (erroneous leading
space).</li>
<li>Bug 4125: remove exit_code data when it is not relevant.</li>
<li>Fix potential bug when changing asynchronously the state of the jobs into
&quot;Terminated&quot; or &quot;Error&quot;.</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-2-6">
<h2><a class="toc-backref" href="#id108">10.26&nbsp;&nbsp;&nbsp;version 2.2.6:</a></h2>
<blockquote>
<ul>
<li><dl class="first docutils">
<dt>Bug fix: job types was not sent to cpuset manager script anymore.</dt>
<dd><p class="first last">(border effect from bug 4069 resolution)</p>
</dd>
</dl>
</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-2-5">
<h2><a class="toc-backref" href="#id109">10.27&nbsp;&nbsp;&nbsp;version 2.2.5:</a></h2>
<blockquote>
<ul class="simple">
<li>Bug fix: remove user command when oar execute the epilogue script on the
nodes.</li>
<li>Clean debug and mail messages format.</li>
<li>Remove bad oarsub syntax from oarsub doc.</li>
<li>Debug xauth path.</li>
<li>bug 3995: set project correctly when resubmitting a job</li>
<li>debug 'bash -c' on Fedora</li>
<li>bug 4069: reservations with CPUSET_ERROR (remove bad hosts and continue
with a right integrity in the database)</li>
<li>bug 4044: fix free resources query for reservation (get the nearest hole
from the beginning of the reservation)</li>
<li>bug 4013: now Dead, Suspected and Absent resources have different colors in
drawgantt with a popup on them.</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-2-4">
<h2><a class="toc-backref" href="#id110">10.28&nbsp;&nbsp;&nbsp;version 2.2.4:</a></h2>
<blockquote>
<ul class="simple">
<li>Redirect third party commands into oar.log (easier to debug).</li>
<li>Add user info into drawgantt interface.</li>
<li>Some bug fixes.</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-2-3">
<h2><a class="toc-backref" href="#id111">10.29&nbsp;&nbsp;&nbsp;version 2.2.3:</a></h2>
<blockquote>
<ul class="simple">
<li>Debug prologue and epilogue when oarexec receives a signal.</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-2-2">
<h2><a class="toc-backref" href="#id112">10.30&nbsp;&nbsp;&nbsp;version 2.2.2:</a></h2>
<blockquote>
<ul class="simple">
<li>Switch nice value of the user processes into 0 in oarsh_shell (in case of
sshd was launched with a different priority).</li>
<li>debug taktuk zombies in pingchecker and oar_Tools</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-2-1">
<h2><a class="toc-backref" href="#id113">10.31&nbsp;&nbsp;&nbsp;version 2.2.1:</a></h2>
<blockquote>
<ul class="simple">
<li>install the &quot;allow_clasic_ssh&quot; feature by default</li>
<li>debug DB installer</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-2">
<h2><a class="toc-backref" href="#id114">10.32&nbsp;&nbsp;&nbsp;version 2.2:</a></h2>
<blockquote>
<ul class="simple">
<li>oar_server_proepilogue.pl: can be used for server prologue and epilogue to
authorize users to access to nodes that are completely allocated by OAR. If
the whole node is assigned then it kills all jobs from the user if all cpus
are assigned.</li>
<li>the same thing can be done with cpuset_manager_PAM.pl as the script used to
configure the cpuset. More efficent if cpusets are configured.</li>
<li>debug cm_availability feature to switch on and off nodes automatically
depending on waiting jobs.</li>
<li>reservations now take care of cm_availability field</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-1-0">
<h2><a class="toc-backref" href="#id115">10.33&nbsp;&nbsp;&nbsp;version 2.1.0:</a></h2>
<blockquote>
<ul class="simple">
<li>add &quot;oarcp&quot; command to help the users to copy files using oarsh.</li>
<li>add sudo configuration to deal with bash. Now oarsub and oarsh have the
same behaviour as ssh (the bash configuration files are loaded correctly)</li>
<li>bug fix in drawgantt (loose jobs after submission of a moldable one)</li>
<li>add SCHEDULER_RESOURCES_ALWAYS_ASSIGNED_TYPE into oar.conf. Thus admin can
add some resources for each jobs (like frontale node)</li>
<li>add possibility to use taktuk to check the aliveness of the nodes</li>
<li>%jobid% is now replaced in stdout and stderr file names by the effective
job id</li>
<li>change interface to shu down or wake up nodes automatically (now the node
list is read on STDIN)</li>
<li>add OARSUB_FORCE_JOB_KEY in oar.conf. It says to create a job ssh key by
default for each job.</li>
<li>%jobid% is now replaced in the ssh job key name (oarsub -k ...).</li>
<li>add NODE_FILE_DB_FIELD_DISTINCT_VALUES in oar.conf that enables the admin
to configure the generated containt of the OAR_NODE_FILE</li>
<li>change ssh job key oarsub options behaviour</li>
<li>add options &quot;--reinitialize&quot; and &quot;--delete-before&quot; to the oaraccounting
command</li>
<li>cpuset are now stored in /dev/cpuset/oar</li>
<li>debian packaging: configure and launch a specific sshd for the user oar</li>
<li>use a file descriptor to send the node list --&gt; able to handle a very large
amount of nodes</li>
<li>every config files are now in /etc/oar/</li>
<li>oardel can add a besteffort type to jobs and vis versa</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-0-2">
<h2><a class="toc-backref" href="#id116">10.34&nbsp;&nbsp;&nbsp;version 2.0.2:</a></h2>
<blockquote>
<ul class="simple">
<li>add warnings and exit code to oarnodesetting when there is a bad node name
or resource number</li>
<li>change package version</li>
<li>change default behaviour for the cpuset_manager.pl (more portable)</li>
<li>enable a user to use the same ssh key for several jobs (at his own risk!)</li>
<li>add node hostnames in oarstat -f</li>
<li>add --accounting and -u options in oarstat</li>
<li>bug fix on index fields in the database (syncro): bug 2020</li>
<li>bug fix about server pro/epilogue: bug 2022</li>
<li>change the default output of oarstat. Now it is usable: bug 1875</li>
<li>remove keys in authorized_keys of oar (on the nodes) that do not
correspond to an active cpuset (clean after a reboot)</li>
<li>reread oar.conf after each database connection tries</li>
<li>add support for X11 forwarding in oarsub -I and -C</li>
<li>debug mysql initialization script in debian package</li>
<li>add a variable in oarsh for the default options of ssh to use
(more useful to change if the ssh version installed does not
handle one of these options)</li>
<li>read oar.conf in oarsh (so admin can more easily change options in this
script)</li>
<li>add support for X11 forwarding via oarsh</li>
<li>change variable for oarsh: OARSH_JOB_ID --&gt; OAR_JOB_ID</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-0-0">
<h2><a class="toc-backref" href="#id117">10.35&nbsp;&nbsp;&nbsp;version 2.0.0:</a></h2>
<blockquote>
<ul>
<li><p class="first">Now, with the ability to declare any type of resources like licences,
VLAN, IP range, computing resources must have the type <em>default</em> and a
network_address not null.</p>
</li>
<li><p class="first">Possibility to declare associated resources like licences, IP ranges, ...
and to reserve them like others.</p>
</li>
<li><p class="first">Now you can connect to your jobs (not only for reservations).</p>
</li>
<li><p class="first">Add &quot;cosystem&quot; job type (execute and do nothing for these jobs).</p>
</li>
<li><p class="first">New scheduler : &quot;oar_sched_gantt_with_timesharing&quot;. You can specify jobs
with the type &quot;timesharing&quot; that indicates that this scheduler can launch
more than 1 job on a resource at a time. It is possible to restrict this
feature with words &quot;user and name&quot;. For example, '-t
timesharing=user,name' indicates that only a job from the same user with
the same name can be launched in the same time than it.</p>
</li>
<li><p class="first">Add PostGresSQL support. So there is a choice to make between MySQL and
PostgresSQL.</p>
</li>
<li><p class="first">New approach for the scheduling : administrators have to insert into the
databases descriptions about resources and not nodes. Resources have a
network address (physical node) and properties. For example, if you have
dual-processor, then you can create 2 different resources with the same
natwork address but with 2 different processor names.</p>
</li>
<li><p class="first">The scheduler can now handle resource properties in a hierarchical
manner. Thus, for example, you can do &quot;oarsub -l /switch=1/cpu=5&quot; which
submit a job on 5 processors on the same switch.</p>
</li>
<li><p class="first">Add a signal handler in oarexec and propagate this signal to the user
process.</p>
</li>
<li><p class="first">Support '#OAR -p ...' options in user script.</p>
</li>
<li><dl class="first docutils">
<dt>Add in oar.conf:</dt>
<dd><ul class="first last simple">
<li>DB_BASE_PASSWD_RO : for security issues, it is possible to execute
request with parts specified by users with a read only account (like
&quot;-p&quot; option).</li>
<li>OARSUB_DEFAULT_RESOURCES : when nothing is specified with the oarsub
command then OAR takes this default resource description.</li>
<li>OAREXEC_DEBUG_MODE : turn on or off debug mode in oarexec (create
/tmp/oar/oar.log on nodes).</li>
<li>FINAUD_FREQUENCY : indicates the frequency when OAR launchs Finaud
(search dead nodes).</li>
<li>SCHEDULER_TIMEOUT : indicates to the scheduler the amount of time
after what it must end itself.</li>
<li>SCHEDULER_JOB_SECURITY_TIME : time between each job.</li>
<li>DEAD_SWITCH_TIME : after this time Absent and Suspected resources are
turned on the Dead state.</li>
<li>PROLOGUE_EPILOGUE_TIMEOUT : the possibility to specify a different
timeout for prologue and epilogue (PROLOGUE_EPILOGUE_TIMEOUT).</li>
<li>PROLOGUE_EXEC_FILE : you can specify the path of the prologue script
executed on nodes.</li>
<li>EPILOGUE_EXEC_FILE : you can specify the path of the epilogue script
executed on nodes.</li>
<li>GENERIC_COMMAND : a specific script may be used instead of ping to
check aliveness of nodes. The script must return bad nodes on STDERR
(1 line for a bad node and it must have exactly the same name that
OAR has given in argument of the command).</li>
<li>JOBDEL_SOFTWALLTIME : time after a normal frag that the system waits
to retry to frag the job.</li>
<li>JOBDEL_WALLTIME : time after a normal frag that the system waits
before to delete the job arbitrary and suspects nodes.</li>
<li>LOG_FILE : specify the path of OAR log file (default :
/var/log/oar.log).</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">Add wait() in pingchecker to avoid zombies.</p>
</li>
<li><p class="first">Better code modularization.</p>
</li>
<li><p class="first">Remove node install part to launch jobs. So it is easier to upgrade from
one version to an other (oarnodesetting must already be installed on each
nodes if we want to use it).</p>
</li>
<li><p class="first">Users can specify a method to be notified (mail or script).</p>
</li>
<li><p class="first">Add cpuset support</p>
</li>
<li><p class="first">Add prologue and epilogue script to be executed on the OAR server before
and after launching a job.</p>
</li>
<li><p class="first">Add dependancy support between jobs (&quot;-a&quot; option in oarsub).</p>
</li>
<li><p class="first">In oarsub you can specify the launching directory (&quot;-d&quot; option).</p>
</li>
<li><p class="first">In oarsub you can specify a job name (&quot;-n&quot; option).</p>
</li>
<li><p class="first">In oarsub you can specify stdout and stderr file names.</p>
</li>
<li><p class="first">User can resubmit a job (option &quot;--resubmit&quot; in oarsub).</p>
</li>
<li><p class="first">It is possible to specify a read only database account and it will be
used to evaluate SQL properties given by the user with the oarsub command
(more scecure).</p>
</li>
<li><p class="first">Add possibility to order assigned resources with their properties by the
scheduler. So you can privilege some resources than others
(SCHEDULER_RESOURCE_ORDER tag in oar.conf file)</p>
</li>
<li><p class="first">a command can be specified to switch off idle nodes
(SCHEDULER_NODE_MANAGER_SLEEP_CMD, SCHEDULER_NODE_MANAGER_IDLE_TIME,
SCHEDULER_NODE_MANAGER_SLEEP_TIME in oar.conf)</p>
</li>
<li><p class="first">a command can be specified to switch on nodes in the Absent state
according to the resource property <em>cm_availability</em> in the table
resources (SCHEDULER_NODE_MANAGER_WAKE_UP_CMD in oar.conf).</p>
</li>
<li><p class="first">if a job goes in Error state and this is not its fault then OAR will
resubmit this one.</p>
</li>
</ul>
</blockquote>
</div>
</div>
</div>
<div class="footer">
<hr class="footer" />
<a class="reference external" href="OAR-DOCUMENTATION-ADMIN.rst">View document source</a>.
Generated on: 2012-11-16 09:26 UTC.
Generated by <a class="reference external" href="http://docutils.sourceforge.net/">Docutils</a> from <a class="reference external" href="http://docutils.sourceforge.net/rst.html">reStructuredText</a> source.

</div>



</div>





</div>

<div id="footer" class="pagefooter">

<div id="pageinfo">









<div class="pagedate">
Last edited <span class="date">Friday 16 November 2012</span>
<!-- Created <span class="date">Wednesday 29 June 2011</span> -->
</div>

</div>


<!-- from OAR -->
</div>

</div>

</body>
</html>
