<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>[OAR Archive] OAR-DOCUMENTATION-USER</title>

<link rel="icon" href="../../../../../favicon.ico" type="image/x-icon" />

<link rel="stylesheet" href="../../../../../style.css" type="text/css" />

<link rel="stylesheet" href="../../../../../local.css" type="text/css" />





</head>
<body>

<div class="page">

<div class="pageheader">
<div class="header">
<span>
<span class="parentlinks">

<a href="../../../../../">OAR</a>/ 

<a href="../../../../../">sources</a>/ 

<a href="../../../../../">2.5</a>/ 

<a href="../../../../../">docs</a>/ 

<a href="../../../../../">documentation</a>/ 

</span>
<span class="title">
OAR-DOCUMENTATION-USER

</span>
</span>

<form method="get" action="http://oar.imag.fr//ikiwiki.cgi" id="searchform">
<div>
<input type="text" id="searchbox" name="P" value="" size="16"
 />
</div>
</form>


</div>





</div>


<div class="sidebar">
<ul>
<li><strong><a href="../../../../../">Home</a></strong></li>
<li><strong><a href="../../../../../about/">About</a></strong></li>
<li><strong><a href="../../../../../news/">News</a></strong></li>
<li><strong><a href="http://oar.imag.fr/archive/wiki-oar">Wiki</a></strong></li>
<li><strong>Getting OAR</strong>
<ul>
<li><a href="../../../../../repositories/">Repositories</a></li>
<li><a href="../../../../../installation/">Installation</a></li>
<li><a href="../../../../../changelog/">Changelog</a></li>
</ul></li>
<li><strong>Using OAR</strong>
<ul>
<li><a href="../../../../../user-quickstart/">First user steps</a></li>
<li><a href="../../../../../user-usecases/">Use cases</a></li>
<li><a href="../../../../../faq/">FAQ</a></li>
</ul></li>
<li><strong>Getting help</strong>
<ul>
<li><a href="../../../../../support/">Contact/Mailing lists</a></li>
<li><a href="../../../../../documentation/">Documentation</a></li>
</ul></li>
<li><strong>Contributing to OAR</strong>
<ul>
<li><a href="../../../../../contributing/repositories/">Source repositories</a></li>
<li><a href="../../../../../contributing/workflow/">Workflow</a></li>
<li><a href="../../../../../contributing/GSOCs/">GSOCs</a></li>
</ul></li>
<li><strong><a href="../../../../../research/">Researchers' Corner</a></strong></li>
<li><strong><a href="../../../../../partners-projects/">Partners &amp; Projects</a></strong></li>
</ul>

</div>


<div id="pagebody">

<div id="content">






OAR Documentation - User Guide



<div class="document" id="oar-documentation-user-guide">
<h1 class="title">OAR Documentation - User Guide</h1>

<img alt="OAR logo" class="align-center" src="../schemas/oar_logo.png" />
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Authors:</th><td class="field-body">Capit Nicolas, Emeras Joseph</td>
</tr>
<tr class="field"><th class="field-name">Address:</th><td class="field-body">Laboratoire d'Informatique de Grenoble
Bat. ENSIMAG - antenne de Montbonnot
ZIRST 51, avenue Jean Kuntzmann
38330 MONTBONNOT SAINT MARTIN</td>
</tr>
<tr class="field"><th class="field-name">Contact:</th><td class="field-body"><a class="reference external" href="mailto:nicolas.capit@imag.fr">nicolas.capit&#64;imag.fr</a>, <a class="reference external" href="mailto:joseph.emeras@imag.fr">joseph.emeras&#64;imag.fr</a></td>
</tr>
<tr class="field"><th class="field-name">Authors:</th><td class="field-body">LIG laboratory</td>
</tr>
<tr class="field"><th class="field-name">Organization:</th><td class="field-body">LIG laboratory</td>
</tr>
<tr class="field"><th class="field-name">Status:</th><td class="field-body">Stable</td>
</tr>
<tr class="field"><th class="field-name">Copyright:</th><td class="field-body">licenced under the GNU GENERAL PUBLIC LICENSE</td>
</tr>
<tr class="field"><th class="field-name">Dedication:</th><td class="field-body">For users.</td>
</tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field"><th class="field-name">Abstract:</th><td class="field-body">OAR is a resource manager (or batch scheduler) for large clusters. By it's
functionnalities, it's near of PBS, LSF, CCS and Condor. It's suitable for
productive plateforms and research experiments.</td>
</tr>
</tbody>
</table>
<p><strong>BE CAREFULL : THIS DOCUMENTATION IS FOR OAR &gt;= 2.3.0</strong></p>
<p>PDF version : <a class="reference external" href="OAR-DOCUMENTATION-USER.pdf">OAR-DOCUMENTATION-USER.pdf</a></p>
<div class="contents topic" id="table-of-contents">
<p class="topic-title first">Table of Contents</p>
<ul class="auto-toc simple">
<li><a class="reference internal" href="#oar-capabilities" id="id7">1&nbsp;&nbsp;&nbsp;OAR capabilities</a></li>
<li><a class="reference internal" href="#description-of-the-different-commands" id="id8">2&nbsp;&nbsp;&nbsp;Description of the different commands</a><ul class="auto-toc">
<li><a class="reference internal" href="#oarstat" id="id9">2.1&nbsp;&nbsp;&nbsp;<em>oarstat</em></a></li>
<li><a class="reference internal" href="#oarnodes" id="id10">2.2&nbsp;&nbsp;&nbsp;<em>oarnodes</em></a></li>
<li><a class="reference internal" href="#oarsub" id="id11">2.3&nbsp;&nbsp;&nbsp;<em>oarsub</em></a></li>
<li><a class="reference internal" href="#oardel" id="id12">2.4&nbsp;&nbsp;&nbsp;<em>oardel</em></a></li>
<li><a class="reference internal" href="#oarhold" id="id13">2.5&nbsp;&nbsp;&nbsp;<em>oarhold</em></a></li>
<li><a class="reference internal" href="#oarresume" id="id14">2.6&nbsp;&nbsp;&nbsp;<em>oarresume</em></a></li>
</ul>
</li>
<li><a class="reference internal" href="#desktop-computing" id="id15">3&nbsp;&nbsp;&nbsp;Desktop computing</a></li>
<li><a class="reference internal" href="#visualisation-tools" id="id16">4&nbsp;&nbsp;&nbsp;Visualisation tools</a><ul class="auto-toc">
<li><a class="reference internal" href="#monika" id="id17">4.1&nbsp;&nbsp;&nbsp;Monika</a></li>
<li><a class="reference internal" href="#drawoargantt" id="id18">4.2&nbsp;&nbsp;&nbsp;DrawOARGantt</a></li>
</ul>
</li>
<li><a class="reference internal" href="#mechanisms" id="id19">5&nbsp;&nbsp;&nbsp;Mechanisms</a><ul class="auto-toc">
<li><a class="reference internal" href="#how-does-an-interactive-oarsub-work" id="id20">5.1&nbsp;&nbsp;&nbsp;How does an interactive <em>oarsub</em> work?</a></li>
<li><a class="reference internal" href="#job-launch" id="id21">5.2&nbsp;&nbsp;&nbsp;Job launch</a></li>
<li><a class="reference internal" href="#cpuset" id="id22">5.3&nbsp;&nbsp;&nbsp;CPUSET</a></li>
<li><a class="reference internal" href="#ssh-connection-key-definition" id="id23">5.4&nbsp;&nbsp;&nbsp;SSH connection key definition</a></li>
<li><a class="reference internal" href="#suspend-resume" id="id24">5.5&nbsp;&nbsp;&nbsp;Suspend/resume</a></li>
<li><a class="reference internal" href="#job-deletion" id="id25">5.6&nbsp;&nbsp;&nbsp;Job deletion</a></li>
<li><a class="reference internal" href="#checkpoint" id="id26">5.7&nbsp;&nbsp;&nbsp;Checkpoint</a></li>
<li><a class="reference internal" href="#scheduling" id="id27">5.8&nbsp;&nbsp;&nbsp;Scheduling</a></li>
<li><a class="reference internal" href="#job-dependencies" id="id28">5.9&nbsp;&nbsp;&nbsp;Job dependencies</a></li>
<li><a class="reference internal" href="#user-notification" id="id29">5.10&nbsp;&nbsp;&nbsp;User notification</a></li>
<li><a class="reference internal" href="#accounting-aggregator" id="id30">5.11&nbsp;&nbsp;&nbsp;Accounting aggregator</a></li>
<li><a class="reference internal" href="#dynamic-nodes-coupling-features" id="id31">5.12&nbsp;&nbsp;&nbsp;Dynamic nodes coupling features</a></li>
<li><a class="reference internal" href="#timesharing" id="id32">5.13&nbsp;&nbsp;&nbsp;Timesharing</a></li>
<li><a class="reference internal" href="#container-jobs" id="id33">5.14&nbsp;&nbsp;&nbsp;Container jobs</a></li>
<li><a class="reference internal" href="#besteffort-jobs" id="id34">5.15&nbsp;&nbsp;&nbsp;Besteffort jobs</a></li>
<li><a class="reference internal" href="#cosystem-jobs" id="id35">5.16&nbsp;&nbsp;&nbsp;Cosystem jobs</a></li>
<li><a class="reference internal" href="#deploy-jobs" id="id36">5.17&nbsp;&nbsp;&nbsp;Deploy jobs</a></li>
<li><a class="reference internal" href="#id1" id="id37">5.18&nbsp;&nbsp;&nbsp;Desktop computing</a></li>
</ul>
</li>
<li><a class="reference internal" href="#faq-user" id="id38">6&nbsp;&nbsp;&nbsp;FAQ - USER</a><ul class="auto-toc">
<li><a class="reference internal" href="#release-policy" id="id39">6.1&nbsp;&nbsp;&nbsp;Release policy</a></li>
<li><a class="reference internal" href="#how-can-i-submit-a-moldable-job" id="id40">6.2&nbsp;&nbsp;&nbsp;How can I submit a moldable job?</a></li>
<li><a class="reference internal" href="#how-can-i-submit-a-job-with-a-non-uniform-description" id="id41">6.3&nbsp;&nbsp;&nbsp;How can I submit a job with a non uniform description?</a></li>
<li><a class="reference internal" href="#can-i-perform-a-fix-scheduled-reservation-and-then-launch-several-jobs-in-it" id="id42">6.4&nbsp;&nbsp;&nbsp;Can I perform a fix scheduled reservation and then launch several jobs in it?</a></li>
<li><a class="reference internal" href="#how-can-a-checkpointable-job-be-resubmitted-automatically" id="id43">6.5&nbsp;&nbsp;&nbsp;How can a checkpointable job be resubmitted automatically?</a></li>
<li><a class="reference internal" href="#how-to-submit-a-non-disturbing-job-for-other-users" id="id44">6.6&nbsp;&nbsp;&nbsp;How to submit a non disturbing job for other users?</a></li>
</ul>
</li>
<li><a class="reference internal" href="#oar-changelog" id="id45">7&nbsp;&nbsp;&nbsp;OAR CHANGELOG</a><ul class="auto-toc">
<li><a class="reference internal" href="#version-2-5-3" id="id46">7.1&nbsp;&nbsp;&nbsp;version 2.5.3:</a></li>
<li><a class="reference internal" href="#version-2-5-2" id="id47">7.2&nbsp;&nbsp;&nbsp;version 2.5.2:</a></li>
<li><a class="reference internal" href="#version-2-5-1" id="id48">7.3&nbsp;&nbsp;&nbsp;version 2.5.1:</a></li>
<li><a class="reference internal" href="#version-2-4-4" id="id49">7.4&nbsp;&nbsp;&nbsp;version 2.4.4:</a></li>
<li><a class="reference internal" href="#version-2-4-3" id="id50">7.5&nbsp;&nbsp;&nbsp;version 2.4.3:</a></li>
<li><a class="reference internal" href="#version-2-4-2" id="id51">7.6&nbsp;&nbsp;&nbsp;version 2.4.2:</a></li>
<li><a class="reference internal" href="#version-2-4-1" id="id52">7.7&nbsp;&nbsp;&nbsp;version 2.4.1:</a></li>
<li><a class="reference internal" href="#version-2-4-0" id="id53">7.8&nbsp;&nbsp;&nbsp;version 2.4.0:</a></li>
<li><a class="reference internal" href="#version-2-3-5" id="id54">7.9&nbsp;&nbsp;&nbsp;version 2.3.5:</a></li>
<li><a class="reference internal" href="#version-2-3-4" id="id55">7.10&nbsp;&nbsp;&nbsp;version 2.3.4:</a></li>
<li><a class="reference internal" href="#version-2-3-3" id="id56">7.11&nbsp;&nbsp;&nbsp;version 2.3.3:</a></li>
<li><a class="reference internal" href="#version-2-3-2" id="id57">7.12&nbsp;&nbsp;&nbsp;version 2.3.2:</a></li>
<li><a class="reference internal" href="#version-2-3-1" id="id58">7.13&nbsp;&nbsp;&nbsp;version 2.3.1:</a></li>
<li><a class="reference internal" href="#version-2-2-12" id="id59">7.14&nbsp;&nbsp;&nbsp;version 2.2.12:</a></li>
<li><a class="reference internal" href="#version-2-2-11" id="id60">7.15&nbsp;&nbsp;&nbsp;version 2.2.11:</a></li>
<li><a class="reference internal" href="#version-2-2-10" id="id61">7.16&nbsp;&nbsp;&nbsp;version 2.2.10:</a></li>
<li><a class="reference internal" href="#version-2-2-9" id="id62">7.17&nbsp;&nbsp;&nbsp;version 2.2.9:</a></li>
<li><a class="reference internal" href="#version-2-2-8" id="id63">7.18&nbsp;&nbsp;&nbsp;version 2.2.8:</a></li>
<li><a class="reference internal" href="#version-2-2-7" id="id64">7.19&nbsp;&nbsp;&nbsp;version 2.2.7:</a></li>
<li><a class="reference internal" href="#id2" id="id65">7.20&nbsp;&nbsp;&nbsp;version 2.2.11:</a></li>
<li><a class="reference internal" href="#id3" id="id66">7.21&nbsp;&nbsp;&nbsp;version 2.2.10:</a></li>
<li><a class="reference internal" href="#id4" id="id67">7.22&nbsp;&nbsp;&nbsp;version 2.2.9:</a></li>
<li><a class="reference internal" href="#id5" id="id68">7.23&nbsp;&nbsp;&nbsp;version 2.2.8:</a></li>
<li><a class="reference internal" href="#id6" id="id69">7.24&nbsp;&nbsp;&nbsp;version 2.2.7:</a></li>
<li><a class="reference internal" href="#version-2-2-6" id="id70">7.25&nbsp;&nbsp;&nbsp;version 2.2.6:</a></li>
<li><a class="reference internal" href="#version-2-2-5" id="id71">7.26&nbsp;&nbsp;&nbsp;version 2.2.5:</a></li>
<li><a class="reference internal" href="#version-2-2-4" id="id72">7.27&nbsp;&nbsp;&nbsp;version 2.2.4:</a></li>
<li><a class="reference internal" href="#version-2-2-3" id="id73">7.28&nbsp;&nbsp;&nbsp;version 2.2.3:</a></li>
<li><a class="reference internal" href="#version-2-2-2" id="id74">7.29&nbsp;&nbsp;&nbsp;version 2.2.2:</a></li>
<li><a class="reference internal" href="#version-2-2-1" id="id75">7.30&nbsp;&nbsp;&nbsp;version 2.2.1:</a></li>
<li><a class="reference internal" href="#version-2-2" id="id76">7.31&nbsp;&nbsp;&nbsp;version 2.2:</a></li>
<li><a class="reference internal" href="#version-2-1-0" id="id77">7.32&nbsp;&nbsp;&nbsp;version 2.1.0:</a></li>
<li><a class="reference internal" href="#version-2-0-2" id="id78">7.33&nbsp;&nbsp;&nbsp;version 2.0.2:</a></li>
<li><a class="reference internal" href="#version-2-0-0" id="id79">7.34&nbsp;&nbsp;&nbsp;version 2.0.0:</a></li>
</ul>
</li>
</ul>
</div>
<hr class="docutils" />
<div class="section" id="oar-capabilities">
<h1><a class="toc-backref" href="#id7">1&nbsp;&nbsp;&nbsp;OAR capabilities</a></h1>
<p>Oar is an opensource batch scheduler which provides a simple and flexible
exploitation of a cluster.</p>
<p>It manages resources of clusters as a traditional batch scheduler
(as PBS / Torque / LSF / SGE). In other words, it doesn't execute your job on
the resources but manages them (reservation, acces granting) in order to allow
you to connect these resources and use them.</p>
<p>Its design is based on high level tools:</p>
<blockquote>
<ul class="simple">
<li>relational database engine MySQL or PostgreSQL,</li>
<li>scripting language Perl,</li>
<li>confinement system mechanism cpuset,</li>
<li>scalable exploiting tool Taktuk.</li>
</ul>
</blockquote>
<p>It is flexible enough to be suitable for production clusters and research
experiments.
It currently manages over than 5000 nodes and has executed more than 5 million
jobs.</p>
<p>OAR advantages:</p>
<blockquote>
<ul class="simple">
<li>No specific daemon on nodes.</li>
<li>No dependence on specific computing libraries like MPI. We support all
sort of parallel user applications.</li>
<li>Upgrades are made on the servers, nothing to do on computing nodes.</li>
<li>CPUSET (2.6 linux kernel) integration which restricts the jobs on
assigned resources (also useful to clean completely a job, even
parallel jobs).</li>
<li>All administration tasks are performed with the taktuk command (a large
scale remote execution deployment): <a class="reference external" href="http://taktuk.gforge.inria.fr/">http://taktuk.gforge.inria.fr/</a>.</li>
<li>Hierarchical resource requests (handle heterogeneous clusters).</li>
<li>Gantt scheduling (so you can visualize the internal scheduler decisions).</li>
<li>Full or partial time-sharing.</li>
<li>Checkpoint/resubmit.</li>
<li>Licences servers management support.</li>
<li>Best effort jobs : if another job wants the same resources then it is
deleted automatically (useful to execute programs like <em>SETI&#64;home</em>).</li>
<li>Environment deployment support (Kadeploy):
<a class="reference external" href="http://kadeploy.imag.fr/">http://kadeploy.imag.fr/</a>.</li>
</ul>
</blockquote>
<p>Other more <em>common</em> features:</p>
<blockquote>
<ul class="simple">
<li>Batch and Interactive jobs.</li>
<li>Admission rules.</li>
<li>Walltime.</li>
<li>Multi-schedulers support.</li>
<li>Multi-queues with priority.</li>
<li>Backfilling.</li>
<li>First-Fit Scheduler.</li>
<li>Reservation.</li>
<li>Support of moldable tasks.</li>
<li>Check compute nodes.</li>
<li>Epilogue/Prologue scripts.</li>
<li>Support of dynamic nodes.</li>
<li>Logging/Accounting.</li>
<li>Suspend/resume jobs.</li>
</ul>
</blockquote>
</div>
<div class="section" id="description-of-the-different-commands">
<h1><a class="toc-backref" href="#id8">2&nbsp;&nbsp;&nbsp;Description of the different commands</a></h1>
<p>All user commands are installed on cluster login nodes. So you must connect to
one of these computers first.</p>
<div class="section" id="oarstat">
<h2><a class="toc-backref" href="#id9">2.1&nbsp;&nbsp;&nbsp;<em>oarstat</em></a></h2>
<p>This command prints jobs in execution mode on the terminal.</p>
<p>Options</p>
<pre class="literal-block">
-j, --job                 show informations only for the specified job (even if it is finished)
-f, --full                show full informations
-s, --state               show only the state of a job (optimized query)
-u, --user                show informations for this user only
-g, --gantt               show job informations between two date-times
-e, --events              show job events
-p, --properties          show job properties
    --accounting          show accounting informations between two dates
    --sql                 restricts display by applying the SQL where clause
                          on the table jobs (ex: &quot;project = 'p1'&quot;)
-D, --dumper              print result in DUMPER format
-X, --xml                 print result in XML format
-Y, --yaml                print result in YAML format
    --backward-compatible OAR 1.* version like display
-V, --version             print OAR version number
-h, --help                show this help screen
</pre>
<p>Examples</p>
<pre class="literal-block">
# oarstat
# oarstat -j 42 -f
# oarstat --sql &quot;project = 'p1'&quot;
# oarstat -s -j 42
</pre>
</div>
<div class="section" id="oarnodes">
<h2><a class="toc-backref" href="#id10">2.2&nbsp;&nbsp;&nbsp;<em>oarnodes</em></a></h2>
<p>This command prints informations about cluster resources (state, which jobs on
which resources, resource properties, ...).</p>
<p>Options</p>
<pre class="literal-block">
-a                : shows all resources with their properties
-r                : show only properties of a resource
-s                : shows only resource states
-l                : shows only resource list
--sql &quot;sql where&quot; : Display resources which matches this sql where clause
-D                : formats outputs in Perl Dumper
-X                : formats outputs in XML
-Y                : formats outputs in YAML
</pre>
<p>Examples</p>
<pre class="literal-block">
# oarnodes
# oarnodes -s
# oarnodes --sql &quot;state = 'Suspected'&quot;
</pre>
</div>
<div class="section" id="oarsub">
<h2><a class="toc-backref" href="#id11">2.3&nbsp;&nbsp;&nbsp;<em>oarsub</em></a></h2>
<p>The user can submit a job with this command. So, what is a job in our context?</p>
<blockquote>
<p>A job is defined by needed resources and a script/program to run. So, the user
must specify how many resources and what kind of them are needed by his
application. Thus, OAR system will give him or not what he wants and will
control the execution. When a job is launched, OAR executes user program only
on the first reservation node. So this program can access some environment
variables to know its environment:</p>
<pre class="literal-block">
$OAR_NODEFILE                 contains the name of a file which lists
                              all reserved nodes for this job
$OAR_JOB_ID                   contains the OAR job identificator
$OAR_RESOURCE_PROPERTIES_FILE contains the name of a file which lists
                              all resources and their properties
$OAR_JOB_NAME                 name of the job given by the &quot;-n&quot; option
$OAR_PROJECT_NAME             job project name
</pre>
</blockquote>
<p>Options:</p>
<pre class="literal-block">
-I, --interactive             Request an interactive job. Open a login shell
                              on the first node of the reservation instead of
                              running a script.
-C, --connect=&lt;job id&gt;        Connect to a running job
-l, --resource=&lt;list&gt;         Set the requested resources for the job.
                              The different parameters are resource properties
                              registered in OAR database, and `walltime' which
                              specifies the duration before the job must be
                              automatically terminated if still running.
                              Walltime format is [hour:mn:sec|hour:mn|hour].
                              Ex: nodes=4/cpu=1,walltime=2:00:00
-S, --scanscript              Batch mode only: asks oarsub to scan the given
                              script for OAR directives (#OAR -l ...)
-q, --queue=&lt;queue&gt;           Set the queue to submit the job to
-p, --property=&quot;&lt;list&gt;&quot;       Add constraints to properties for the job.
                              (format is a WHERE clause from the SQL syntax)
-r, --reservation=&lt;date&gt;      Request a job start time reservation,
                              instead of a submission. The date format is
                              &quot;YYYY-MM-DD HH:MM:SS&quot;.
    --checkpoint=&lt;delay&gt;      Enable the checkpointing for the job. A signal
                              is sent DELAY seconds before the walltime on
                              the first processus of the job
    --signal=&lt;#sig&gt;           Specify the signal to use when checkpointing
                              Use signal numbers, default is 12 (SIGUSR2)
-t, --type=&lt;type&gt;             Specify a specific type (deploy, besteffort,
                              cosystem, checkpoint, timesharing)
-d, --directory=&lt;dir&gt;         Specify the directory where OAR will launch the
                              command (default is current directory)
    --project=&lt;txt&gt;           Specify a name of a project the job belongs to
-n, --name=&lt;txt&gt;              Specify an arbitrary name for the job
-a, --anterior=&lt;job id&gt;       Anterior job that must be terminated to start
                              this new one
    --notify=&lt;txt&gt;            Specify a notification method
                              (mail or command to execute). Ex:
                                  --notify &quot;mail:name\&#64;domain.com&quot;
                                  --notify &quot;exec:/path/to/script args&quot;
    --resubmit=&lt;job id&gt;       Resubmit the given job as a new one
-k, --use-job-key             Activate the job-key mechanism.
-i, --import-job-key-from-file=&lt;file&gt;
                              Import the job-key to use from a files instead
                              of generating a new one.
    --import-job-key-inline=&lt;txt&gt;
                              Import the job-key to use inline instead of
                              generating a new one.
-e  --export-job-key-to-file=&lt;file&gt;
                              Export the job key to a file. Warning: the
                              file will be overwritten if it already exists.
                              (the %jobid% pattern is automatically replaced)
-O  --stdout=&lt;file&gt;           Specify the file that will store the standart
                              output stream of the job.
                              (the %jobid% pattern is automatically replaced)
-E  --stderr=&lt;file&gt;           Specify the file that will store the standart
                              error stream of the job.
                              (the %jobid% pattern is automatically replaced)
    --hold                    Set the job state into Hold instead of Waiting,
                              so that it is not scheduled (you must run
                              &quot;oarresume&quot; to turn it into the Waiting state)
-D, --dumper                  Print result in DUMPER format
-X, --xml                     Print result in XML format
-Y, --yaml                    Print result in YAML format
-h, --help                    Print this help message
-V, --version                 Print OAR version number
</pre>
<p>Wanted resources have to be described in a hierarchical manner using the
&quot;-l&quot; syntax option.</p>
<p>Moreover it is possible to give a specification that must be matched on properties.</p>
<p>So the long and complete syntax is of the form:</p>
<pre class="literal-block">
&quot;{ sql1 }/prop1=1/prop2=3+{sql2}/prop3=2/prop4=1/prop5=1+...,walltime=1:00:00&quot;
</pre>
<dl class="docutils">
<dt>where:</dt>
<dd><ul class="first last simple">
<li><em>sql1</em> : SQL WHERE clause on the table of resources that filters resource
names used in the hierarchical description</li>
<li><em>prop1</em> : first type of resources</li>
<li><em>prop2</em> : second type of resources</li>
<li><em>+</em> : add another resource hierarchy to the previous one</li>
<li><em>sql2</em> : SQL WHERE clause to apply on the second hierarchy request</li>
<li>...</li>
</ul>
</dd>
</dl>
<p>So we want to reserve 3 resources with the same value of the type <em>prop2</em> and
with the same property <em>prop1</em> and these resources must fit <em>sql1</em>. To that
possible resources we want to add 2 others which fit <em>sql2</em> and the hierarchy
<em>/prop3=2/prop4=1/prop5=1</em>.</p>
<div class="figure">
<a class="reference external image-reference" href="../schemas/hierarchical_resources.png"><img alt="Hierarchical resource example" src="../schemas/hierarchical_resources.png" /></a>
<p class="caption">Example of a resource hierarchy and 2 different oarsub commands</p>
</div>
<p><a class="reference external" href="../schemas/hierarchical_resources.svg">hierarchical_resources.svg</a></p>
<p>Examples</p>
<pre class="literal-block">
# oarsub -l /nodes=4 test.sh
</pre>
<p>(the &quot;test.sh&quot; script will be run on 4 entire nodes in the default queue with
the default walltime)</p>
<pre class="literal-block">
# oarsub --stdout='test12.%jobid%.stdout' --stderr='test12.%jobid%.stderr' -l
  /nodes=4 test.sh
  ...
  OAR_JOB_ID=702
  ...
</pre>
<p>(same example than above but here the standard output of &quot;test.sh&quot; will be
written in the file &quot;test12.702.stdout&quot; and the standard error in
&quot;test12.702.stderr&quot;)</p>
<pre class="literal-block">
# oarsub -q default -l /nodes=10/cpu=3,walltime=2:15:00 \
  -p &quot;switch = 'sw1'&quot; /home/users/toto/prog
</pre>
<p>(the &quot;/home/users/toto/prog&quot; script will be run on 10 nodes with 3 cpus (so a
total of 30 cpus) in the default queue with a walltime of  2:15:00.
Moreover &quot;-p&quot; option restricts resources only on the switch 'sw1')</p>
<pre class="literal-block">
# oarsub -r &quot;2009-04-27 11:00:00&quot; -l /nodes=12/cpu=2
</pre>
<p>(a reservation will begin at &quot;2009-04-27 11:00:00&quot; on 12 nodes with 2 cpus
on each one)</p>
<pre class="literal-block">
#  oarsub -C 42
</pre>
<p>(connects to the job 42 on the first node and set all OAR environment
variables)</p>
<pre class="literal-block">
#  oarsub -p &quot;not host like 'nodename.%'&quot;
</pre>
<p>(To exclude a node from the request)</p>
<pre class="literal-block">
# oarsub -I
</pre>
<p>(gives a shell on a resource)</p>
</div>
<div class="section" id="oardel">
<h2><a class="toc-backref" href="#id12">2.4&nbsp;&nbsp;&nbsp;<em>oardel</em></a></h2>
<p>This command is used to delete or checkpoint job(s). They are designed by
their identifier.</p>
<p>Option</p>
<pre class="literal-block">
--sql     : delete/checkpoint jobs which respond to the SQL where clause
            on the table jobs (ex: &quot;project = 'p1'&quot;)
-c job_id : send checkpoint signal to the job (signal was
            definedwith &quot;--signal&quot; option in oarsub)
</pre>
<p>Examples</p>
<pre class="literal-block">
# oardel 14 42
</pre>
<p>(delete jobs 14 and 42)</p>
<pre class="literal-block">
# oardel -c 42
</pre>
<p>(send checkpoint signal to the job 42)</p>
</div>
<div class="section" id="oarhold">
<h2><a class="toc-backref" href="#id13">2.5&nbsp;&nbsp;&nbsp;<em>oarhold</em></a></h2>
<p>This command is used to remove a job from the scheduling queue if it is in
the &quot;Waiting&quot; state.</p>
<p>Moreover if its state is &quot;Running&quot; <a class="reference internal" href="#oarhold">oarhold</a> can suspend the execution and
enable other jobs to use its resources. In that way, a <strong>SIGINT</strong> signal
is sent to every processes.</p>
<p>Options</p>
<pre class="literal-block">
--sql : hold jobs which respond to the SQL where clause on the table
        jobs (ex: &quot;project = 'p1'&quot;)
-r    : Manage not only Waiting jobs but also Running one
        (can suspend the job)
</pre>
</div>
<div class="section" id="oarresume">
<h2><a class="toc-backref" href="#id14">2.6&nbsp;&nbsp;&nbsp;<em>oarresume</em></a></h2>
<p>This command resumes jobs in the states <em>Hold</em> or <em>Suspended</em></p>
<p>Option</p>
<pre class="literal-block">
--sql : resume jobs which respond to the SQL where clause on the table
        jobs (ex: &quot;project = 'p1'&quot;)
</pre>
</div>
</div>
<div class="section" id="desktop-computing">
<h1><a class="toc-backref" href="#id15">3&nbsp;&nbsp;&nbsp;Desktop computing</a></h1>
<p>If you want to compute jobs on nodes without SSH connections then this
feature is for you.</p>
<p>On the nodes you have to run &quot;oar-agent.pl&quot;. This script polls the OAR
server via a CGI HTTP script.</p>
<dl class="docutils">
<dt>Usage examples:</dt>
<dd><ul class="first last">
<li><p class="first">if you want to run a program that you know is installed on nodes:</p>
<pre class="literal-block">
oarsub -t desktop_computing /path/to/program
</pre>
<p>Then /path/to/program is run and the files created in the
oar-agent.pl running directory is retrieved where oarsub was
launched.</p>
</li>
<li><p class="first">if you want to copy a working environment and then launch the program:</p>
<pre class="literal-block">
oarsub -t desktop_computing -s . ./script.sh
</pre>
<p>The content of &quot;.&quot; is transfred to the node, &quot;./script.sh&quot; is run and
everything will go back.</p>
</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="visualisation-tools">
<h1><a class="toc-backref" href="#id16">4&nbsp;&nbsp;&nbsp;Visualisation tools</a></h1>
<div class="section" id="monika">
<h2><a class="toc-backref" href="#id17">4.1&nbsp;&nbsp;&nbsp;Monika</a></h2>
<p>This is a web cgi normally installed on the cluster frontal. This tool connects
to the DB, gets relevant information then format data in a html page.</p>
<p>Thus you can have a global view of cluster state and where your jobs are
running.</p>
</div>
<div class="section" id="drawoargantt">
<h2><a class="toc-backref" href="#id18">4.2&nbsp;&nbsp;&nbsp;DrawOARGantt</a></h2>
<p>This is also a web cgi. It creates a Gantt chart which shows job repartition on
nodes in the time. It is very useful to see cluster occupation in the past
and to know when a job will be launched in the future.</p>
</div>
</div>
<div class="section" id="mechanisms">
<h1><a class="toc-backref" href="#id19">5&nbsp;&nbsp;&nbsp;Mechanisms</a></h1>
<div class="section" id="how-does-an-interactive-oarsub-work">
<span id="interactive"></span><h2><a class="toc-backref" href="#id20">5.1&nbsp;&nbsp;&nbsp;How does an interactive <em>oarsub</em> work?</a></h2>
<div class="figure">
<a class="reference external image-reference" href="../schemas/interactive_oarsub_scheme.png"><img alt="interactive oarsub decomposition" src="../schemas/interactive_oarsub_scheme.png" /></a>
<p class="caption">Interactive oarsub decomposition</p>
</div>
<p><a class="reference external" href="../schemas/interactive_oarsub_scheme.svg">interactive_oarsub_scheme.svg</a></p>
</div>
<div class="section" id="job-launch">
<h2><a class="toc-backref" href="#id21">5.2&nbsp;&nbsp;&nbsp;Job launch</a></h2>
<p>For PASSIVE jobs, the mechanism is similar to the <a class="reference internal" href="#interactive">INTERACTIVE</a> one, except for
the shell launched from the frontal node.</p>
<p>The job is finished when the user command ends. Then oarexec return its exit
value (what errors occured) on the Almighty via the SERVER_PORT if
DETACH_JOB_FROM_SERVER was set to 1 otherwise it returns directly.</p>
</div>
<div class="section" id="cpuset">
<h2><a class="toc-backref" href="#id22">5.3&nbsp;&nbsp;&nbsp;CPUSET</a></h2>
<p>The cpuset name is effectively created on each nodes and is composed as
&quot;user_jobid&quot;.</p>
<p>OAR system steps:</p>
<blockquote>
<ol class="arabic simple">
<li>Before each job, the Runner initialize the CPUSET (see <cite>CPUSET
definition</cite>) with OPENSSH_CMD and an efficient launching tool :
<a class="reference external" href="http://taktuk.gforge.inria.fr/">Taktuk</a>. If it is not
installed and configured (TAKTUK_CMD) then OAR uses an internal
launching tool less optimized.
The processors assigned to this cpuset are taken from the defined database
field by JOB_RESOURCE_MANAGER_PROPERTY_DB_FIELD in the table resources.</li>
<li>After each job, OAR deletes all processes stored in the associated CPUSET.
Thus all nodes are clean after a OAR job.</li>
</ol>
</blockquote>
<p>If you don't want to use this feature, you can, but nothing will warranty that
every user processes will be killed after the end of a job.</p>
<p>If you want you can implement your own cpuset management. This is done by
editing 3 files (see also <cite>CPUSET installation</cite>):</p>
<blockquote>
<ul class="simple">
<li>cpuset_manager.pl : this script creates the cpuset on each nodes
and also delete it at the end of the job. For more informations, you have to
look at this script (there are several comments).</li>
<li>oarsh : (OARSH) this script is used to replace the standard &quot;ssh&quot;
command. It gets the cpuset name where it is running and transfer this
information via &quot;ssh&quot; and the &quot;SendEnv&quot; option. In this file, you have
to change the &quot;get_current_cpuset&quot; function.</li>
<li>oarsh_shell : (OARSH_SHELL) this script is the shell of the oar user on
each nodes. It gets environment variables and look at if there is a cpuset
name. So if there is one it assigns the current process and its father to
this cpusetname. So all further user processes will remind in the cpuset.
In this file you just have to change the &quot;add_process_to_cpuset&quot; function.</li>
</ul>
</blockquote>
</div>
<div class="section" id="ssh-connection-key-definition">
<h2><a class="toc-backref" href="#id23">5.4&nbsp;&nbsp;&nbsp;SSH connection key definition</a></h2>
<p>This function is performed by <a class="reference internal" href="#oarsub">oarsub</a> with the --ssh_private_key and
--ssh_public_key options.</p>
<p>It enables the user to define a ssh key pair to connect on their nodes.
So oarsh can be used on nodes of different clusters to connect
each others if the same ssh keys are used with each <a class="reference internal" href="#oarsub">oarsub</a>.</p>
<p>So a grid reservation (&quot;-r&quot; option of <a class="reference internal" href="#oarsub">oarsub</a> on each OAR batch scheduler of
each wanted clusters) can be done with this functionality.</p>
<p>Example:</p>
<pre class="literal-block">
ssh-keygen -f oar_key
oarsub --ssh_private_key &quot;$(cat oar_key)&quot; --ssh_public_key &quot;$(cat oar_key.pub)&quot; ./script.sh
</pre>
</div>
<div class="section" id="suspend-resume">
<h2><a class="toc-backref" href="#id24">5.5&nbsp;&nbsp;&nbsp;Suspend/resume</a></h2>
<p>Jobs can be suspended with the command <a class="reference internal" href="#oarhold">oarhold</a> (send a &quot;SIGSTOP&quot; on every
processes on every nodes) to allow other jobs to be executed.</p>
<p>&quot;Suspended&quot; jobs can be resumed with the command <a class="reference internal" href="#oarresume">oarresume</a> (send a &quot;SIGSTOP&quot;
on every suspended processes on every nodes). They will
pass into &quot;Running&quot; when assigned resources will be free.</p>
<p>IMPORTANT: This feature is available only if <a class="reference internal" href="#cpuset">CPUSET</a> is configured.</p>
<p>You can specify 2 scripts if you have to perform any actions just after
(JUST_AFTER_SUSPEND_EXEC_FILE) suspend and just before resume
(JUST_BEFORE_RESUME_EXEC_FILE).</p>
<p>Moreover you can perform other actions (than send signals to processes)
if you want: just edit the &quot;suspend_resume_manager.pl&quot; file.</p>
</div>
<div class="section" id="job-deletion">
<h2><a class="toc-backref" href="#id25">5.6&nbsp;&nbsp;&nbsp;Job deletion</a></h2>
<p>Leon tries to connect to OAR Perl script running on the first job node (find
it thanks to the file <em>/tmp/oar/pid_of_oarexec_for_jobId_id</em>) and sends a
&quot;SIGTERM&quot; signal. Then the script catch it and normally end the job (kill
processes that it has launched).</p>
<p>If this method didn't succeed then Leon will flush the OAR database for the
job and nodes will be &quot;Suspected&quot; by NodeChangeState.</p>
<p>If your job is check pointed and is of the type <em>idempotent</em> (<a class="reference internal" href="#oarsub">oarsub</a> &quot;-t&quot;
option) and its exit code is equal to 99 then another job is automatically
created and scheduled with same behaviours.</p>
</div>
<div class="section" id="checkpoint">
<h2><a class="toc-backref" href="#id26">5.7&nbsp;&nbsp;&nbsp;Checkpoint</a></h2>
<p>The checkpoint is just a signal sent to the program specified with the <a class="reference internal" href="#oarsub">oarsub</a>
command.</p>
<p>If the user uses &quot;--checkpoint&quot; option then Sarko will ask the OAR Perl script running
on the first node to send the signal to the process (SIGUSR2 or the one
specified with &quot;--signal&quot;).</p>
<p>You can also use <a class="reference internal" href="#oardel">oardel</a> command to send the signal.</p>
</div>
<div class="section" id="scheduling">
<h2><a class="toc-backref" href="#id27">5.8&nbsp;&nbsp;&nbsp;Scheduling</a></h2>
<p>General steps used to schedule a job:</p>
<blockquote>
<ol class="arabic simple">
<li>All previous scheduled jobs are stored in a Gantt data structure.</li>
<li>All resources that match property constraints of the job(&quot;-p&quot; option and
indication in the &quot;{...}&quot; from the &quot;-l&quot; option of the <a class="reference internal" href="#oarsub">oarsub</a>) are stored in
a tree data structure according to the hierarchy given with the &quot;-l&quot; option.</li>
<li>Then this tree is given to the Gantt library to find the first hole where
the job can be launched.</li>
<li>The scheduler stores its decision into the database in the
gantt_jobs_predictions and gantt_jobs_resources tables.</li>
</ol>
</blockquote>
<p>See User section from the FAQ for more examples and features.</p>
</div>
<div class="section" id="job-dependencies">
<h2><a class="toc-backref" href="#id28">5.9&nbsp;&nbsp;&nbsp;Job dependencies</a></h2>
<p>A job dependency is a situation where a job needs the ending of another job
to start. OAR deals with job dependency problems by refusing to schedule
dependant jobs if their required job is in Terminated state and have an exit
code != 0 (an error occured). If the required job is resubmited, its jobId is
no longer the same and OAR updates the database and sets the job_id_required
field to this new jobId for the dependant job.</p>
</div>
<div class="section" id="user-notification">
<h2><a class="toc-backref" href="#id29">5.10&nbsp;&nbsp;&nbsp;User notification</a></h2>
<p>This section explains how the &quot;--notify&quot; <a class="reference internal" href="#oarsub">oarsub</a> option is handled by OAR:</p>
<blockquote>
<ul>
<li><dl class="first docutils">
<dt>The user wants to receive an email:</dt>
<dd><p class="first">The syntax is &quot;mail:name&#64;domain.com&quot;. Mail section in the <cite>Configuration
file</cite> must be present otherwise the mail cannot be sent.
The subject of the mail is of the form:</p>
<p class="last">*OAR* [<em>TAG</em>]: job_id (job_name) on OAR_server_hostname</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>The user wants to launch a script:</dt>
<dd><p class="first last">The syntax is &quot;exec:/path/to/script args&quot;. OAR server will connect (using
OPENSSH_CMD) on the node where the <a class="reference internal" href="#oarsub">oarsub</a> command was invoked and then
launches the script with the following arguments : <em>job_id</em>, <em>job_name</em>, <em>TAG</em>,
<em>comments</em>.</p>
</dd>
</dl>
</li>
</ul>
</blockquote>
<dl class="docutils">
<dt><em>TAG</em> can be:</dt>
<dd><ul class="first last simple">
<li>RUNNING : when the job is launched</li>
<li>END : when the job is finished normally</li>
<li>ERROR : when the job is finished abnormally</li>
<li>INFO : used when oardel is called on the job</li>
<li>SUSPENDED : when the job is suspended</li>
<li>RESUMING : when the job is resumed</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="accounting-aggregator">
<h2><a class="toc-backref" href="#id30">5.11&nbsp;&nbsp;&nbsp;Accounting aggregator</a></h2>
<p>In the <cite>Configuration file</cite> you can set the ACCOUNTING_WINDOW parameter. Thus
the command oaraccounting will split the time with this amount and feed the
table accounting.</p>
<p>So this is very easily and faster to get usage statistics of the cluster. We
can see that like a &quot;data warehousing&quot; information extraction method.</p>
</div>
<div class="section" id="dynamic-nodes-coupling-features">
<h2><a class="toc-backref" href="#id31">5.12&nbsp;&nbsp;&nbsp;Dynamic nodes coupling features</a></h2>
<p>We are working with the <a class="reference external" href="http://www.icatis.com/">Icatis</a> company on clusters
composed by Intranet computers. These nodes can be switch in computing mode
only at specific times. So we have implemented a functionality that can
request to power on some hardware if they can be in the cluster.</p>
<p>We are using the field <em>available_upto</em> from the table resources
to know when a node will be inaccessible in the cluster mode (easily settable
with oarnodesetting command). So when the OAR scheduler wants some potential
available computers to launch the jobs then it executes the command
SCHEDULER_NODE_MANAGER_WAKE_UP_CMD.</p>
<p>Moreover if a node didn't execute a job for SCHEDULER_NODE_MANAGER_IDLE_TIME
seconds and no job is scheduled on it before SCHEDULER_NODE_MANAGER_SLEEP_TIME
seconds then OAR will launch the command SCHEDULER_NODE_MANAGER_SLEEP_CMD.</p>
</div>
<div class="section" id="timesharing">
<h2><a class="toc-backref" href="#id32">5.13&nbsp;&nbsp;&nbsp;Timesharing</a></h2>
<p>It is possible to share the slot time of a job with other ones.
To perform this feature you have to specify the type <em>timesharing</em> when you use
<a class="reference internal" href="#oarsub">oarsub</a>.</p>
<p>You have 4 different ways to share your slot:</p>
<blockquote>
<ol class="arabic simple">
<li><em>timesharing=*,*</em> : This is the default behavior if nothing but
timesharing is specified.
It indicates that the job can be shared with all users and every job
names.</li>
<li><em>timesharing=user,*</em> : This indicates that the job can be shared only
with the same user and every job names.</li>
<li><em>timesharing=*,job_name</em> : This indicates that the job can be shared
with all users but only one with the same name.</li>
<li><em>timesharing=user,job_name</em> : This indicates that the job can be shared
only with the same user and one with the same job name.</li>
</ol>
</blockquote>
<p>See User section from the FAQ for more examples and features.</p>
</div>
<div class="section" id="container-jobs">
<h2><a class="toc-backref" href="#id33">5.14&nbsp;&nbsp;&nbsp;Container jobs</a></h2>
<p>With this functionality it is possible to execute jobs within another one. So
it is like a sub-scheduling mechanism.</p>
<p>First a job of the type <em>container</em> must be submitted, for example:</p>
<pre class="literal-block">
oarsub -I -t container -l nodes=10,walltime=2:10:00
...
OAR_JOB_ID=42
...
</pre>
<p>Then it is possible to use the <em>inner</em> type to schedule the new jobs within the
previously created container job:</p>
<pre class="literal-block">
oarsub -I -t inner=42 -l nodes=7
oarsub -I -t inner=42 -l nodes=1
oarsub -I -t inner=42 -l nodes=10
</pre>
<p>Notes:</p>
<blockquote>
<ul>
<li><p class="first">In the case:</p>
<pre class="literal-block">
oarsub -I -t inner=42 -l nodes=11
</pre>
<p>This job will never be scheduled because the container job &quot;42&quot; reserved only 10
nodes.</p>
</li>
<li><p class="first">&quot;-t container&quot; is handled by every kind of jobs (passive, interactive and
reservations). But &quot;-t inner=...&quot; cannot be used with a reservation.</p>
</li>
</ul>
</blockquote>
</div>
<div class="section" id="besteffort-jobs">
<h2><a class="toc-backref" href="#id34">5.15&nbsp;&nbsp;&nbsp;Besteffort jobs</a></h2>
<p>Besteffort jobs are scheduled in the besteffort queue. Their particularity is
that they are deleted if another not besteffort job wants resources where they
are running.</p>
<p>For example you can use this feature to maximize the use of your cluster with
multiparametric jobs. This what it is done by the
<a class="reference external" href="http://cigri.ujf-grenoble.fr">CIGRI</a> project.</p>
<p>When you submit a job you have to use &quot;-t besteffort&quot; option of <a class="reference internal" href="#oarsub">oarsub</a> to
specify that this is a besteffort job.</p>
<p>Important : a besteffort job cannot be a reservation.</p>
<p>If your job is of the type <em>besteffort</em> and <em>idempotent</em> (<a class="reference internal" href="#oarsub">oarsub</a> &quot;-t&quot;
option) and killed by the OAR scheduler then another job is automatically
created and scheduled with same behaviours.</p>
</div>
<div class="section" id="cosystem-jobs">
<h2><a class="toc-backref" href="#id35">5.16&nbsp;&nbsp;&nbsp;Cosystem jobs</a></h2>
<p>This feature enables to reserve some resources without launching any
program on corresponding nodes. Thus nothing is done by OAR on computing nodes
when a job is starting except on the COSYSTEM_HOSTNAME defined in the
configuration file.</p>
<p>This is useful with an other launching system that will declare its time
slot in OAR. So yo can have two different batch scheduler.</p>
<p>When you submit a job you have to use &quot;-t cosystem&quot; option of <a class="reference internal" href="#oarsub">oarsub</a> to
specify that this is a cosystem job.</p>
<p>These jobs are stopped by the <a class="reference internal" href="#oardel">oardel</a> command or when they reach their
walltime or their command has finished.
They also use the node COSYSTEM_HOSTNAME to launch the specified program
or shell.</p>
</div>
<div class="section" id="deploy-jobs">
<h2><a class="toc-backref" href="#id36">5.17&nbsp;&nbsp;&nbsp;Deploy jobs</a></h2>
<p>This feature is useful when you want to enable the users to reinstall their
reserved nodes. So the OAR jobs will not log on the first computer of the
reservation but on the DEPLOY_HOSTNAME.</p>
<p>So prologue and epilogue scripts are executed on DEPLOY_HOSTNAME and if the
user wants to launch a script it is also executed on DEPLOY_HOSTNAME.</p>
<p>OAR does nothing on computing nodes because they normally will be rebooted to
install a new system image.</p>
<p>This feature is strongly used in the <a class="reference external" href="https://www.grid5000.fr/">Grid5000</a>
project with <a class="reference external" href="http://ka-tools.imag.fr/">Kadeploy</a> tools.</p>
<p>When you submit a job you have to use &quot;-t deploy&quot; option of <a class="reference internal" href="#oarsub">oarsub</a> to
specify that this is a deploy job.</p>
</div>
<div class="section" id="id1">
<h2><a class="toc-backref" href="#id37">5.18&nbsp;&nbsp;&nbsp;Desktop computing</a></h2>
<p>If you cannot contact the computers via SSH you can install the &quot;desktop
computing&quot; OAR mode.
This kind of installation is based on two programs:</p>
<blockquote>
<ul class="simple">
<li>oar-cgi : this is a web CGI used by the nodes to communicate with
the OAR server via a HTTP server on the OAR server node.</li>
<li>oar-agent.pl : This program asks periodically the server web CGI to know what it
has to do.</li>
</ul>
</blockquote>
<p>This method replaces the SSH command. Computers which want to register them into
OAR just has to be able to contact OAR HTTP server.</p>
<p>In this situation we don't have a NFS file system to share the same directories
over all nodes so we have to use a stagein/stageout solution. In this case you
can use the <a class="reference internal" href="#oarsub">oarsub</a> option &quot;stagein&quot; to migrate your data.</p>
</div>
</div>
<div class="section" id="faq-user">
<h1><a class="toc-backref" href="#id38">6&nbsp;&nbsp;&nbsp;FAQ - USER</a></h1>
<div class="section" id="release-policy">
<h2><a class="toc-backref" href="#id39">6.1&nbsp;&nbsp;&nbsp;Release policy</a></h2>
<dl class="docutils">
<dt>Since the version 2.2, release numbers are divided into 3 parts:</dt>
<dd><ul class="first last simple">
<li>The first represents the design and the implementation used.</li>
<li>The second represents a set of OAR functionalities.</li>
<li>The third is incremented after bug fixes.</li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="how-can-i-submit-a-moldable-job">
<h2><a class="toc-backref" href="#id40">6.2&nbsp;&nbsp;&nbsp;How can I submit a moldable job?</a></h2>
<p>You just have to use several &quot;-l&quot; <a class="reference internal" href="#oarsub">oarsub</a> option(one for each moldable
description). By default the OAR scheduler will launch the moldable job which
will end first.</p>
<p>So you can see some free resources but the scheduler can decide to start your
job later because they will have more free resources and the job walltime will
be smaller.</p>
</div>
<div class="section" id="how-can-i-submit-a-job-with-a-non-uniform-description">
<h2><a class="toc-backref" href="#id41">6.3&nbsp;&nbsp;&nbsp;How can I submit a job with a non uniform description?</a></h2>
<p>Example:</p>
<pre class="literal-block">
oarsub -I -l '{switch = &quot;sw1&quot; or switch = &quot;sw5&quot;}/switch=1+/node=1'
</pre>
<p>This example asks OAR to reserve all resources from the switch sw1 or the
switch sw2 <strong>and</strong> a node on another switch.</p>
<p>You can see the &quot;+&quot; syntax as a sub-reservation directive.</p>
</div>
<div class="section" id="can-i-perform-a-fix-scheduled-reservation-and-then-launch-several-jobs-in-it">
<h2><a class="toc-backref" href="#id42">6.4&nbsp;&nbsp;&nbsp;Can I perform a fix scheduled reservation and then launch several jobs in it?</a></h2>
<p>Yes. You have to use the OAR scheduler &quot;timesharing&quot; feature.
To use it, the reservation and your further jobs must be of the type
timesharing (only for you).</p>
<p>Example:</p>
<blockquote>
<ol class="arabic">
<li><p class="first">Make your reservation:</p>
<pre class="literal-block">
oarsub -r &quot;2006-09-12 8:00:00&quot; -l /switch=1 -t 'timesharing=user,*'
</pre>
<p>This command asks all resources from one switch at the given date for the
default walltime. It also specifies that this job can be shared with
himself and without a constraint on the job name.</p>
</li>
<li><p class="first">Once your reservation has begun then you can launch:</p>
<pre class="literal-block">
oarsub -I -l /node=2,walltime=0:50:00 -p 'switch = &quot;nom_du_switch_schedule&quot;'\
-t 'timesharing=user,*'
</pre>
<p>So this job will be scheduled on nodes assigned from the previous reservation.</p>
</li>
</ol>
</blockquote>
<p>The &quot;timesharing&quot; <a class="reference internal" href="#oarsub">oarsub</a> command possibilities are enumerated in <a class="reference internal" href="#timesharing">Timesharing</a>.</p>
</div>
<div class="section" id="how-can-a-checkpointable-job-be-resubmitted-automatically">
<h2><a class="toc-backref" href="#id43">6.5&nbsp;&nbsp;&nbsp;How can a checkpointable job be resubmitted automatically?</a></h2>
<p>You have to specify that your job is <em>idempotent</em> and exit from your script
with the exit code 99. So, after a successful checkpoint, if the job is
resubmitted then all will go right and there will have no problem (like file
creation, deletion, ...).</p>
<p>Example:</p>
<pre class="literal-block">
oarsub --checkpoint 600 --signal 2 -t idempotent /path/to/prog
</pre>
<p>So this job will send a signal <em>SIGINT</em> (see <em>man kill</em> to know signal
numbers) 10 minutes before the walltime ends. Then if everything goes
well and the exit code is 99 it will be resubmitted.</p>
</div>
<div class="section" id="how-to-submit-a-non-disturbing-job-for-other-users">
<h2><a class="toc-backref" href="#id44">6.6&nbsp;&nbsp;&nbsp;How to submit a non disturbing job for other users?</a></h2>
<p>You can use the <em>besteffort</em> job type. Thus your job will be launched only
if there is a hole and will be deleted if another job wants its resources.</p>
<p>Example:</p>
<pre class="literal-block">
oarsub -t besteffort /path/to/prog
</pre>
</div>
</div>
<div class="section" id="oar-changelog">
<h1><a class="toc-backref" href="#id45">7&nbsp;&nbsp;&nbsp;OAR CHANGELOG</a></h1>
<div class="section" id="version-2-5-3">
<h2><a class="toc-backref" href="#id46">7.1&nbsp;&nbsp;&nbsp;version 2.5.3:</a></h2>
<blockquote>
<ul class="simple">
<li>Add the &quot;Name&quot; field on the main Monika page. This is easier for the users
to find there jobs.</li>
<li>Add MAX_CONCURRENT_JOB_TERMINATIONS into the oar.conf ofthe master. This
limits the number of concurrent processes launched by the Almighty when the
the jobs finish.</li>
<li>Bug fix in ssh key feature in oarsub.</li>
<li>Added --compact, -c option to oarstat (compact view or array jobs)</li>
<li>Improvements of the API: media upload from html forms, listing of files,
security fixes, add of new configuration options, listing of the scheduled
nodes into jobs, fixed bad reinitialization of the limit parameter...
See OAR-DOCUMENTATION-API-USER for more informations.</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-5-2">
<h2><a class="toc-backref" href="#id47">7.2&nbsp;&nbsp;&nbsp;version 2.5.2:</a></h2>
<blockquote>
<ul>
<li><p class="first">Bugfix: /var/lib/oar/.bash_oar was empty due to an error in the common
setup script.</p>
</li>
<li><p class="first">Bugfix: the PINGCHECKER_COMMAND in oar.conf depends now on %%OARDIR%%.</p>
</li>
<li><dl class="first docutils">
<dt>Bug #13939: the job_resource_manager.pl and job_resource_manager_cgroups.pl</dt>
<dd><p class="first last">now deletes the user files in /tmp, /var/tmp and /dev/shm at
the end of the jobs.</p>
</dd>
</dl>
</li>
<li><p class="first">Bugfix: in oardodo.c, the preprocessed variables was not defined correclty.</p>
</li>
<li><p class="first">Finaud: fix race condition when there was a PINGCHECKER error jsut before
another problem. The node became Alive again when the PINGCHECKER said OK
BUT there was another error to resolve.</p>
</li>
<li><p class="first">Bugfix: The feature CHECK_NODES_WITH_RUNNING_JOB=yes never worked before.</p>
</li>
<li><p class="first">Speedup monika (X5).</p>
</li>
<li><p class="first">Monika: Add the conf max_cores_per_line to have several lines if the number
of cores are too big.</p>
</li>
<li><dl class="first docutils">
<dt>Minor changes into API:</dt>
<dd><ul class="first last simple">
<li>added cmd_output into POST /jobs.</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">API: Added GET /select_all?query=&lt;query&gt; (read only mode).</p>
</li>
<li><p class="first">Add the field &quot;array_index&quot; into the jobs table. So that resubmit a job
from an array will have the right array_index anvironment variable.</p>
</li>
<li><p class="first">oarstat: order the output by job_id.</p>
</li>
<li><p class="first">Speedup oarnodes.</p>
</li>
<li><p class="first">Fix a spelling error in the oaradmin manpage.</p>
</li>
<li><p class="first">Bugfix #14122 : the oar-node init.d script wasn't executing
start_oar_node/stop_oar_node during the 'restart' action.</p>
</li>
<li><p class="first">Allow the dash  character into the --notify &quot;exec:...&quot; oarsub option.</p>
</li>
<li><dl class="first docutils">
<dt>Remove some old stuffs from the tarball:</dt>
<dd><ul class="first last simple">
<li>visualization_interfaces/{tgoar,accounting,poar};</li>
<li>scheduler/moldable;</li>
<li>pbs-oar-lib.</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">Fix some licence issues.</p>
</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-5-1">
<h2><a class="toc-backref" href="#id48">7.3&nbsp;&nbsp;&nbsp;version 2.5.1:</a></h2>
<blockquote>
<ul>
<li><p class="first">Sources directories reorganized</p>
</li>
<li><p class="first">New &quot;Phoenix&quot; tool to try to reboot automatically broken nodes
(to setup into /etc/oar/oar_phoenix.pl)</p>
</li>
<li><p class="first">New (experimental!) scheduler written in Ocaml</p>
</li>
<li><p class="first">Cpusets are activated by default</p>
</li>
<li><p class="first">Bugfix #11065: oar_resource_init fix (add a space)</p>
</li>
<li><p class="first">Bug 10999: memory leak into Hulot when used with postgresql. The leak has been
minimized, but it is still there (DBD::Pg bug)</p>
</li>
<li><p class="first">Almighty cleans ipcs used by oar on exit</p>
</li>
<li><p class="first">Bugfix #10641 and #10999 : Hulot is automatically and periodically restarted</p>
</li>
<li><p class="first">Feature request #10565: add the possibility to check the aliveness of the
nodes of a job at the end of this one (pingchecker)</p>
</li>
<li><p class="first">REST API heavily updated: new data structures with paginated results, desktop
computing functions, rspec tests, oaradmin resources management, admission
rules edition, relative/absolutes uris fixed</p>
</li>
<li><p class="first">New ruby desktop computing agent using REST API (experimental)</p>
</li>
<li><p class="first">Experimental testsuite</p>
</li>
<li><p class="first">Poar: web portal using the REST API (experimental)</p>
</li>
<li><p class="first">Oaradmin YAML export support for resources creation (for the REST API)</p>
</li>
<li><p class="first">Bugfix #10567: enabling to bypass window mechanism of hulot.</p>
</li>
<li><p class="first">Bugfix #10568: Wake up timeout changing with the number of nodes</p>
</li>
<li><p class="first">Add in oar.conf the tag &quot;RUNNER_SLIDING_WINDOW_SIZE&quot;: it allows the runner
to use a sliding window to launch the bipbip processes if
&quot;DETACH_JOB_FROM_SERVER=1&quot;. This feature avoids the overload of the server
if plenty of jobs have to be launched at the same time.</p>
</li>
<li><p class="first">Fix problem when deleting a job in the Suspended state (oarexec was stopped
by a SIGSTOP so it was not able to handle the delete operation)</p>
</li>
<li><p class="first">Make the USER_SIGNAL feature of oardel multi job independant and remove the
temporary file at the end of the job</p>
</li>
<li><dl class="first docutils">
<dt>Monika: display if the job is of timesharing type or not</dt>
<dd><p class="first last">add in the job listing the initial_request (is there a reason to
not display it?)</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>IoLib: update scheduler_priority resources property for timesharing jobs.</dt>
<dd><p class="first last">So the scheduler will be able to avoid to launch every timesharing
jobs on the same resources (they can be dispatched)</p>
</dd>
</dl>
</li>
<li><p class="first">OAREXEC: unmask SIGHUP and SIGPIPE for user script</p>
</li>
<li><p class="first">node_change_state: do not Suspect the first node of a job which was
EXTERMINATED by Leon if the cpuset feature is configured (let do the job by
the cpuset)</p>
</li>
<li><p class="first">OAREXEC: ESRF detected that sometime oarexec think that he notified the
Almighty with it exit code but nothing was seen on the server. So try to
resend the exit code until oarexec is killed.</p>
</li>
<li><p class="first">oar_Tools: add in notify_almighty a check on the print and on the close of
the socket connected to Almighty.</p>
</li>
<li><p class="first">oaraccounting: --sql is now possible into a &quot;oarstat --accounting&quot; query</p>
</li>
<li><p class="first">Add more logs to the command &quot;oarnodes -e host&quot; when a node turns into
Suspected</p>
</li>
<li><p class="first">Execute user commands with /proc/self/oom_adj to 15. So the first processes
that will be killed when there is no more memory available is the user
ones.
Hence the system will remain up and running and the user job will finished.
Drawback: this file can be changed manually by the user so if someone knows
a method to do the same thing but only managed by root, we take???</p>
</li>
<li><p class="first">Bugfix API: quotes where badly escaped into job submission (<a class="reference external" href="mailto:Ugo.Meda@insa-rennes.fr">Ugo.Meda&#64;insa-rennes.fr</a>)</p>
</li>
<li><p class="first">Add the possibility to automatically resubmit idempotent job which ends
with an exit code of 99: oarsub -t idempotent &quot;sleep 5; exit 99&quot;</p>
</li>
<li><p class="first">Bugfix API: Some informations where missing into jobs/details, especially the
scheduled resources.</p>
</li>
<li><p class="first">API: added support of &quot;param_file&quot; value for array job submissions. This value
is a string representing the content of a parameters file. Sample submission:</p>
<pre class="literal-block">
{&quot;resource&quot;:&quot;/cpu=1&quot;, &quot;command&quot;:&quot;sleep&quot;, &quot;param_file&quot;:&quot;60\n90\n30&quot;}
</pre>
<p>This submits 3 sleep jobs with differents sleep values.</p>
</li>
<li><p class="first">Remove any reference to gridlibs and gridapi as these components are obselete</p>
</li>
<li><p class="first">Add stdout and stderr files of each job in oarstat output.</p>
</li>
<li><p class="first">API now supports fastcgi (big performance raise!)</p>
</li>
<li><p class="first">Add &quot;-f&quot; option to oarnodesetting to read hostnames from a file.</p>
</li>
<li><p class="first">API can get/upload files (GET or POST /media/&lt;file_path&gt;)</p>
</li>
<li><p class="first">Make &quot;X11 forwarding&quot; working even if the user XAUTHORITY environment
variable does not contain ~/.Xauthority (GDM issue).</p>
</li>
<li><p class="first">Add job_resource_manager_cgroups which handles cpuset + other cgroup
features like network packet tagging, IO disk shares, ...</p>
</li>
<li><p class="first">Bugfix #13351: now oar_psql_db_init is executed with root privileges</p>
</li>
<li><p class="first">Bugfix #13434: reservation were not handled correctly with the energy
saving feature</p>
</li>
<li><p class="first">Add cgroups FREEZER feature to the suspend/resume script (better than kill
SIGSTOP/SIGCONT).
This is doable thanks to the new job_resource_manager_cgroups.</p>
</li>
<li><p class="first">Implement a new script 'oar-database' to manage the oar database.
oar_mysql_init &amp; oar_psql_init are dropped.</p>
</li>
<li><p class="first">Huge code reorganisation to allow a better packaging and system integration</p>
</li>
<li><p class="first">Drop the oarsub/oarstat 2.3 version that was kept for compatiblity issues
during the 2.4.x branch.</p>
</li>
<li><p class="first">By default the oar scheduler is now
'oar_sched_gantt_with_timesharing_and_fairsharing' and the following values
has been set in oar.conf: SCHEDULER_TIMEOUT to 30, SCHEDULER_NB_PROCESSES to 4
and SCHEDULER_FAIRSHARING_MAX_JOB_PER_USER to 30</p>
</li>
<li><p class="first">Add a limitation on the number of concurrent bipbip processes on the server
(for detached jobs).</p>
</li>
<li><p class="first">Add IPC cleaning to the job_resource_manager* when there is no other job of
the same user on the nodes.</p>
</li>
<li><p class="first">make better scheduling behaviour for dependency jobs</p>
</li>
<li><p class="first">API: added missing stop_time into /jobs/details</p>
</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-4-4">
<h2><a class="toc-backref" href="#id49">7.4&nbsp;&nbsp;&nbsp;version 2.4.4:</a></h2>
<blockquote>
<ul class="simple">
<li>oar_resource_init: bad awk delimiter. There's a space and if the property
is the first one then there is not a ','.</li>
<li>job suspend: oardo does not exist anymore (long long time ago). Replace it
with oardodo.</li>
<li>oarsub: when an admission rule died micheline returns an integer and not an
array ref. Now oarsub ends nicely.</li>
<li>Monika: add a link on each jobid on the node display area.</li>
<li>sshd_config: with nodes with a lot of core, 10 // connections could be too
few</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-4-3">
<h2><a class="toc-backref" href="#id50">7.5&nbsp;&nbsp;&nbsp;version 2.4.3:</a></h2>
<blockquote>
<ul class="simple">
<li>Hulot module now has customizable keepalive feature</li>
<li>Added a hook to launch a healing command when nodes are suspected
(activate the SUSPECTED_HEALING_EXEC_FILE variable)</li>
<li>Bugfix #9995: oaraccouting script doesn't freeze anymore when db is unreachable.</li>
<li>Bugfix #9990: prevent from inserting jobs with invalid username (like an empty username)</li>
<li>Oarnodecheck improvements: node is not checked if a job is already running</li>
<li>New oaradmin option: --auto-offset</li>
<li>Feature request #10565: add the possibility to check the aliveness of the
nodes of a job at the end of this one (pingchecker)</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-4-2">
<h2><a class="toc-backref" href="#id51">7.6&nbsp;&nbsp;&nbsp;version 2.4.2:</a></h2>
<blockquote>
<ul class="simple">
<li>New &quot;Hulot&quot; module for intelligent and configurable energy saving</li>
<li>Bug #9906: fix bad optimization in the gantt lib (so bad scheduling</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-4-1">
<h2><a class="toc-backref" href="#id52">7.7&nbsp;&nbsp;&nbsp;version 2.4.1:</a></h2>
<blockquote>
<ul class="simple">
<li>Bug #9038: Security flaw in oarsub --notify option</li>
<li>Bug #9601: Cosystem jobs are no more killed when a resource is set to Absent</li>
<li>Fixed some packaging bugs</li>
<li>API bug fixes in job submission parsing</li>
<li>Added standby info into <cite>oarnodes -s</cite> and available_upto info into
/resources uri of the API</li>
<li>Bug Grid'5000 #2687 Fix possible crashes of the scheduler.</li>
<li>Bug fix: with MySQL DB Finaud suspected resources which are not of the
&quot;default&quot; type.</li>
<li>Signed debian packages (install oar-keyring package)</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-4-0">
<h2><a class="toc-backref" href="#id53">7.8&nbsp;&nbsp;&nbsp;version 2.4.0:</a></h2>
<blockquote>
<ul>
<li><p class="first">Bug #8791: added CHECK_NODES_WITH_RUNNING_JOB=no to prevent from checking
occupied nodes</p>
</li>
<li><p class="first">Fix bug in oarnodesetting command generated by oar_resources_init (detect_resources)</p>
</li>
<li><p class="first">Added a --state option to oarstat to only get the status of specified jobs
(optimized query, to allow scripting)</p>
</li>
<li><p class="first">Added a REST API for OAR and OARGRID</p>
</li>
<li><p class="first">Added JSON support into oarnodes, oarstat and oarsub</p>
</li>
<li><p class="first">New Makefile adapted to build packages as non-root user</p>
</li>
<li><p class="first">add the command &quot;oar_resources_init&quot; to easily detect and initialize the
whole resources of a cluster.</p>
</li>
<li><p class="first">&quot;oaradmin version&quot; : now retrieve the most recent database schema number</p>
</li>
<li><p class="first">Fix rights on the &quot;schema&quot; table in postgresql.</p>
</li>
<li><p class="first">Bug #7509: fix bug in add_micheline_subjob for array jobs + jobtypes</p>
</li>
<li><p class="first">Ctrl-C was not working anymore in oarsub.
It seems that the signal handler does not handle the previous syntax
($SIG = 'qdel')</p>
</li>
<li><p class="first">Fix bug in oarsh with the &quot;-l&quot; option</p>
</li>
<li><p class="first">Bug #7487: bad initialisation of the gnatt for the container jobs.</p>
</li>
<li><p class="first">Scheduler: move the &quot;delete_unnecessary_subtrees&quot; directly into
&quot;find_first_hole&quot;. Thus this is possible to query a job like:</p>
<pre class="literal-block">
oarsub -I -l nodes=1/core=1+nodes=4/core=2
(no hard separation between each group)
</pre>
<dl class="docutils">
<dt>For the same behaviour as before, you can query:</dt>
<dd><p class="first last">oarsub -I -l {prop=1}/nodes=1/core=1+{prop=2}/nodes=4/core=2</p>
</dd>
</dl>
</li>
<li><p class="first">Bug #7634: test if the resource property value is effectively defined
otherwise print a ''</p>
</li>
<li><p class="first">Optional script to take into account cpu/core topology of the nodes at boot
time (to activate inside oarnodesetting_ssh)</p>
</li>
<li><p class="first">Bug #7174: Cleaned default PATH from &quot;./&quot; into oardodo</p>
</li>
<li><p class="first">Bug #7674: remove the computation of the scheduler_priority field for
besteffort jobs from the asynchronous OAR part. Now the value is set when
the jobs are turned into toLaunch state and in Error/Terminated.</p>
</li>
<li><p class="first">Bug #7691: add --array and --array-param-file options parsing into the
submitted script. Fix also some parsing errors.</p>
</li>
<li><p class="first">Bug #7962: enable resource property &quot;cm_availability&quot; to be manipulated by
the oarnodesetting command</p>
</li>
<li><dl class="first docutils">
<dt>Added the (standby) information to a node state in oarnodes when it's state</dt>
<dd><p class="first last">is Absent and cm_availability != 0</p>
</dd>
</dl>
</li>
<li><p class="first">Changed the name of cm_availability to available_upto which is more relevant</p>
</li>
<li><p class="first">add a --maintenance option to oarnodesetting that sets the state of a resource
to Absent and its available_upto to 0 if maintenance is on and resets previous
values if maintenance is off.</p>
</li>
<li><p class="first">added a --signal option to oardel that allow a user to send a signal to one of
his jobs</p>
</li>
<li><p class="first">added a name field in the schema table that will refer to the OAR version name</p>
</li>
<li><p class="first">added a table containing scheduler name, script and description</p>
</li>
<li><p class="first">Bug #8559: Almighty: Moved OAREXEC_XXXX management code out of the queue for
immediate action, to prevent potential problems in case of scheduler timeouts.</p>
</li>
<li><p class="first">oarnodes, oarstat and the REST API are no more making retry connections to the
database in case of failure, but exit with an error instead. The retry behavior
is left for daemons.</p>
</li>
<li><p class="first">improved packaging (try to install files in more standard places)</p>
</li>
<li><p class="first">improved init script for Almighty (into deb and rpm packages)</p>
</li>
<li><p class="first">fixed performance issue on oarstat (array_id index missing)</p>
</li>
<li><p class="first">fixed performance issue (job_id index missing in event_log table)</p>
</li>
<li><p class="first">fixed a performance issue at job submission (optimized a query and added an
index on challenges table)
decisions).</p>
</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-3-5">
<h2><a class="toc-backref" href="#id54">7.9&nbsp;&nbsp;&nbsp;version 2.3.5:</a></h2>
<blockquote>
<ul class="simple">
<li>Bug #8139: Drawgantt nil error (Add condition to test the presence of nil
value in resources table.)</li>
<li>Bug #8416: When a the automatic halt/wakeup feature is enabled then there
was a problem to determine idle nodes.</li>
<li>Debug a mis-initialization of the Gantt with running jobs in the
metascheduler (concurrency access to PG database)</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-3-4">
<h2><a class="toc-backref" href="#id55">7.10&nbsp;&nbsp;&nbsp;version 2.3.4:</a></h2>
<blockquote>
<ul class="simple">
<li>add the command &quot;oar_resources_init&quot; to easily detect and initialize the
whole resources of a cluster.</li>
<li>&quot;oaradmin version&quot; : now retrieve the most recent database schema number</li>
<li>Fix rights on the &quot;schema&quot; table in postgresql.</li>
<li>Bug #7509: fix bug in add_micheline_subjob for array jobs + jobtypes</li>
<li>Ctrl-C was not working anymore in oarsub.
It seems that the signal handler does not handle the previous syntax
($SIG = 'qdel')</li>
<li>Bug #7487: bad initialisation of the gnatt for the container jobs.</li>
<li>Fix bug in oarsh with the &quot;-l&quot; option</li>
<li>Bug #7634: test if the resource property value is effectively defined
otherwise print a ''</li>
<li>Bug #7674: remove the computation of the scheduler_priority field for
besteffort jobs from the asynchronous OAR part. Now the value is set when
the jobs are turned into toLaunch state and in Error/Terminated.</li>
<li>Bug #7691: add --array and --array-param-file options parsing into the
submitted script. Fix also some parsing errors.</li>
<li>Bug #7962: enable resource property &quot;cm_availability&quot; to be manipulated by
the oarnodesetting command</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-3-3">
<h2><a class="toc-backref" href="#id56">7.11&nbsp;&nbsp;&nbsp;version 2.3.3:</a></h2>
<blockquote>
<ul>
<li><p class="first">Fix default admission rules: case unsensitive check for properties used in
oarsub</p>
</li>
<li><p class="first">Add new oaradmin subcommand : oaradmin conf. Useful to edit conf files and
keep changes in a Subversion repository.</p>
</li>
<li><p class="first">Kill correctly each taktuk command children in case of a timeout.</p>
</li>
<li><p class="first">New feature: array jobs (option --array)  (on oarsub, oarstat oardel,
oarhold and oarresume) and file-based parametric array jobs
(oarsub --array-param-file)
/!in this version the DB scheme has changed. If you want to upgrade your
installation from a previous 2.3 release then you have to execute in your
database one of these SQL script (stop OAR before):</p>
<pre class="literal-block">
mysql:
    DB/mysql_structure_upgrade_2.3.1-2.3.3.sql

postgres:
    DB/pg_structure_upgrade_2.3.1-2.3.3.sql
</pre>
</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-3-2">
<h2><a class="toc-backref" href="#id57">7.12&nbsp;&nbsp;&nbsp;version 2.3.2:</a></h2>
<blockquote>
<ul class="simple">
<li>Change scheduler timeout implementation to schedule the maximum of jobs.</li>
<li>Bug #5879: do not show initial_request in oarstat when it is not a job of
the user who launched the oarstat command (oar or root).</li>
<li>Add a --event option to oarnodes and oarstat to display events recorded for
a job or node</li>
<li>Display reserved resources for a validated waiting reservation, with a hint
in their state</li>
<li>Fix oarproperty: property names are lowercase</li>
<li>Fix OAR_JOB_PROPERTIES_FILE: do not display system properties</li>
<li>Add a new user command: oarprint which allow to pretty print resource
properties of a job</li>
<li>Debug temporary job UID feature</li>
<li>Add 'kill -9' on subprocesses that reached a timeout (avoid Perl to
wait something)</li>
<li>desktop computing feature is now available again. (ex: oarsub -t
desktop_computing date)</li>
<li>Add versioning feature for admission rules with Subversion</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-3-1">
<h2><a class="toc-backref" href="#id58">7.13&nbsp;&nbsp;&nbsp;version 2.3.1:</a></h2>
<blockquote>
<ul class="simple">
<li>Add new oarmonitor command. This will permit to monitor OAR jobs on compute
nodes.</li>
<li>Remove sudo dependency and replace it by the commands &quot;oardo&quot; and
&quot;oardodo&quot;.</li>
<li>Add possibility to create a temporary user for each jobs on compute nodes.
So you can perform very strong restrictions for each job (ex: bandwidth
restrictions with iptable, memory management, ... everything that can be
handled with a user id)</li>
<li>Debian packaging: Run OAR specific sshd with root privileges (under heavy
load, kernel may be more responsive for root processes...)</li>
<li>Remove ALLOWED_NETWORKS tag in oar.conf (added more complexeity than
resolving problems)</li>
<li>/!change database scheme for the field <em>exit_code</em> in the table <em>jobs</em>.
Now <em>oarstat</em> <em>exit_code</em> line reflects the right exit code of the user
passive job (before, even when the user script was not launched the
<em>exit_code</em> was 0 which was BAD)</li>
<li>/!add DB field <em>initial_request</em> in the table <em>jobs</em> that stores the
oarsub line of the user</li>
<li>Feature Request #4868: Add a parameter to specify what the &quot;nodes&quot; resource
is a synomym for. Network_address must be seen as an internal data and not
used.</li>
<li>Scheduler: add timeout for each job == 1/4 of the remaining scheduler
timeout.</li>
<li>Bug #4866: now the whole node is Suspected instead of just the par where
there is no job onto. So it is possible to have a job on Suspected nodes.</li>
<li>Add job walltime (in seconds) in parameter of prologue and epilogue on
compute nodes.</li>
<li>oarnodes does not show system properties anymore.</li>
<li>New feature: container job type now allows to submit inner jobs for a
scheduling within the container job</li>
<li>Monika refactoring and now in the oar packaging.</li>
<li>Added a table schema in the db with the field version, reprensenting the
version of the db schema.</li>
<li>Added a field DB_PORT in the oar config file.</li>
<li>Bug #5518: add right initialization of the job user name.</li>
<li>Add new oaradmin command. This will permit to create resources and
manage admission rules more easily.</li>
<li>Bug #5692: change source code into a right Perl 5.10 syntax.</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-2-12">
<h2><a class="toc-backref" href="#id59">7.14&nbsp;&nbsp;&nbsp;version 2.2.12:</a></h2>
<blockquote>
<ul class="simple">
<li>Bug #5239: fix the bug if there are spaces into job name or project</li>
<li>Fix the bug in Iolib if DEAD_SWITCH_TIME &gt;0</li>
<li>Fix a bug in bipbip when calling the cpuset_manager to clean jobs in error</li>
<li>Bug #5469: fix the bug with reservations and Dead resources</li>
<li>Bug #5535: checks for reservations made at a same time was wrong.</li>
<li>New feature: local checks on nodes can be plugged in the oarnodecheck
mechanism. Results can be asynchronously checked from the server (taktuk
ping checker)</li>
<li>Add 2 new tables to keep track of the scheduling decisions
(gantt_jobs_predictions_log and gantt_jobs_resources_log). This will help
debugging scheduling troubles (see SCHEDULER_LOG_DECISIONS in oar.conf)</li>
<li>Now reservations are scheduled only once (at submission time). Resources
allocated to a reservations are definitively set once the validated is
done and won't change in next scheduler's pass.</li>
<li>Fix DrawGantt to not display besteffort jobs in the future which is
meaningless.</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-2-11">
<h2><a class="toc-backref" href="#id60">7.15&nbsp;&nbsp;&nbsp;version 2.2.11:</a></h2>
<blockquote>
<ul class="simple">
<li>Fix Debian package dependency on a CGI web server.</li>
<li>Fix little bug: remove notification (scheduled start time) for Interactive
reservation.</li>
<li>Fix bug in reservation: take care of the SCHEDULER_JOB_SECURITY_TIME for
reservations to check.</li>
<li>Fix bug: add a lock around the section which creates and feed the OAR
cpuset.</li>
<li>Taktuk command line API has changed (we need taktuk &gt;= 3.6).</li>
<li>Fix extra ' in the name of output files when using a job name.</li>
<li>Bug #4740: open the file in oarsub with user privileges (-S option)</li>
<li>Bug #4787: check if the remote socket is defined (problem of timing with
nmap)</li>
<li>Feature Request #4874: check system names when renaming properties</li>
<li>DrawGantt can export charts to be reused to build a global multi-OAR view
(e.g. DrawGridGantt).</li>
<li>Bug #4990: DrawGantt now uses the database localtime as its time reference.</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-2-10">
<h2><a class="toc-backref" href="#id61">7.16&nbsp;&nbsp;&nbsp;version 2.2.10:</a></h2>
<blockquote>
<ul class="simple">
<li>Job dependencies: if the required jobs do not have an exit code == 0 and in
the state Terminated then the schedulers refuse to schedule this job.</li>
<li>Add the possibility to disable the halt command on nodes with
cm_availability value.</li>
<li>Enhance oarsub &quot;-S&quot; option (more #OAR parsed).</li>
<li>Add the possibility to use oarsh without configuring the CPUSETs (can be
useful for users that don't want to configure there ssh keys)</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-2-9">
<h2><a class="toc-backref" href="#id62">7.17&nbsp;&nbsp;&nbsp;version 2.2.9:</a></h2>
<blockquote>
<ul class="simple">
<li>Bug 4225: Dump only 1 data structure when using -X or -Y or -D.</li>
<li>Bug fix in Finishing sequence (Suspect right nodes).</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-2-8">
<h2><a class="toc-backref" href="#id63">7.18&nbsp;&nbsp;&nbsp;version 2.2.8:</a></h2>
<blockquote>
<ul class="simple">
<li>Bug 4159: remove unneeded Dump print from oarstat.</li>
<li>Bug 4158: replace XML::Simple module by XML::Dumper one.</li>
<li>Bug fix for reservation (recalculate the right walltime).</li>
<li>Print job dependencies in oarstat.</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-2-7">
<h2><a class="toc-backref" href="#id64">7.19&nbsp;&nbsp;&nbsp;version 2.2.7:</a></h2>
</div>
<div class="section" id="id2">
<h2><a class="toc-backref" href="#id65">7.20&nbsp;&nbsp;&nbsp;version 2.2.11:</a></h2>
<blockquote>
<ul class="simple">
<li>Fix Debian package dependency on a CGI web server.</li>
<li>Fix little bug: remove notification (scheduled start time) for Interactive
reservation.</li>
<li>Fix bug in reservation: take care of the SCHEDULER_JOB_SECURITY_TIME for
reservations to check.</li>
<li>Fix bug: add a lock around the section which creates and feed the OAR
cpuset.</li>
<li>Taktuk command line API has changed (we need taktuk &gt;= 3.6).</li>
<li>Fix extra ' in the name of output files when using a job name.</li>
<li>Bug #4740: open the file in oarsub with user privileges (-S option)</li>
<li>Bug #4787: check if the remote socket is defined (problem of timing with
nmap)</li>
<li>Feature Request #4874: check system names when renaming properties</li>
<li>DrawGantt can export charts to be reused to build a global multi-OAR view
(e.g. DrawGridGantt).</li>
<li>Bug #4990: DrawGantt now uses the database localtime as its time reference.</li>
</ul>
</blockquote>
</div>
<div class="section" id="id3">
<h2><a class="toc-backref" href="#id66">7.21&nbsp;&nbsp;&nbsp;version 2.2.10:</a></h2>
<blockquote>
<ul class="simple">
<li>Job dependencies: if the required jobs do not have an exit code == 0 and in
the state Terminated then the schedulers refuse to schedule this job.</li>
<li>Add the possibility to disable the halt command on nodes with
cm_availability value.</li>
<li>Enhance oarsub &quot;-S&quot; option (more #OAR parsed).</li>
<li>Add the possibility to use oarsh without configuring the CPUSETs (can be
useful for users that don't want to configure there ssh keys)</li>
</ul>
</blockquote>
</div>
<div class="section" id="id4">
<h2><a class="toc-backref" href="#id67">7.22&nbsp;&nbsp;&nbsp;version 2.2.9:</a></h2>
<blockquote>
<ul class="simple">
<li>Bug 4225: Dump only 1 data structure when using -X or -Y or -D.</li>
<li>Bug fix in Finishing sequence (Suspect right nodes).</li>
</ul>
</blockquote>
</div>
<div class="section" id="id5">
<h2><a class="toc-backref" href="#id68">7.23&nbsp;&nbsp;&nbsp;version 2.2.8:</a></h2>
<blockquote>
<ul class="simple">
<li>Bug 4159: remove unneeded Dump print from oarstat.</li>
<li>Bug 4158: replace XML::Simple module by XML::Dumper one.</li>
<li>Bug fix for reservation (recalculate the right walltime).</li>
<li>Print job dependencies in oarstat.</li>
</ul>
</blockquote>
</div>
<div class="section" id="id6">
<h2><a class="toc-backref" href="#id69">7.24&nbsp;&nbsp;&nbsp;version 2.2.7:</a></h2>
<blockquote>
<ul class="simple">
<li>Bug 4106: fix oarsh and oarcp issue with some options (erroneous leading
space).</li>
<li>Bug 4125: remove exit_code data when it is not relevant.</li>
<li>Fix potential bug when changing asynchronously the state of the jobs into
&quot;Terminated&quot; or &quot;Error&quot;.</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-2-6">
<h2><a class="toc-backref" href="#id70">7.25&nbsp;&nbsp;&nbsp;version 2.2.6:</a></h2>
<blockquote>
<ul>
<li><dl class="first docutils">
<dt>Bug fix: job types was not sent to cpuset manager script anymore.</dt>
<dd><p class="first last">(border effect from bug 4069 resolution)</p>
</dd>
</dl>
</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-2-5">
<h2><a class="toc-backref" href="#id71">7.26&nbsp;&nbsp;&nbsp;version 2.2.5:</a></h2>
<blockquote>
<ul class="simple">
<li>Bug fix: remove user command when oar execute the epilogue script on the
nodes.</li>
<li>Clean debug and mail messages format.</li>
<li>Remove bad oarsub syntax from oarsub doc.</li>
<li>Debug xauth path.</li>
<li>bug 3995: set project correctly when resubmitting a job</li>
<li>debug 'bash -c' on Fedora</li>
<li>bug 4069: reservations with CPUSET_ERROR (remove bad hosts and continue
with a right integrity in the database)</li>
<li>bug 4044: fix free resources query for reservation (get the nearest hole
from the beginning of the reservation)</li>
<li>bug 4013: now Dead, Suspected and Absent resources have different colors in
drawgantt with a popup on them.</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-2-4">
<h2><a class="toc-backref" href="#id72">7.27&nbsp;&nbsp;&nbsp;version 2.2.4:</a></h2>
<blockquote>
<ul class="simple">
<li>Redirect third party commands into oar.log (easier to debug).</li>
<li>Add user info into drawgantt interface.</li>
<li>Some bug fixes.</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-2-3">
<h2><a class="toc-backref" href="#id73">7.28&nbsp;&nbsp;&nbsp;version 2.2.3:</a></h2>
<blockquote>
<ul class="simple">
<li>Debug prologue and epilogue when oarexec receives a signal.</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-2-2">
<h2><a class="toc-backref" href="#id74">7.29&nbsp;&nbsp;&nbsp;version 2.2.2:</a></h2>
<blockquote>
<ul class="simple">
<li>Switch nice value of the user processes into 0 in oarsh_shell (in case of
sshd was launched with a different priority).</li>
<li>debug taktuk zombies in pingchecker and oar_Tools</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-2-1">
<h2><a class="toc-backref" href="#id75">7.30&nbsp;&nbsp;&nbsp;version 2.2.1:</a></h2>
<blockquote>
<ul class="simple">
<li>install the &quot;allow_clasic_ssh&quot; feature by default</li>
<li>debug DB installer</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-2">
<h2><a class="toc-backref" href="#id76">7.31&nbsp;&nbsp;&nbsp;version 2.2:</a></h2>
<blockquote>
<ul class="simple">
<li>oar_server_proepilogue.pl: can be used for server prologue and epilogue to
authorize users to access to nodes that are completely allocated by OAR. If
the whole node is assigned then it kills all jobs from the user if all cpus
are assigned.</li>
<li>the same thing can be done with cpuset_manager_PAM.pl as the script used to
configure the cpuset. More efficent if cpusets are configured.</li>
<li>debug cm_availability feature to switch on and off nodes automatically
depending on waiting jobs.</li>
<li>reservations now take care of cm_availability field</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-1-0">
<h2><a class="toc-backref" href="#id77">7.32&nbsp;&nbsp;&nbsp;version 2.1.0:</a></h2>
<blockquote>
<ul class="simple">
<li>add &quot;oarcp&quot; command to help the users to copy files using oarsh.</li>
<li>add sudo configuration to deal with bash. Now oarsub and oarsh have the
same behaviour as ssh (the bash configuration files are loaded correctly)</li>
<li>bug fix in drawgantt (loose jobs after submission of a moldable one)</li>
<li>add SCHEDULER_RESOURCES_ALWAYS_ASSIGNED_TYPE into oar.conf. Thus admin can
add some resources for each jobs (like frontale node)</li>
<li>add possibility to use taktuk to check the aliveness of the nodes</li>
<li>%jobid% is now replaced in stdout and stderr file names by the effective
job id</li>
<li>change interface to shu down or wake up nodes automatically (now the node
list is read on STDIN)</li>
<li>add OARSUB_FORCE_JOB_KEY in oar.conf. It says to create a job ssh key by
default for each job.</li>
<li>%jobid% is now replaced in the ssh job key name (oarsub -k ...).</li>
<li>add NODE_FILE_DB_FIELD_DISTINCT_VALUES in oar.conf that enables the admin
to configure the generated containt of the OAR_NODE_FILE</li>
<li>change ssh job key oarsub options behaviour</li>
<li>add options &quot;--reinitialize&quot; and &quot;--delete-before&quot; to the oaraccounting
command</li>
<li>cpuset are now stored in /dev/cpuset/oar</li>
<li>debian packaging: configure and launch a specific sshd for the user oar</li>
<li>use a file descriptor to send the node list --&gt; able to handle a very large
amount of nodes</li>
<li>every config files are now in /etc/oar/</li>
<li>oardel can add a besteffort type to jobs and vis versa</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-0-2">
<h2><a class="toc-backref" href="#id78">7.33&nbsp;&nbsp;&nbsp;version 2.0.2:</a></h2>
<blockquote>
<ul class="simple">
<li>add warnings and exit code to oarnodesetting when there is a bad node name
or resource number</li>
<li>change package version</li>
<li>change default behaviour for the cpuset_manager.pl (more portable)</li>
<li>enable a user to use the same ssh key for several jobs (at his own risk!)</li>
<li>add node hostnames in oarstat -f</li>
<li>add --accounting and -u options in oarstat</li>
<li>bug fix on index fields in the database (syncro): bug 2020</li>
<li>bug fix about server pro/epilogue: bug 2022</li>
<li>change the default output of oarstat. Now it is usable: bug 1875</li>
<li>remove keys in authorized_keys of oar (on the nodes) that do not
correspond to an active cpuset (clean after a reboot)</li>
<li>reread oar.conf after each database connection tries</li>
<li>add support for X11 forwarding in oarsub -I and -C</li>
<li>debug mysql initialization script in debian package</li>
<li>add a variable in oarsh for the default options of ssh to use
(more useful to change if the ssh version installed does not
handle one of these options)</li>
<li>read oar.conf in oarsh (so admin can more easily change options in this
script)</li>
<li>add support for X11 forwarding via oarsh</li>
<li>change variable for oarsh: OARSH_JOB_ID --&gt; OAR_JOB_ID</li>
</ul>
</blockquote>
</div>
<div class="section" id="version-2-0-0">
<h2><a class="toc-backref" href="#id79">7.34&nbsp;&nbsp;&nbsp;version 2.0.0:</a></h2>
<blockquote>
<ul>
<li><p class="first">Now, with the ability to declare any type of resources like licences,
VLAN, IP range, computing resources must have the type <em>default</em> and a
network_address not null.</p>
</li>
<li><p class="first">Possibility to declare associated resources like licences, IP ranges, ...
and to reserve them like others.</p>
</li>
<li><p class="first">Now you can connect to your jobs (not only for reservations).</p>
</li>
<li><p class="first">Add &quot;cosystem&quot; job type (execute and do nothing for these jobs).</p>
</li>
<li><p class="first">New scheduler : &quot;oar_sched_gantt_with_timesharing&quot;. You can specify jobs
with the type &quot;timesharing&quot; that indicates that this scheduler can launch
more than 1 job on a resource at a time. It is possible to restrict this
feature with words &quot;user and name&quot;. For example, '-t
timesharing=user,name' indicates that only a job from the same user with
the same name can be launched in the same time than it.</p>
</li>
<li><p class="first">Add PostGresSQL support. So there is a choice to make between MySQL and
PostgresSQL.</p>
</li>
<li><p class="first">New approach for the scheduling : administrators have to insert into the
databases descriptions about resources and not nodes. Resources have a
network address (physical node) and properties. For example, if you have
dual-processor, then you can create 2 different resources with the same
natwork address but with 2 different processor names.</p>
</li>
<li><p class="first">The scheduler can now handle resource properties in a hierarchical
manner. Thus, for example, you can do &quot;oarsub -l /switch=1/cpu=5&quot; which
submit a job on 5 processors on the same switch.</p>
</li>
<li><p class="first">Add a signal handler in oarexec and propagate this signal to the user
process.</p>
</li>
<li><p class="first">Support '#OAR -p ...' options in user script.</p>
</li>
<li><dl class="first docutils">
<dt>Add in oar.conf:</dt>
<dd><ul class="first last simple">
<li>DB_BASE_PASSWD_RO : for security issues, it is possible to execute
request with parts specified by users with a read only account (like
&quot;-p&quot; option).</li>
<li>OARSUB_DEFAULT_RESOURCES : when nothing is specified with the oarsub
command then OAR takes this default resource description.</li>
<li>OAREXEC_DEBUG_MODE : turn on or off debug mode in oarexec (create
/tmp/oar/oar.log on nodes).</li>
<li>FINAUD_FREQUENCY : indicates the frequency when OAR launchs Finaud
(search dead nodes).</li>
<li>SCHEDULER_TIMEOUT : indicates to the scheduler the amount of time
after what it must end itself.</li>
<li>SCHEDULER_JOB_SECURITY_TIME : time between each job.</li>
<li>DEAD_SWITCH_TIME : after this time Absent and Suspected resources are
turned on the Dead state.</li>
<li>PROLOGUE_EPILOGUE_TIMEOUT : the possibility to specify a different
timeout for prologue and epilogue (PROLOGUE_EPILOGUE_TIMEOUT).</li>
<li>PROLOGUE_EXEC_FILE : you can specify the path of the prologue script
executed on nodes.</li>
<li>EPILOGUE_EXEC_FILE : you can specify the path of the epilogue script
executed on nodes.</li>
<li>GENERIC_COMMAND : a specific script may be used instead of ping to
check aliveness of nodes. The script must return bad nodes on STDERR
(1 line for a bad node and it must have exactly the same name that
OAR has given in argument of the command).</li>
<li>JOBDEL_SOFTWALLTIME : time after a normal frag that the system waits
to retry to frag the job.</li>
<li>JOBDEL_WALLTIME : time after a normal frag that the system waits
before to delete the job arbitrary and suspects nodes.</li>
<li>LOG_FILE : specify the path of OAR log file (default :
/var/log/oar.log).</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first">Add wait() in pingchecker to avoid zombies.</p>
</li>
<li><p class="first">Better code modularization.</p>
</li>
<li><p class="first">Remove node install part to launch jobs. So it is easier to upgrade from
one version to an other (oarnodesetting must already be installed on each
nodes if we want to use it).</p>
</li>
<li><p class="first">Users can specify a method to be notified (mail or script).</p>
</li>
<li><p class="first">Add cpuset support</p>
</li>
<li><p class="first">Add prologue and epilogue script to be executed on the OAR server before
and after launching a job.</p>
</li>
<li><p class="first">Add dependancy support between jobs (&quot;-a&quot; option in oarsub).</p>
</li>
<li><p class="first">In oarsub you can specify the launching directory (&quot;-d&quot; option).</p>
</li>
<li><p class="first">In oarsub you can specify a job name (&quot;-n&quot; option).</p>
</li>
<li><p class="first">In oarsub you can specify stdout and stderr file names.</p>
</li>
<li><p class="first">User can resubmit a job (option &quot;--resubmit&quot; in oarsub).</p>
</li>
<li><p class="first">It is possible to specify a read only database account and it will be
used to evaluate SQL properties given by the user with the oarsub command
(more scecure).</p>
</li>
<li><p class="first">Add possibility to order assigned resources with their properties by the
scheduler. So you can privilege some resources than others
(SCHEDULER_RESOURCE_ORDER tag in oar.conf file)</p>
</li>
<li><p class="first">a command can be specified to switch off idle nodes
(SCHEDULER_NODE_MANAGER_SLEEP_CMD, SCHEDULER_NODE_MANAGER_IDLE_TIME,
SCHEDULER_NODE_MANAGER_SLEEP_TIME in oar.conf)</p>
</li>
<li><p class="first">a command can be specified to switch on nodes in the Absent state
according to the resource property <em>cm_availability</em> in the table
resources (SCHEDULER_NODE_MANAGER_WAKE_UP_CMD in oar.conf).</p>
</li>
<li><p class="first">if a job goes in Error state and this is not its fault then OAR will
resubmit this one.</p>
</li>
</ul>
</blockquote>
</div>
</div>
</div>
<div class="footer">
<hr class="footer" />
<a class="reference external" href="OAR-DOCUMENTATION-USER.rst">View document source</a>.
Generated on: 2012-11-16 09:25 UTC.
Generated by <a class="reference external" href="http://docutils.sourceforge.net/">Docutils</a> from <a class="reference external" href="http://docutils.sourceforge.net/rst.html">reStructuredText</a> source.

</div>



</div>





</div>

<div id="footer" class="pagefooter">

<div id="pageinfo">









<div class="pagedate">
Last edited <span class="date">Friday 16 November 2012</span>
<!-- Created <span class="date">Wednesday 15 June 2011</span> -->
</div>

</div>


<!-- from OAR -->
</div>

</div>

</body>
</html>
